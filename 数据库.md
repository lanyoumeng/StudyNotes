# mysql

## shell

```sql
连接 mysql -h localhost -u root -p  # 连接 MariaDB，-h 指定主机，-P 指定监听端口，-u 指定登录用户，-p 指定登录密码

show databases；   //不加 “ ；” 不会结束

use ziroom //使用哪个数据库
show tables; // 有哪些表
select * from （表名）
desc (表名) 查看表结构


更改密码： ALTER USER 'root'@'localhost' IDENTIFIED BY '服务器密码';
刷新 MySQL 权限，以确保新密码生效：FLUSH PRIVILEGES;

创建 myblog 用户
grant all on myblog.* TO myblog@127.0.0.1 identified by  ''  // 登录 mysql -h 127.0.0.1 -u myblog -p 
flush privileges;

使用.sql文件
sourse config/myblog.sql

//根据表生成go结构体 (shell)
db2struct --gorm --no-json -H 127.0.0.1 -d myblog -t user --package model --struct UserM -u myblog -p '' --target=user.go
 
 //mysqldump 将创建数据库和表的 SQL 语句保存在 configs 目录下供以后部署使用：
mysqldump -h127.0.0.1 -u myblog --databases myblog -p'密码' --add-drop-database --add-drop-table --add-drop-trigger --add-locks --no-data > configs/myblog.sql
```



## 执行流程

MySQL 的架构共分为两层：**Server 层和存储引擎层**，

- **Server 层负责建立连接、分析和执行 SQL**。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。
- **存储引擎层负责数据的存储和提取**。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408182322296.png" alt="mysql_E6_9F_A5_E8_AF_A2_E6_B5_81_E7_A8_8B" style="zoom:50%;" />



### 第一步：连接器

 **MySQL 是基于 TCP 协议进行传输的**

> 如何查看 MySQL 服务被多少个客户端连接了？

如果你想知道当前 MySQL 服务被多少个客户端连接了，你可以执行 `show processlist` 命令进行查看。

![_E6_9F_A5_E7_9C_8B_E8_BF_9E_E6_8E_A5](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408251648174.png)

比如上图的显示结果，共有两个用户名为 root 的用户连接了 MySQL 服务，其中 id 为 6 的用户的 Command 列的状态为 `Sleep` ，这意味着该用户连接完 MySQL 服务就没有再执行过任何命令，也就是说这是一个空闲的连接，并且空闲的时长是 736 秒（ Time 列）。

> 空闲连接会一直占用着吗？

当然不是了，MySQL 定义了空闲连接的最大空闲时长，由 `wait_timeout` 参数控制的，默认值是 8 小时（28880秒），如果空闲连接超过了这个时间，连接器就会自动将它断开。

```sql
mysql> show variables like 'wait_timeout';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| wait_timeout  | 28800 |
+---------------+-------+
1 row in set (0.00 sec)
```

当然，我们自己也可以手动断开空闲的连接，使用的是 kill connection + id 的命令。

```sql
mysql> kill connection +6;
Query OK, 0 rows affected (0.00 sec)
```

一个处于空闲状态的连接被服务端主动断开后，这个客户端并不会马上知道，等到客户端在发起下一个请求的时候，才会收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。

> MySQL 的连接数有限制吗？

MySQL 服务支持的最大连接数由 max_connections 参数控制，比如我的 MySQL 服务默认是 151 个,超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。

```sql
mysql> show variables like 'max_connections';
+-----------------+-------+
| Variable_name   | Value |
+-----------------+-------+
| max_connections | 151   |
+-----------------+-------+
1 row in set (0.00 sec)
```

MySQL 的连接也跟 HTTP 一样，有短连接和长连接的概念，它们的区别如下：

```c
// 短连接
连接 mysql 服务（TCP 三次握手）
执行sql
断开 mysql 服务（TCP 四次挥手）

// 长连接
连接 mysql 服务（TCP 三次握手）
执行sql
执行sql
执行sql
....
断开 mysql 服务（TCP 四次挥手）
```

可以看到，使用长连接的好处就是可以减少建立连接和断开连接的过程，所以一般是推荐使用长连接。

但是，使用长连接后可能会占用内存增多，因为 MySQL 在执行查询过程中临时使用内存管理连接对象，这些连接对象资源只有在连接断开时才会释放。如果长连接累计很多，将导致 MySQL 服务占用内存太大，有可能会被系统强制杀掉，这样会发生 MySQL 服务异常重启的现象。

> 怎么解决长连接占用内存的问题？

有两种解决方式。

第一种，**定期断开长连接**。既然断开连接后就会释放连接占用的内存资源，那么我们可以定期断开长连接。

第二种，**客户端主动重置连接**。MySQL 5.7 版本实现了 `mysql_reset_connection()` 函数的接口，注意这是接口函数不是命令，那么当客户端执行了一个很大的操作后，在代码里调用 mysql_reset_connection 函数来重置连接，达到释放内存的效果。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。



### 第二步：查询缓存

这里说的查询缓存是 server 层的，

对于更新比较频繁的表，查询缓存的命中率很低的，因为只要一个表有更新操作，那么这个表的查询缓存就会被清空。

MySQL 8.0 版本直接将查询缓存删掉了

### 第三步：解析 SQL

这个工作交由「解析器」来完成。



解析器会做如下两件事情。

第一件事情，**词法分析**。MySQL 会根据你输入的字符串识别出关键字出来，例如，SQL语句 select username from userinfo，在分析之后，会得到4个Token，其中有2个Keyword，分别为select和from：

| 关键字 | 非关键字 | 关键字 | 非关键字 |
| ------ | -------- | ------ | -------- |
| select | username | from   | userinfo |

第二件事情，**语法分析**。根据词法分析的结果，语法解析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法，如果没问题就会构建出 SQL 语法树，这样方便后面模块获取 SQL 类型、表名、字段名、 where 条件等等。

![db-mysql-sql-parser-2](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408251651149.png)



### 第四步：执行 SQL

经过解析器后，接着就要进入执行 SQL 查询语句的流程了，每条`SELECT` 查询语句流程主要可以分为下面这三个阶段：

1. prepare 阶段，也就是预处理阶段；

   - 检查 SQL 查询语句中的表或者字段是否存在；
   - 将 `select *` 中的 `*` 符号，扩展为表上的所有列；

   我下面这条查询语句，test 这张表是不存在的，这时 MySQL 就会在执行 SQL 查询语句的 prepare 阶段中报错。

   ```sql
   mysql> select * from test;
   ERROR 1146 (42S02): Table 'mysql.test' doesn't exist
   ```

   表或字段是否存在的判断，不是在解析器里做的，而是在 prepare 阶段。

2. optimize 阶段，也就是优化阶段；

   - **优化器主要负责将 SQL 查询语句的执行方案确定下来**，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。
   - 加个 `explain` 命令，这样就会输出这条 SQL 语句的执行计划

3. execute 阶段，也就是执行阶段；

   - 主键索引查询
   - 全表扫描
   - 索引下推
     - 通过在索引扫描阶段提前应用WHERE条件，减少回表次数，优化查询性能。
     - **回表** 是在索引无法提供查询所需的所有字段时，数据库需要额外访问主表以获取完整数据的过程。



### 底层通信模型

MySQL 客户端和服务器之间的通信采用基于**请求-响应模式（Request-Response Model）\**的半双工通信模型，底层使用的是基于 TCP/IP 协议的 Socket 通信。MySQL 使用了\**自定义的二进制协议**来处理客户端和服务器之间的请求和响应。

1. **连接建立**：客户端与服务器使用三次握手建立 TCP 连接，并进行身份验证（用户名和密码校验）。
2. **请求发送**：客户端发送 SQL 查询请求给服务器。
3. **响应接收**：服务器执行 SQL 语句，将执行结果封装成二进制响应包返回给客户端。
4. **连接保持和断开**：客户端可以在一个 TCP 连接中多次发送 SQL 查询，直到客户端主动关闭连接。

MySQL 的通信模型采用**半双工模式**，意味着客户端和服务器在同一时间内只能进行单向数据传输（客户端发送请求或服务器返回响应）。这种模式可以减少通信的复杂度，但在高并发情况下，可能导致客户端等待时间较长。



## 存储引擎

MySQL 存储引擎有 MyISAM 、InnoDB、Memory



### InnoDB

每创建一个 database（数据库） 都会在 /var/lib/mysql/ 目录里面创建一个以 database 为名的目录，然后保存表结构和表数据的文件都会存放在这个目录里。

```shell
[root@xiaolin ~]#ls /var/lib/mysql/my_test
db.opt  
t_order.frm  
t_order.ibd
```

可以看到，共有三个文件，这三个文件分别代表着：

- db.opt，用来存储当前数据库的默认字符集和字符校验规则。
- t_order.frm ，t_order 的**表结构**会保存在这个文件。在 MySQL 中建立一张表都会生成一个.frm 文件，该文件是用来保存每个表的元数据信息的，主要包含表结构定义。
- t_order.ibd，t_order 的**表数据**会保存在这个文件。表数据既可以存在共享表空间文件（文件名：ibdata1）里，也可以存放在独占表空间文件（文件名：表名字.ibd）。这个行为是由参数 innodb_file_per_table 控制的，若设置了参数 innodb_file_per_table 为 1，则会将存储的数据、索引等信息单独存储在一个独占表空间，从 MySQL 5.6.6 版本开始，它的默认值就是 1 了，因此从这个版本之后， MySQL 中每一张表的数据都存放在一个独立的 .ibd 文件。

好了，现在我们知道了一张数据库表的数据是保存在「 表名字.ibd 」的文件里的，这个文件也称为独占表空间文件。



**表空间由段（segment）、区（extent）、页（page）、行（row）组成**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408251703247.png" alt="_E8_A1_A8_E7_A9_BA_E9_97_B4_E7_BB_93_E6_9E_84.drawio" style="zoom:50%;" />

#### 1、行（row）

**一行数据的最大字节数 为65535，包含「变长字段长度列表」和 「NULL 值列表」所占用的字节数的**

字段是允许为 NULL 的，所以**会用 1 字节来表示「NULL 值列表」**。

**如果有多个字段的话，要保证所有字段的长度 + 变长字段字节数列表所占用的字节数 + NULL值列表所占用的字节数 <= 65535**。



数据库表中的记录都是按行（row）进行存放的，每行记录根据不同的行格式，有不同的存储结构。

InnoDB 提供了 4 种行格式，分别是 Redundant、Compact、Dynamic和 Compressed 行格式。

- Redundant 是很古老的行格式了， MySQL 5.0 版本之前用的行格式，现在基本没人用了。
- 由于 Redundant 不是一种紧凑的行格式，所以 MySQL 5.0 之后引入了 **Compact** 行记录存储方式，Compact 是一种紧凑的行格式，设计的初衷就是为了让一个数据页中可以存放更多的行记录，从 MySQL 5.1 版本之后，行格式默认设置成 Compact。
- Dynamic 和 Compressed 两个都是紧凑的行格式，它们的行格式都和 Compact 差不多，因为都是基于 Compact 改进一点东西。从 MySQL5.7 版本之后，默认使用 **Dynamic** 行格式。

##### Compact

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408251711350.png" alt="COMPACT.drawio" style="zoom: 33%;" />

记录的额外信息包含 3 个部分：变长字段长度列表、NULL 值列表、记录头信息。

###### 1.变长字段长度列表

**「变长字段长度列表」只出现在数据表有变长字段的时候**

char 是定长的，varchar 是变长的，所以，在存储数据的时候，也要把数据占用的大小存到「变长字段长度列表」里面

这些变长字段的真实数据占用的字节数会按照列的顺序**逆序存放**，

主要是因为

1. 「记录头信息」中指向下一个记录的**指针**，指向的是下一条记录的**「记录头信息」和「真实数据」之间的位置**，这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。
2. 可以使得位置靠前的记录的真实数据和数据对应的字段长度信息可以同时在一个 **CPU Cache Line** 中，这样就可以提高 CPU Cache 的命中率。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408251717397.png" alt="_E5_8F_98_E9_95_BF_E5_AD_97_E6_AE_B5_E9_95_BF_E5_BA_A6_E5_88_97_E8_A1_A81" style="zoom: 33%;" />

###### 2.null

**当数据表的字段都定义成 NOT NULL 的时候，这时候表里的行格式就不会有 NULL 值列表了**。

NULL 值列表的信息也需要逆序存放

存在允许 NULL 值的列，则每个列对应一个二进制位（bit），二进制位按照列的顺序逆序排列。

- 二进制位的值为`1`时，代表该列的值为NULL。
- 二进制位的值为`0`时，代表该列的值不为NULL。

另外，NULL 值列表必须用整数个字节的位表示（1字节8位），如果使用的二进制位个数不足整数个字节，则在字节的高位补 `0`。

「NULL 值列表」的空间不是固定 1 字节的。

当一条记录有 9 个字段值都是 NULL，那么就会创建 2 字节空间的「NULL 值列表」

###### 3. 记录头信息

记录头信息中包含的内容很多，我就不一一列举了，这里说几个比较重要的：

- delete_mask ：标识此条数据是否被删除。从这里可以知道，我们执行 detele 删除记录的时候，并不会真正的删除记录，只是将这个记录的 delete_mask 标记为 1。
- next_record：下一条记录的位置。从这里可以知道，记录与记录之间是通过链表组织的。在前面我也提到了，指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。
- record_type：表示当前记录的类型，0表示普通记录，1表示B+树非叶子节点记录，2表示最小记录，3表示最大记录



###### 真实数据



记录真实数据部分除了我们定义的字段，还有三个隐藏字段，分别为：row_id、trx_id、roll_pointer

<img src="D:\DownLoad\_E8_AE_B0_E5_BD_95_E7_9A_84_E7_9C_9F_E5_AE_9E_E6_95_B0_E6_8D_AE.png" alt="_E8_AE_B0_E5_BD_95_E7_9A_84_E7_9C_9F_E5_AE_9E_E6_95_B0_E6_8D_AE" style="zoom:50%;" />

- row_id

如果我们建表的时候指定了主键或者唯一约束列，那么就没有 row_id 隐藏字段了。如果既没有指定主键，又没有唯一约束，那么 InnoDB 就会为记录添加 row_id 隐藏字段。row_id不是必需的，占用 6 个字节。

- trx_id

事务id，表示这个数据是由哪个事务生成的。 trx_id是必需的，占用 6 个字节。

- roll_pointer

这条记录上一个版本的指针。roll_pointer 是必需的，占用 7 个字节。







###### 行溢出

**发生行溢出，多的数据就会存到另外的「溢出页」中**

![_E8_A1_8C_E6_BA_A2_E5_87_BA](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408251833405.png)



#### 2、页（page）

记录是按照行来存储的，但是**InnoDB 的数据是按「页」为单位来读写的**

**默认每个页的大小为 16KB**，也就是最多能保证 16KB 的连续存储空间，常见的有数据页、undo 日志页、溢出页等等。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408262032474.png" alt="243b1466779a9e107ae3ef0155604a17" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408262032036.png" alt="fabd6dadd61a0aa342d7107213955a72" style="zoom:50%;" />

在 File Header 中有两个指针，分别指向上一个数据页和下一个数据页，连接起来的页相当于一个双向的链表，

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408262037480.png" alt="557d17e05ce90f18591c2305871af665" style="zoom:50%;" />

采用链表的结构是让数据页之间不需要是物理上的连续的，而是逻辑上的连续。

**数据页中的记录按照「主键」顺序组成单向链表**，单向链表的特点就是插入、删除非常方便，但是检索效率不高，最差的情况下需要遍历链表上的所有节点才能完成检索。

因此，数据页中有一个**页目录**，起到记录的索引作用

页目录创建的过程如下：

1. 将所有的记录划分成几个组，这些记录包括最小记录和最大记录，但不包括标记为“已删除”的记录；
2. 每个记录组的最后一条记录就是组内最大的那条记录，并且最后一条记录的头信息中会存储该组一共有多少条记录，作为 n_owned 字段（上图中粉红色字段）
3. 页目录用来存储每组最后一条记录的地址偏移量，这些地址偏移量会按照先后顺序存储起来，每组的地址偏移量也被称之为槽（slot），**每个槽相当于指针指向了不同组的最后一个记录**。

从图可以看到，**页目录就是由多个槽组成的，槽相当于分组记录的索引**。然后，因为记录是按照「主键值」从小到大排序的，所以**我们通过槽查找记录时，可以使用二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到对应的记录**

InnoDB 对每个分组中的记录条数都是有规定的，槽内的记录就只有几条：

- 第一个分组中的记录只能有 1 条记录；
- 最后一个分组中的记录条数范围只能在 1-8 条之间；
- 剩下的分组中记录条数范围只能在 4-8 条之间。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408262038869.png" alt="261011d237bec993821aa198b97ae8ce" style="zoom:50%;" />

####  3、区（extent）

 InnoDB 存储引擎是用 B+ 树来组织数据的。B+ 树中每一层都是通过双向链表连接起来的，如果是以页为单位来分配存储空间，那么链表中相邻的两个页之间的物理位置并不是连续的，可能离得非常远，那么磁盘查询时就会有大量的随机I/O，随机 I/O 是非常慢的。

解决这个问题也很简单，就是让链表中相邻的页的物理位置也相邻，这样就可以使用顺序 I/O 了，那么在范围查询（扫描叶子节点）的时候性能就会很高。

那具体怎么解决呢？

**在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区（extent）为单位分配。每个区的大小为 1MB，对于 16KB 的页来说，连续的 64 个页会被划为一个区，这样就使得链表中相邻的页的物理位置也相邻，就能使用顺序 I/O 了**。

#### 4、段（segment）

表空间是由各个段（segment）组成的，段是由多个区（extent）组成的。段一般分为数据段、索引段和回滚段等。

- 索引段：存放 B + 树的非叶子节点的区的集合；
- 数据段：存放 B + 树的叶子节点的区的集合；
- 回滚段：存放的是回滚数据的区的集合，之前讲[事务隔离 (opens new window)](https://xiaolincoding.com/mysql/transaction/mvcc.html)的时候就介绍到了 MVCC 利用了回滚段实现了多版本查询数据。



###  MyISAM 

#### 介绍

**MyISAM** 是 MySQL 数据库管理系统中的一种存储引擎，它曾经是 MySQL 的默认存储引擎（在 MySQL 5.5 之前），在某些应用场景中具有重要的作用。尽管 MyISAM 在某些情况下仍然有用，但它已经被更为强大的 InnoDB 引擎所取代，特别是在需要事务支持和数据完整性的应用中。

MyISAM 的主要特点

1. **不支持事务**：
   - MyISAM 不支持事务处理，也不支持ACID（原子性、一致性、隔离性、持久性）特性。因此，对于需要确保数据一致性和安全性（如银行或电商系统）的应用，MyISAM 并不是合适的选择。

2. **表级锁**：
   - MyISAM 使用表级锁（Table-level Locking），这意味着当一个表正在被写入时，整个表都会被锁定，其他查询请求必须等待写操作完成。这对于写操作频繁的应用可能会成为性能瓶颈，但对于以读取为主的应用，MyISAM 的性能表现不错。

3. **支持全文索引**：
   - MyISAM 支持全文索引（FULLTEXT），这使得它在进行文本搜索（如搜索引擎）时具有优势，特别是对于较大文本字段的快速搜索。

4. **存储结构**：
   - 每个 MyISAM 表对应三个文件：
     - `.frm` 文件：存储表的结构定义。
     - `.MYD` 文件：存储表的数据。
     - `.MYI` 文件：存储表的索引。
   - 数据和索引是分开存储的，这在某些场景下可以提高读取性能。

5. **容易崩溃但容易修复**：
   - 由于 MyISAM 不支持事务和崩溃恢复机制，当系统崩溃或意外关机时，MyISAM 表可能会损坏。不过，MyISAM 提供了一些工具（如 `myisamchk`）来修复损坏的表，虽然不能保证数据完全不丢失，但大多数情况下可以修复大部分损坏的数据。

6. **表和行的压缩**：
   - MyISAM 支持表和行的压缩，通过这种方式可以减少磁盘的占用空间，从而提高查询性能，但这也会导致写入性能下降。

MyISAM 的应用场景

1. **读多写少的场景**：
   - 由于 MyISAM 在读操作上的表现较好，特别适用于读多写少的场景，如数据分析、报表生成等。

2. **全文搜索**：
   - 在需要高效文本搜索的应用中，MyISAM 的全文索引功能使其成为一个好的选择，尽管现在 InnoDB 也支持全文索引。

3. **历史数据归档**：
   - 对于不需要频繁更新的数据，如日志数据或归档数据，MyISAM 可以通过压缩功能有效地节省存储空间。



#### 对比

- **事务支持**：
  
  - MyISAM 不支持事务，而 InnoDB 支持事务，并提供了更高的数据安全性和一致性。
  
- InnoDB 和 MyISAM 都支持 B+ 树索引，但是它们数据的存储结构实现方式不同。不同之处在于：
  
  - InnoDB 存储引擎：B+ 树索引的叶子节点保存数据本身；
  - MyISAM 存储引擎：B+ 树索引的叶子节点保存数据的物理地址；
  
- **锁机制**：
  - MyISAM 使用表级锁，适合读操作多的场景；InnoDB 使用行级锁，在高并发场景下表现更好。

- **外键支持**：
  - MyISAM 不支持外键，而 InnoDB 支持外键，能够强制维护数据的引用完整性。

- **性能**：
  - MyISAM 在读密集型应用中性能优越，而 InnoDB 在需要频繁写操作和事务管理的场景中表现更好。

- **崩溃恢复**：
  - MyISAM 在崩溃后需要手动修复表，容易丢失数据；InnoDB 提供自动崩溃恢复机制，数据更安全。
  
    

## 存储类型

MySQL 提供了多种数据类型，主要用于存储不同类型的数据。它们可以分为 **数值类型**、**日期和时间类型**、**字符串类型** 及 **空间类型**（Geospatial Types）。

以下是 MySQL 中所有主要的数据类型分类及其简要说明：

### 1. 数值类型

#### **整数类型**（Integer Types）
- **TINYINT**：1 字节，范围为 `-128` 到 `127`（有符号），或 `0` 到 `255`（无符号）。
- **SMALLINT**：2 字节，范围为 `-32,768` 到 `32,767`（有符号），或 `0` 到 `65,535`（无符号）。
- **MEDIUMINT**：3 字节，范围为 `-8,388,608` 到 `8,388,607`（有符号），或 `0` 到 `16,777,215`（无符号）。
- **INT / INTEGER**：4 字节，范围为 `-2,147,483,648` 到 `2,147,483,647`（有符号），或 `0` 到 `4,294,967,295`（无符号）。
- **BIGINT**：8 字节，范围为 `-9,223,372,036,854,775,808` 到 `9,223,372,036,854,775,807`（有符号），或 `0` 到 `18,446,744,073,709,551,615`（无符号）。

#### **浮点数和定点数类型**（Floating-Point and Fixed-Point Types）
- **FLOAT(M,D)**：4 字节，用于存储小范围的浮点数。`M` 是数字总位数，`D` 是小数点后的位数。
- **DOUBLE(M,D)**：8 字节，用于存储大范围的浮点数。`M` 和 `D` 的含义同上。
- **DECIMAL(M,D)** / **NUMERIC(M,D)**：用于存储定点数。与浮点数不同，`DECIMAL` 类型精确到小数点后 `D` 位。

#### **位类型**（Bit Type）
- **BIT(M)**：用于存储 `M` 位的位字段，`M` 的取值范围为 `1` 到 `64`。

### 2. 日期和时间类型

- **DATE**：日期，格式为 `YYYY-MM-DD`，范围为 `1000-01-01` 到 `9999-12-31`。
- **TIME**：时间，格式为 `HH:MM:SS`，范围为 `-838:59:59` 到 `838:59:59`。
- **DATETIME**：日期和时间，格式为 `YYYY-MM-DD HH:MM:SS`，范围为 `1000-01-01 00:00:00` 到 `9999-12-31 23:59:59`。
- **TIMESTAMP**：时间戳，格式同 `DATETIME`，存储自 `1970-01-01 00:00:01` UTC 以来的秒数，范围为 `1970-01-01` 到 `2038-01-19`。
- **YEAR(M)**：年份，格式为 `YYYY`，范围为 `1901` 到 `2155`。

### 3. 字符串类型

#### **字符类型**（String Types）
- **CHAR(M)**：固定长度的字符串，长度 `M` 最多为 255 个字符。
- **VARCHAR(M)**：可变长度的字符串，长度 `M` 最多为 65,535 个字符（包含所有其他字符和列的开销）。

#### **文本类型**（Text Types）
- **TINYTEXT**：可存储最多 255 个字符的文本。
- **TEXT**：可存储最多 65,535 个字符的文本。
- **MEDIUMTEXT**：可存储最多 16,777,215 个字符的文本。
- **LONGTEXT**：可存储最多 4,294,967,295 个字符的文本。

#### **二进制类型**（Binary Types）
- **BINARY(M)**：固定长度的二进制数据，最多 `M` 字节。
- **VARBINARY(M)**：可变长度的二进制数据，最多 `M` 字节。

#### **BLOB 类型**（Binary Large Objects）
- **TINYBLOB**：最多存储 255 字节的二进制数据。
- **BLOB**：最多存储 65,535 字节的二进制数据。
- **MEDIUMBLOB**：最多存储 16,777,215 字节的二进制数据。
- **LONGBLOB**：最多存储 4,294,967,295 字节的二进制数据。

#### **枚举和集合类型**（Enumeration and Set Types）
- **ENUM**：枚举类型，字符串对象，其值必须从预定义的列表中选出，每个表最多允许 65,535 个不同的枚举值。
- **SET**：集合类型，一个字符串对象，其值可以是从预定义列表中选出的多个值的组合（即一个字段可以包含多个值）。

### 4. 空间数据类型（Spatial Types）
MySQL 提供对 GIS（地理信息系统）数据的支持，这些类型用于存储空间数据：
- **GEOMETRY**：存储几何类型的通用类型。
- **POINT**：表示一个地理坐标（x, y）。
- **LINESTRING**：表示一条线，由多个点构成。
- **POLYGON**：表示一个多边形区域。
- **MULTIPOINT**：由多个 `POINT` 组成。
- **MULTILINESTRING**：由多个 `LINESTRING` 组成。
- **MULTIPOLYGON**：由多个 `POLYGON` 组成。
- **GEOMETRYCOLLECTION**：表示几何对象的集合，可以包含 `POINT`、`LINESTRING` 和 `POLYGON`。

### 5. JSON 数据类型
- **JSON**：用于存储 JSON 格式的数据，MySQL 5.7 及更高版本开始支持。



### 数据类型选择的基本原则
- **整数类型**：当需要存储整数时，根据数值大小选择合适的整数类型（如 `TINYINT`、`INT`、`BIGINT` 等）。
- **浮点数和定点数**：当需要存储小数时，`FLOAT` 和 `DOUBLE` 用于近似值存储，`DECIMAL` 用于存储精确的小数。
- **日期和时间类型**：用于存储日期、时间或时间戳，根据应用需求选择合适的日期类型（如 `DATE`、`TIME`、`DATETIME`、`TIMESTAMP`）。
- **字符串类型**：存储字符串时，`CHAR` 用于固定长度的字符串，`VARCHAR` 用于可变长度的字符串，`TEXT` 和 `BLOB` 适用于大文本或二进制数据。

选择正确的数据类型可以提高数据库性能，并确保数据的存储和处理更加高效。



## SQL语句

### 示例

SELECT子句及其顺序:

| 子句     | 说明               | 是否必须使用 |
| -------- | ------------------ | ------------ |
| SELECT   | 要返回的列或表达式 | 是           |
| FROM     | 从中检索数据的表   | 是           |
| WHERE    | 行级过滤           | 否           |
| GROUP BY | 分组说明           | 否           |
| HAVING   | 组级过滤           | 否           |
| ORDER BY | 输出排序顺序       | 否           |

```SQL
#数据库从0开始
#   多行注释/**/
#别名用AS  AS关键字是可选的，不过最好使用它，这被视为一条最佳实践。

#DISTINCT 去除重复数据
SELECT DISTINCT vend_id
FROM Products;

#limit  OFFSET从哪一行开始
SELECT prod_name
FROM Products 
LIMIT 5  OFFSET 5;

#默认升序  DESC  降序 关键字只应用到直接位于其前面的列名。 （ASC升序）
# where   BETWEEN在指定的两个值之间    IS NULL为NULL值
#and or  注意加括号  在处理OR操作符前，优先处理AND操作符
#IN操作符一般比一组OR操作符执行得更快   包含其他SELECT语句
#NOT   在更复杂的子句中，NOT是非常有用的
SELECT prod_id, prod_price, prod_name
FROM Products
WHERE prod_price = 3.49  and/or prod_name is null
#  IN ('DLL01','BRS01') 
ORDER BY prod_price DESC, prod_name ; 

SELECT prod_id, prod_price, prod_name FROM Products ORDER BY 2, 3;

//
#通配符   只能用于文本字段（字符串） like _
# WHERE prod_name LIKE '%'不会匹配产品名称为NULL的行
#'_'匹配一个字符
#mysql不支持：方括号（[]）通配符      用来指定一个字符集，它必须匹配指定位置（通配符的位置）的一个字符。
SELECT prod_id, prod_name 
FROM Products 
WHERE prod_name LIKE 'Fish%'; 
WHERE prod_name LIKE '__ inch teddy bear';

//
#计算字段
#1.拼接字段   Concat
#RTRIM()函数去掉值右边的所有空格
SELECT Concat(RTrim(vend_name), ' (',RTrim(vend_country), ')') AS vend_title
FROM Vendors
ORDER BY vend_name;
#输出
vend_title
------------------
Bear Emporium (USA)
Bears R Us (USA)
#2.创建计算字段 +-*/
SELECT prod_id,quantity,item_price,quantity*item_price AS expanded_price
FROM OrderItems
WHERE order_num = 20008; 

测试计算 SELECT语句为测试、检验函数和计算提供了很好的方法。虽然SELECT通常用于从表中检索数据，但是省略了FROM子句后就是简单地访问和处理表达式，例如SELECT 3 * 2;将返回6，SELECT Trim(' abc ');将返回abc，SELECT Curdate();使用Curdate()函数返回当前日期和时间。
```

### 增、删、改、查

在 MySQL 中，增、删、改、查操作对应的 SQL 语句分别是 `INSERT`、`DELETE`、`UPDATE` 和 `SELECT`。下面是每种操作的基本用法示例：

#### 1. 插入数据 (`INSERT`)

插入数据到表中，可以指定插入的列和对应的值。

```sql
-- 插入一条完整数据，适用于表中所有列都有值的情况
INSERT INTO table_name (column1, column2, column3) 
VALUES (value1, value2, value3);

-- 插入多条数据
INSERT INTO table_name (column1, column2, column3) 
VALUES 
    (value1, value2, value3),
    (value4, value5, value6);

-- 插入部分列，未指定的列将使用默认值
INSERT INTO table_name (column1, column2) 
VALUES (value1, value2);
```

**示例**：将用户信息插入到 `users` 表中。

```sql
INSERT INTO users (username, email, age) 
VALUES ('john_doe', 'john@example.com', 25);
```

#### 2. 删除数据 (`DELETE`)

删除表中符合条件的行。如果不指定条件，将删除表中的所有数据（慎用）。

```sql
-- 删除符合条件的行
DELETE FROM table_name WHERE condition;

-- 删除所有行（慎用）
DELETE FROM table_name;
```

**示例**：删除 `users` 表中年龄小于 18 岁的用户。

```sql
DELETE FROM users WHERE age < 18;
```

#### 3. 更新数据 (`UPDATE`)

更新表中符合条件的行，可以更新一列或多列。

```sql
-- 更新符合条件的行
UPDATE table_name 
SET column1 = value1, column2 = value2 
WHERE condition;
```

**示例**：更新 `users` 表中用户名为 `john_doe` 的用户的邮箱。

```sql
UPDATE users 
SET email = 'new_email@example.com' 
WHERE username = 'john_doe';
```

#### 4. 查询数据 (`SELECT`)

从表中查询数据，可以指定要查询的列，并使用条件过滤、排序和分页。

指定**DISTINCT**关键词，去掉表中重复的行 

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409231716689.png" alt="image-20240923171640309" style="zoom:50%;" />



```sql
-- 查询指定的列
SELECT column1, column2 
FROM table_name 
WHERE condition 
ORDER BY column1 ASC|DESC 
LIMIT offset, count;

-- 查询所有列
SELECT * FROM table_name;
```

**示例 2**：查询 `users` 表中年龄大于 20 岁的用户，按年龄降序排列，并只返回前 10 条记录。

```sql
SELECT * 
FROM users 
WHERE age > 20 
ORDER BY age DESC 
LIMIT 10;

-- 连接查询
SELECT * FROM table1
JOIN table2 ON table1.id = table2.id;  -- 内连接
SELECT * FROM table1
LEFT JOIN table2 ON table1.id = table2.id;  -- 左连接
SELECT * FROM table1
RIGHT JOIN table2 ON table1.id = table2.id;  -- 右连接
```





### 函数

#### 文本处理函数

| 函数        | 说明                                                     |
| ----------- | -------------------------------------------------------- |
| `LEFT()`    | 返回字符串左边的字符。                                   |
| `LENGTH()`  | 返回字符串的长度。也可以使用 `DATALENGTH()` 或 `LEN()`。 |
| `LOWER()`   | 将字符串转换为小写。                                     |
| `LTRIM()`   | 去掉字符串左边的空格。                                   |
| `RIGHT()`   | 返回字符串右边的字符。也可以使用子字符串函数。           |
| `RTRIM()`   | 去掉字符串右边的空格。                                   |
| `SUBSTR()`  | 提取字符串的组成部分。也可以使用 `SUBSTRING()` 函数。    |
| `SOUNDEX()` | 返回字符串的 SOUNDEX 值。                                |
| `UPPER()`   | 将字符串转换为大写。                                     |

这些文本处理函数在SQL查询中用于处理和操作字符串数据，可以根据需要在数据库中使用它们。不同的数据库管理系统可能会有一些差异，但通常这些函数在大多数主流数据库中都有相应的实现。

```SQL
#SOUNDEX是一个将任何文本串转换为描述其语音表示的字母数字模式的算法，能对字符串进行发音比较而不是字母比较
SELECT cust_name, cust_contact
FROM Customers
WHERE SOUNDEX(cust_contact) = SOUNDEX('Michael Green'); 
输出：
cust_name           cust_contact
----------          -------------
Kids Place          Michelle Green
```

#### 日期和时间处理函数

各个数据库最不一致，兼容性差

#### 数值处理函数

| 函数     | 说明                     |
| -------- | ------------------------ |
| `ABS()`  | 返回一个数的绝对值。     |
| `COS()`  | 返回一个角度的余弦值。   |
| `EXP()`  | 返回一个数的指数值。     |
| `PI()`   | 返回圆周率 π（pi）的值。 |
| `SIN()`  | 返回一个角度的正弦值。   |
| `SQRT()` | 返回一个数的平方根。     |
| `TAN()`  | 返回一个角度的正切值。   |

#### 聚集函数

<1>一般忽略列值为NULL的行。

但 COUNT(*) 对表中行的数目进行计数，包含空值（NULL）

<2>对非数值数据使用MAX() MIN()

虽然MAX()一般用来找出最大的数值或日期值，但许多（并非所有）DBMS允许将它用来返回任意列中的最大值，包括返回文本列中的最大值。在用于文本数据时，MAX()返回按该列排序后的最后一行。

| 函数    | 说明             |
| ------- | ---------------- |
| AVG()   | 返回某列的平均值 |
| COUNT() | 返回某列的行数   |
| MAX()   | 返回某列的最大值 |
| MIN()   | 返回某列的最小值 |
| SUM()   | 返回某列值之和   |

聚集不同值\

以上5个聚集函数都可以如下使用。

对所有行执行计算，指定ALL参数或不指定参数（因为ALL是默认行为） 。

只包含不同的值，指定DISTINCT参数。

**DISTINCT不能用于COUNT(\*)**

```SQL
SELECT AVG(DISTINCT prod_price) AS avg_price
FROM Products
WHERE vend_id = 'DLL01';
```



#### count对比

count() 是一个聚合函数，函数的参数不仅可以是字段名，也可以是其他任意表达式，该函数作用是**统计符合查询条件的记录中，函数指定的参数不为 NULL 的记录有多少个**。

- COUNT(\*) 和 COUNT(1)：用来统计所有行，MySQL 内部优化后执行效率一致。
- COUNT(字段)：只统计该字段不为 NULL 的行，额外的 NULL 检查可能使其稍慢，特别是当该字段没有索引时。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408270825866.png" alt="af711033aa3423330d3a4bc6baeb9532" style="zoom:50%;" />

**count(`*`) 也就是count(`0`)     ==count(1)**

count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。

所以，如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。

再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引



##### 为什么要通过遍历的方式来计数？

InnoDB 存储引擎是支持事务的，同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的



##### 如何优化 count(*)？

1. 使用 show table status 或者 explain 命令来表进行估算。
2. 额外表保存计数值，在数据表插入一条记录的同时，将计数表中的计数字段 + 1。







### 分组数据

#### GROUP BY

指示DBMS分组数据，然后对每个组而不是整个结果集进行聚集。

GROUP BY子句可以包含任意数目的列，因而可以对分组进行嵌套，更细致地进行数据分组。

如果在GROUP BY子句中嵌套了分组，数据将在最后指定的分组上进行汇总。换句话说，在建立分组时，指定的所有列都一起计算（所以不能从个别的列取回数据）。

GROUP BY子句中列出的每一列都必须是检索列或有效的表达式（但不能是聚集函数）。如果在SELECT中使用表达式，则必须在GROUP BY子句中指定相同的表达式。不能使用别名。

大多数SQL实现不允许GROUP BY列带有长度可变的数据类型（如文本或备注型字段）。

除聚集计算语句外，SELECT语句中的每一列都必须在GROUP BY子句中给出。

如果分组列中包含具有NULL值的行，则NULL将作为一个分组返回。如果列中有多行NULL值，它们将分为一组。

GROUP BY子句必须出现在WHERE子句之后，ORDER BY子句之前。



#### Having

使用HAVING时应该结合GROUP BY子句

所有类型的WHERE子句都可以用HAVING来替代。唯一的差别是，WHERE过滤行，而HAVING过滤分组。

**WHERE在数据分组前进行过滤，HAVING在数据分组后进行过滤。**

```SQL
SELECT order_num, COUNT(*) AS items
FROM OrderItems
WHERE prod_price >= 4
GROUP BY order_num
HAVING COUNT(*) >= 3
ORDER BY items, order_num;
```

### 子查询

#### 利用子查询进行过滤

```SQL
SELECT cust_id
FROM Orders
WHERE order_num IN (SELECT order_num
                                        FROM OrderItems
                                        WHERE prod_id = 'RGAN01');
```

#### 作为计算字段使用子查询

```SQL
比较Orders表中的cust_id和当前正从Customers表中检索的cust_id：
完全限定列名 
SELECT cust_name, cust_state,(SELECT COUNT(*) 
                                    FROM Orders
                           WHERE Orders.cust_id = Customers.cust_id) AS orders
FROM Customers 
ORDER BY cust_name; 
```

### 联结

联结是一种机制，用来在一条SELECT语句中关联表



### **游标**（Cursor）

是 MySQL 中的一种数据库对象，主要用于处理查询返回的多行结果集。通过游标，开发者可以逐行处理查询结果，而不是一次性获取所有行，特别适用于需要在存储过程中逐条处理数据的场景。

MySQL 游标是一种非常有用的工具，适用于需要逐行处理结果集的场景。尽管游标提供了灵活性，但在处理大量数据时应谨慎使用，以避免性能瓶颈。游标在存储过程中的应用非常常见，特别是当简单的 SQL 查询无法满足复杂的数据操作需求时，游标可以提供逐行精确控制的能力。

#### 游标的主要功能

1. **逐行处理数据**: 游标允许你从结果集中一行一行地读取数据，适合处理复杂逻辑或者无法直接通过 SQL 完成的操作。
2. **常用于存储过程**: 在 MySQL 中，游标通常与存储过程一起使用，便于在循环中逐行操作查询结果。

#### 游标的工作步骤

1. **声明游标**:
   - 在 MySQL 中，游标必须先声明，声明时不会立即执行查询，查询操作会在游标被打开时执行。
2. **打开游标**:
   - 游标声明后，需要通过 `OPEN` 语句打开游标，执行查询并准备开始遍历结果集。
3. **取数据**:
   - 使用 `FETCH` 语句从游标中提取一行数据，并存储到指定的变量中。每次调用 `FETCH` 语句时，游标会自动移向下一行。
4. **关闭游标**:
   - 当处理完所有数据后，游标需要显式关闭。关闭游标会释放与游标关联的所有资源。

#### 游标的基本操作

1. **声明游标**: 使用 `DECLARE` 语句在存储过程中声明一个游标，游标的查询可以是任何返回结果集的 `SELECT` 语句。

   ```sql
   DECLARE cursor_name CURSOR FOR 
   SELECT column1, column2 FROM table_name WHERE condition;
   ```

2. **打开游标**: 使用 `OPEN` 语句打开游标，准备开始读取数据。

   ```sql
   OPEN cursor_name;
   ```

3. **取数据**: 使用 `FETCH` 语句从游标中提取一行数据，存入指定的变量中。

   ```sql
   FETCH cursor_name INTO var1, var2;
   ```

4. **关闭游标**: 使用 `CLOSE` 语句关闭游标，释放资源。

   ```sql
   CLOSE cursor_name;
   ```

#### 游标的完整使用示例

下面是一个存储过程的示例，展示如何使用游标逐行处理查询结果：

```sql
DELIMITER $$

CREATE PROCEDURE process_users()
BEGIN
    DECLARE done INT DEFAULT 0;
    DECLARE user_id INT;
    DECLARE user_name VARCHAR(100);
    
    -- 声明游标
    DECLARE user_cursor CURSOR FOR 
    SELECT id, name FROM users WHERE active = 1;

    -- 声明处理完成的条件
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = 1;
    
    -- 打开游标
    OPEN user_cursor;
    
    -- 循环处理游标数据
    read_loop: LOOP
        FETCH user_cursor INTO user_id, user_name;
        
        -- 检查是否已到结果集的末尾
        IF done THEN
            LEAVE read_loop;
        END IF;
        
        -- 处理每一行数据的逻辑
        -- 在这里可以执行任意操作，比如更新其他表或调用其他存储过程
        UPDATE users SET processed = 1 WHERE id = user_id;
    END LOOP;
    
    -- 关闭游标
    CLOSE user_cursor;
END$$

DELIMITER ;
```

1. **DECLARE 游标**: `DECLARE user_cursor CURSOR FOR SELECT` 语句声明了一个游标，查询返回所有活跃用户的 `id` 和 `name` 列。
2. **声明 CONTINUE HANDLER**: `DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = 1;` 是一个异常处理器，用于处理当游标读取到结果集末尾时的情况。它设置一个 `done` 标志位，以便在循环中停止读取。
3. **打开游标**: `OPEN user_cursor;` 打开游标，准备开始读取数据。
4. **循环读取数据**: `FETCH user_cursor INTO user_id, user_name;` 每次从游标中提取一行数据，并存储到变量中，直到结果集结束。
5. **关闭游标**: `CLOSE user_cursor;` 关闭游标，释放资源。

#### 游标的注意事项

1. **性能问题**: 游标逐行处理数据，相比直接的 SQL 语句操作，性能可能较低。因此，应尽量避免在处理大数据集时使用游标，除非有特定的需求。
2. **事务处理**: 游标的使用通常与事务相结合，确保在处理数据时的一致性。需要注意的是，在 MySQL 中如果游标的查询涉及到多个表，确保合适的事务管理。
3. **游标作用域**: 游标只能在存储过程、函数或触发器中使用，并且它的作用域是该存储过程、函数或触发器的生命周期。





### alert

在数据库中，`ALTER` 是用来修改**表结构**的关键字，通常与 `ALTER TABLE` 语句一起使用，能够执行多种表结构的更改操作，包括添加、修改或删除列，以及应用约束等。

下面是一些常见的 `ALTER TABLE` 操作：

添加列：

```SQL
ALTER TABLE table_name ADD column_name datatype;
```

删除列：

```SQL
ALTER TABLE table_name DROP COLUMN column_name;
```

修改列的数据类型：

```SQL
ALTER TABLE table_name MODIFY column_name new_datatype;
```

添加约束（比如添加主键约束）：

```SQL
ALTER TABLE table_name ADD CONSTRAINT pk_constraint PRIMARY KEY (column_name);
```

添加外键约束：

```SQL
ALTER TABLE table_name ADD CONSTRAINT fk_constraint FOREIGN KEY (column_name) REFERENCES other_table(other_column);
```

添加索引：

```SQL
ALTER TABLE table_name ADD INDEX index_name (column_name);
```

修改表名：

```SQL
ALTER TABLE old_table_name RENAME TO new_table_name;
```

这些是一些常见的 `ALTER TABLE` 操作，它们允许你在数据库中动态地修改表的结构，以满足数据模型的变化或优化数据库设计。在执行任何 `ALTER TABLE` 操作之前，请务必备份数据库，以防止意外损失数据。



### 分页查询

分页常适用于性能优化，如以下场景：

业务只需要获取部分数据，如top 10，此时查询所有数据到客户端，浪费数据库磁盘IO/网络IO、应用内存
数据量太大，传输时间可能过长甚至中断、查询到客户端内存开销巨大，每次只查询较小数量的行，分多次查询
MySQL中使用limit实现分页查询。

#### 基本语法

```sql
SELECT * FROM table 
LIMIT {[offset,] row_count | row_count OFFSET offset}
```

如上所示，limit可接受1或2个参数，参数必须是非负整数。

```sql
LIMIT offset, row_count
```

释义：从指定偏移位置开始返回，一共返回多少行

注意：第一行的偏移量为0，而不是1

例：SELECT * FROM tbl LIMIT 5,10，表示从第6行开始返回、一共返回10行。

这是MySQL分页最常见的写法。

```sql
LIMIT row_count
等价于LIMIT 0, row_count。

LIMIT row_count OFFSET offset
为了兼容PostgreSQL，参见《PostgreSQL Limit分页》
```

例：LIMIT 10 OFFSET 5 等价于 LIMIT 5,10

推荐采用该方法，语义更明确、兼容性更好。

#### 性能优化

##### 执行器自动优化

MySQL执行器会自动优化含LIMIT的语句，包括但不限于如下情况：

1. limit较少行时，可能会直接进行全表扫描、而不走索引
2. limit和order by、group by、distinct同时使用时，排序/聚合后的行数，一旦达到limit的row_count，排序/聚合就会停止。
   因为这个特性，如果多个行的排序列值相同，那么使用LIMIT时其返回的顺序可能不同。
3. limit 0会快速返回一个空结果集
4. MySQL一旦向客户端发送了所需的行数，它将立即终止查询

##### 主动优化执行计划

1. **加排序索引**
   limit和order by同时使用的概率非常高。如果排序列有索引，结合limit找到所需行数会立刻返回的特性，查询效率非常高
   例：

   ```sql
   SELECT * FROM tbl WHERE rid = 123 ORDER BY id LIMIT 50, 10
   ```

   建议增加rid、id的联合索引（rid在前，这样索引会先匹配rid、然后再按id的顺序获取数据）。

2. **偏移量较大时**，使用索引
   偏移量较大时，MySQL需要扫描过偏移量之前的所有行，然后再读取所需行返回，磁盘IO(随机IO)很多。因此分页查询越往后越慢。

3. 可通过**子查询优化**：

   ```sql
   SELECT * FROM tbl WHERE rid = 123 ORDER BY id LIMIT 100000, 10
   ```

   实际上只需要10行数据，但是要在磁盘中遍历100000行后，才真正开始获取数据。

   ```sql
   SELECT * FROM 
   	(SELECT id FROM tbl WHERE rid = 123 ORDER BY id LIMIT 100000, 10) temp 
   	LEFT JOIN tbl t ON temp.id = t.id
   ```

   通过子查询，可以直接在索引中获取到匹配行的主键。MySQL的索引是B+树，本身有序、并且节点中存放了主键，所以检索效率非常高。



### 开窗函数

MySQL 的**开窗函数（Window Functions）** 是 MySQL 8.0 引入的强大功能，它允许你在不使用子查询或临时表的情况下执行类似聚合的操作，但保留详细的行数据。**与聚合函数不同，开窗函数不会将结果合并为单行，而是在查询结果的每一行中返回聚合计算的结果。**

开窗函数为复杂查询提供了强大的分析能力，尤其适用于需要保留详细数据的同时进行聚合计算的场景。通过开窗函数，可以方便地进行排名、累计、区间计算等操作，使得查询更加高效和灵活。

#### 开窗函数的语法

开窗函数的基本语法是：

```sql
<window_function> OVER (
    [PARTITION BY <partition_column>]
    [ORDER BY <order_column>]
    [ROWS or RANGE <frame_definition>]
)
```

#### 常用的开窗函数

以下是 MySQL 中常用的开窗函数：

1. **`ROW_NUMBER()`**：返回当前行在窗口分区中的序号，从 1 开始递增。
   
   ```sql
   SELECT name, salary, ROW_NUMBER() OVER (ORDER BY salary DESC) AS rank
   FROM employees;
   ```

2. **`RANK()`**：返回当前行在窗口分区中的排名，但如果有相同的值，它们会有相同的排名，并且后续的排名会跳过。

   ```sql
   SELECT name, salary, RANK() OVER (ORDER BY salary DESC) AS rank
   FROM employees;
   ```

3. **`DENSE_RANK()`**：类似于 `RANK()`，但不会跳过排名。

   ```sql
   SELECT name, salary, DENSE_RANK() OVER (ORDER BY salary DESC) AS rank
   FROM employees;
   ```

4. **`NTILE(n)`**：将结果集划分为 n 个桶，并返回当前行所在的桶号。

   ```sql
   SELECT name, salary, NTILE(4) OVER (ORDER BY salary DESC) AS quartile
   FROM employees;
   ```

5. **`LEAD()`** 和 **`LAG()`**：获取相对于当前行的前一行或后一行的值。
   - `LEAD()`：返回当前行之后的某行值。
   - `LAG()`：返回当前行之前的某行值。

   ```sql
   -- LAG() 示例：获取上一行的工资
   SELECT name, salary, LAG(salary, 1) OVER (ORDER BY salary) AS prev_salary
   FROM employees;
   
   -- LEAD() 示例：获取下一行的工资
   SELECT name, salary, LEAD(salary, 1) OVER (ORDER BY salary) AS next_salary
   FROM employees;
   ```

6. **`SUM()`、`AVG()`、`MAX()`、`MIN()`** 等聚合函数：也可以作为开窗函数使用，来对窗口中的行进行累计、平均、最大值、最小值等操作。

   ```sql
   SELECT name, salary, SUM(salary) OVER (PARTITION BY department) AS total_salary
   FROM employees;
   ```



#### 开窗函数中的 `OVER()` 子句

`OVER()` 子句用于定义窗口的范围和行为，包含以下几个部分：

1. **PARTITION BY**：指定如何对数据进行分组，类似于 `GROUP BY`，但开窗函数不会合并数据，而是在每组数据上执行窗口操作。
   
   ```sql
   SELECT name, department, salary, SUM(salary) OVER (PARTITION BY department) AS department_total
   FROM employees;
   ```

2. **ORDER BY**：指定如何对窗口内的行进行排序，排序决定了函数如 `ROW_NUMBER()` 或 `RANK()` 的行为。

   ```sql
   SELECT name, salary, ROW_NUMBER() OVER (ORDER BY salary DESC) AS rank
   FROM employees;
   ```

3. **ROWS 和 RANGE**：用于定义窗口的帧，即窗口函数操作的行范围。`ROWS` 和 `RANGE` 的行为略有不同，`ROWS` 更加严格，而 `RANGE` 是基于值的范围。

   - **ROWS**：基于行的数量来定义帧范围。
   - **RANGE**：基于值范围来定义帧范围。

   ```sql
   -- 计算当前行和前两行的累计和
   SELECT name, salary, SUM(salary) OVER (ORDER BY salary ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS cumulative_sum
   FROM employees;
   ```

#### 示例

以下是一个实际的例子，展示了使用开窗函数计算部门内员工工资的累计和及排名：

```sql
SELECT 
    department, 
    name, 
    salary, 
    SUM(salary) OVER (PARTITION BY department ORDER BY salary DESC) AS department_cumulative_salary,
    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS salary_rank
FROM employees;
```

这个查询会返回每个员工在其部门内的工资累计和以及工资的排名。





### sql注入

SQL 注入是指你通过客户端向应用程序输入数据来插入或注入一个 SQL 查询。

成功的攻击使攻击者能够访问数据库中的敏感数据，修改数据库数据，可能关闭数据库或发出其他管理员命令，恢复文件内容，以及偶尔向操作系统发出命令。

这种类型的攻击相对容易检测和利用，因此迅速修复任何易受攻击的系统尤为重要。



#### SQL 注入如何工作

当数据从不可信任的来源进入程序，并被用于动态构建 SQL 查询时，就会发生 SQL 注入。

因为 **SQL 无法区分控制平面和数据平面**，攻击者可以在数据输入中插入元字符（一种不被解释为数据的字符，例如下划线 _，在 SQL 中，它会被解释为单个字符的通配符），然后在控制平面中输入 SQL 命令。

例如，在下面的漫画中，如果字符串 `Robert'); DROP TABLE Students;–-` 被输入到一个请求 studentName 的查询中，那么查询将变为以下内容：

```
AND studentName = 'Robert';
DROP TABLE Students;
--'
```

DROP TABLE 命令用于删除一个表及其所有行数据，而一对连字符（hyphens）告诉大多数数据库服务器，该语句的剩余部分应被视为注释（这使得服务器可以忽略修改后的查询中留下的尾部）。



许多数据库服务器允许一次执行多个查询，只要它们用分号隔开。如果允许这样做，这种攻击类型就允许攻击者对数据库执行多个命令（有些数据库服务器，包括 Oracle，不允许这种执行方式）。

**防止 SQL 注入实际上相当简单 - 要么不允许动态查询，要么阻止包含恶意 SQL 的用户输入影响查询逻辑。**

#### 其他类型的 SQL 注入

1. 基于错误的 SQL 注入

   攻击者依赖于数据库的详细错误信息来了解数据库结构。为了防止这种情况，只应显示通用错误信息。

2. 盲注 SQL 注入

   当应用程序容易受到 SQL 注入攻击时，但只显示通用错误信息（而不是详细的错误信息或查询结果）。

3. 一种访问信息的方法是使用 true/false 查询并一次提取一个问题的信息。另一种选择是发送一个命令，要求数据库在返回响应之前等待特定的时间。

   根据数据库返回错误信息所需的时间长短，攻击者可以推断命令是返回 true 还是 false。

4. UNION SQL 注入

   利用 UNION 操作符从数据库中的多个表中检索数据。

5. 带外 SQL 注入

   相对罕见，但当攻击者无法在提交命令的同一通道中接收到响应时会发生。

   相反，它依赖于服务器使用另一种协议（如 HTTP 或 DNS）来向攻击者的查询传递响应的能力。




#### 如何防止 SQL 注入攻击

**防止 SQL 注入实际上相当简单 - 要么不允许动态查询，要么阻止包含恶意 SQL 的用户输入影响查询逻辑。**

##### 预处理语句（带参数化查询）

参数化查询要求开发者定义所有 SQL 代码并稍后将每个参数传递给查询。然后，数据库可以区分代码和数据，而不受用户输入的影响。

例如，如果攻击者输入名字 `Robert'); DROP TABLE Students;–-` ，参数化查询将不再容易受到攻击，而是寻找一个与整个字符串 `Robert'); DROP TABLE Students;–-` 完全匹配的名字。

预处理语句的优点是 SQL 代码保留在应用程序内，使其（大部分）独立于数据库。

在极少数情况下，这可能会影响性能。如果确实如此，开发者需要验证所有数据，或者使用针对数据库的转义程序转义所有用户提供的输入。



##### 存储过程

存储过程是带参数的预创建 SQL 语句，不包括任何动态 SQL 生成（可以做，但不应该做）。为了设置存储过程，开发者需要为所需的输入构建带参数的 SQL 语句。

存储过程和预处理语句之间的区别在于，**存储过程是在数据库中定义和存储的，但是从应用程序中调用的。**

此外，由于存储过程在某些 DBMS 中需要执行权限（默认情况下不可用），因此创建一个具有最少权限的单独帐户而不是授予所有者访问权限非常重要。



##### 白名单输入验证

白名单输入验证将外部输入与一组已知的、批准的输入进行比较，对于不匹配的输入将失败。这只应用于不允许绑定变量的情况（SQL 语句中实际值的占位符）。

白名单输入验证还可以作为在将输入传递给查询之前检测输入的备选方案。



##### 输入验证和清洗

对用户输入进行严格的验证，确保数据符合预期格式，比如使用正则表达式。

转义所有用户提供的输入

只有在前面的选项不可行时，才应使用此方法，因为它无法防止所有 SQL 注入。仅对无法重写以使用前面推荐方法的遗留代码使用它。遗憾的是，这是一个非常针对特定数据库的实现。

每个 DBMS 都支持字符转义方案。如果使用正确的方案转义所有用户输入，DBMS 将能够区分输入和开发者编写的 SQL 代码。



##### 最小权限

最小权限不是防御 SQL 注入的手段，而是一种限制任何攻击可能造成的损害的方法。

只授予确切需要的访问权限。

例如，如果一个帐户只需要只读访问权限，请确保它只对所需的表（甚至是表的一部分）具有只读访问权限。如果可以的话，避免授予数据库帐户创建或删除访问权限。每个用户/应用程序应该有一个单独的帐户。

此外，审查数据库管理系统（DBMS）运行所在的操作系统帐户的权限。默认情况下，许多帐户具有非常强大的权限 - 将其更改为更合适的权限。



##### WAF

**启用Web应用防火墙（WAF）**：使用WAF可以监测和过滤恶意请求。



### 慢sql优化

优化慢 SQL 查询是提高数据库性能和响应速度的重要步骤。以下是一些常见的优化方法：

#### 1. **索引优化**
   - **创建合适的索引**：确保在常用的查询条件（如 `WHERE`、`JOIN`、`ORDER BY`）的字段上创建索引，以加速数据检索。
   - **避免过多索引**：虽然索引可以加快查询速度，但过多的索引会降低插入、更新和删除操作的性能。要找到平衡点。

#### 2. **查询重写**
   - **简化查询**：去除不必要的字段，避免使用 `SELECT *`，只选择需要的列。
   - **使用子查询或联接**：有时将复杂的查询拆分为多个简单的查询，或者使用 `JOIN` 代替子查询，能提高效率。
   - **避免使用函数**：在 `WHERE` 子句中避免对列使用函数，这样可能会导致索引失效。

#### 3. **分析执行计划**
   - 使用 `EXPLAIN` 语句查看查询的执行计划，了解查询是如何执行的。识别瓶颈，找到需要优化的部分。

#### 4. **数据库结构优化**
   - **范式化与反范式化**：根据应用需求考虑对数据库进行范式化或反范式化，以优化查询性能。
   - **分区**：对于大表，可以考虑使用分区，以减少扫描的数据量。

#### 5. **合理使用缓存**
   - 使用查询缓存或结果缓存来减少重复查询的开销。

#### 6. **连接优化**
   - **减少连接数量**：尽量避免复杂的多表连接，尤其是大表的连接。
   - **使用合适的连接类型**：确保使用适当的连接类型（如内连接、外连接）以提高性能。

#### 7. **定期维护**
   - **重建索引**：定期重建和更新索引，保持索引的健康状态。
   - **清理和优化表**：定期清理不必要的数据，使用 `OPTIMIZE TABLE` 命令来整理表的碎片。

#### 8. **配置优化**
   - 根据业务需要调整数据库配置参数（如缓存大小、连接池大小等），以提高性能。

#### 9. **监控和分析**
   - 使用数据库性能监控工具，定期检查慢查询日志，找到并优化常见的慢查询。

#### 10. **代码级优化**
   - 在应用层考虑减少查询次数，使用批量操作，避免在循环中执行查询。

#### 示例

假设有一个慢查询：

```sql
SELECT * FROM orders WHERE YEAR(order_date) = 2023;
```

**优化方法**：
1. 在 `order_date` 列上创建索引。
2. 将查询重写为：

```sql
SELECT * FROM orders WHERE order_date >= '2023-01-01' AND order_date < '2024-01-01';
```

3. 使用 `EXPLAIN` 观察查询性能，进行进一步优化。

通过这些方法，可以有效提高 SQL 查询的性能，减少慢查询的发生。



### 执行顺序

```
SELECT ... FROM ... WHERE ... GROUP BY ... HAVING ... ORDER BY ... LIMIT ...
```

但实际的执行顺序是不同的。MySQL 的查询处理过程会按以下步骤依次执行，每一步都生成一个中间（虚拟）结果，供下一步使用：

1. FROM 子句
   - 解析要查询的表，并执行表之间的 JOIN 操作。
2. ON 子句
   - 在 JOIN 过程中，先根据 ON 中的条件筛选出符合连接条件的行。
3. JOIN 操作
   - 将多个数据源结合成一个初始结果集（包括 INNER JOIN、LEFT/RIGHT OUTER JOIN 等）。
4. WHERE 子句
   - 对 FROM 得到的结果集进行过滤，保留满足条件的行。
5. GROUP BY 子句
   - 根据指定的列对数据进行分组，为后续的聚合函数计算做准备。
6. HAVING 子句
   - 在分组后，对每个组进行过滤（常用于对聚合函数结果进行条件筛选）。
7. SELECT 子句
   - 根据 SELECT 中指定的列或表达式，计算出最终的结果集（此时可以使用在 GROUP BY 后定义的别名）。
8. DISTINCT 关键字
   - 如果指定了 DISTINCT，则在结果集中去除重复行。
9. ORDER BY 子句
   - 对结果集进行排序，排序操作通常比较耗费资源，所以最好在必要时使用。
10. LIMIT 子句
    - 最后，根据 LIMIT 限制返回的行数。

因此，MySQL 实际的执行顺序为：

**FROM → ON → JOIN → WHERE → GROUP BY → HAVING → SELECT → DISTINCT → ORDER BY → LIMIT**

这种顺序说明了为什么在 WHERE 子句中不能使用 SELECT 子句中的别名，因为 SELECT 在最后才被执行，而 GROUP BY 和 HAVING 可以使用聚合函数及别名。了解这一点有助于我们编写更高效、逻辑更清晰的 SQL 语句。



------

## 范式

数据库范式（Database Normalization）是一组设计规则，旨在组织关系型数据库中的数据，以减少冗余、避免数据不一致和更新异常。通过对数据进行规范化，可以使数据结构更加清晰、维护更加容易。常见的范式包括：

### 1. 第一范式（1NF）

- **要求**：表中的每个字段都必须是不可再分的原子值（即不可拆分的最小数据单位）。
- **目的**：消除重复的组或数组。

### 2. 第二范式（2NF）

- **要求**：在满足1NF的基础上，每个非主属性都必须完全依赖于主键（不能只依赖于主键的一部分）。
- **目的**：消除部分依赖，确保每个属性都依赖于整个主键（适用于复合主键的情况）。

### 3. 第三范式（3NF）

- **要求**：在满足2NF的基础上，消除非主属性之间的传递依赖。
- **目的**：确保每个非主属性直接依赖于主键，而不是依赖于其他非主属性。

### 4. BCNF（Boyce-Codd Normal Form）

- **要求**：比3NF更严格，要求每个决定因素（决定其他属性的属性集合）必须是候选键。
- **目的**：解决3NF中可能存在的某些特殊依赖问题，是一种更为严格的3NF。

### 5. 第四范式（4NF）与第五范式（5NF）

- **4NF**：在BCNF的基础上，处理多值依赖问题，确保一个表中不会存在非平凡的多值依赖。
- **5NF**：也称为投影-连接范式（PJ/NF），解决由于连接依赖导致的数据冗余问题。

------

### 总结

通过采用数据库范式：

- **减少数据冗余**：避免在多个地方存储相同的数据，从而节省存储空间并降低数据不一致的风险。
- **避免更新异常**：在插入、更新、删除操作时，由于数据分散在多个表中而不容易出现数据不一致或“更新异常”。
- **提高数据完整性**：通过分解表结构，使数据库更符合实际业务逻辑和数据依赖关系，从而保证数据的准确性。

不过，范式化虽然能提升数据一致性和完整性，但过度规范化有时也会导致查询时需要多表连接，从而影响性能。实际设计中，通常会在规范化和性能之间寻找一个平衡点，有时会进行适当的反规范化以提高查询效率。

参考资料：
 citeturn3search0
 citeturn3search2

------



## 索引

### 定义

**索引是数据的目录**。

在创建表时，InnoDB 存储引擎会根据不同的场景选择不同的列作为索引：

- 如果有主键，默认会使用主键作为聚簇索引的索引键（key）；
- 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键（key）；
- 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键（key）；

其它索引都属于辅助索引（Secondary Index），也被称为二级索引或非聚簇索引。**创建的主键索引和二级索引默认使用的是 B+Tree 索引**。

### 效率

**执行效率从低到高的顺序为**：

- All（全表扫描）；
- index（全索引扫描）；
- range（索引范围扫描）；
- ref（非唯一索引扫描）；
- eq_ref（唯一索引扫描）；
- const（结果只有一条的主键或唯一索引扫描）。

 const 类型和 eq_ref 都使用了主键或唯一索引，不过这两个类型有所区别，**const 是与常量进行比较，查询效率会更快，而 eq_ref 通常用于多表联查中**。

**需要尽量让 SQL 查询可以使用到 range 这一级别及以上的 type 访问方式**。



------



### B+Tree

B+Tree 是一种**多叉树**，**叶子节点才存放数据，非叶子节点只存放索引**，而且每个节点里的数据是**按主键顺序存放**的。每一层父节点的索引值都会出现在下层子节点的索引值中，因此在叶子节点中，包括了所有的索引值信息，并且每一个叶子节点都有两个指针，分别指向下一个叶子节点和上一个叶子节点，形成一个双向链表。

数据库的索引和数据都是存储在硬盘的，我们可以把读取一个节点当作一次磁盘 I/O 操作。

B+Tree 存储**千万级的数据只需要 3-4 层高度**就可以满足，这意味着从千万级的表查询目标数据最多需要 3-4 次磁盘 I/O，所以B+Tree 相比于 B 树和二叉树来说，最大的优势在于查询效率很高，因为即使在数据量很大的情况，查询一个数据的磁盘 I/O 依然维持在 3-4次。

**例外：二级索引的 B+Tree 的叶子节点存放的是主键值，而不是实际数据。即key为索引列，val为主键值**

​          

InnoDB 里的 B+ 树中的**每个节点都是一个数据页**，只有叶子节点（最底层的节点）才存放了数据，非叶子节点（其他上层节）仅用来存放目录项作为索引。

**定位记录所在哪一个页时，通过二分法快速定位到包含该记录的页。定位到该页后，又会在该页内进行二分法快速定位记录所在的分组（槽号），最后在分组内进行遍历查找**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408262040597.png" alt="7c635d682bd3cdc421bb9eea33a5a413" style="zoom: 67%;" />

#### B 树

B+ 树与 B 树差异的点，主要是以下这几点：

- 叶子节点（最底部的节点）才会存放实际数据（索引+记录），非叶子节点只会存放索引；
- 所有索引都会在叶子节点出现，叶子节点之间构成一个有序链表；B 树没有将所有叶子节点用链表串联起来的结构
- 非叶子节点的索引也会同时存在在子节点中，并且是在子节点中所有索引的最大（或最小）。
- 非叶子节点中有多少个子节点，就有多少个索引；

#### 好处

1. B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少。
2. B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 B+ 树在插入、删除的效率都更高，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化；
3. B+ 树叶子节点之间用链表连接了起来，有利于范围查询，而 B 树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。



### 分类

1. 按「数据结构」分类：**B+tree索引、Hash索引、Full-text索引**。

   - **B-Tree 索引：** 是一种树状结构，**适用于范围查询和排序**。常见于许多数据库管理系统，如MySQL、PostgreSQL等。
   - **哈希索引：** 使用哈希函数将键值映射到一个哈希表中的特定位置。适用于等值查询，但不适用于范围查询。例如，MySQL中的Memory引擎支持哈希索引。
   - **全文索引（Full-Text Index）：** 用于全文搜索，允许对文本字段进行高效的文本搜索操作。常见于支持全文搜索的数据库，如MySQL的FULLTEXT索引。

2. 按「物理存储」分类：**聚簇索引（主键索引）、二级索引（辅助索引）**。

   - **聚集索引（Clustered Index）：** 在数据库表中按照索引的顺序**物理地重新排列数据**。**一个表只能有一个**聚集索引，因为数据实际上按照该索引的顺序进行排序。

     **如果表有主键约束，该约束会默认作为聚集索引。**

   - **非聚集索引（Non-Clustered Index）：** 索引中存储的是指向实际数据行的指针，而不是对数据行本身进行排序。一个表可以有多个非聚集索引。

   - **二级索引：**并不直接影响主键的顺序，而是维护了一个指向主键的映射关系。二级索引的本质是一个辅助数据结构（通常是 B+ 树或哈希表），它存储了**索引字段的值**以及指向该记录**主键的指针**。通过该映射，可以快速定位到主键，从而快速找到记录。

3. 按「字段特性」分类：**主键索引、唯一索引、普通索引、前缀索引**。

   - **普通索引**：允许重复的值。

   - **主键索引**：主键索引就是建立在主键字段上的索引，通常在创建表的时候一起创建，一张表最多只有一个主键索引，索引列的值不允许有空值。

   - **唯一索引（Unique Index）：** 索引列的值必须是唯一的，不允许重复值。

   - **复合索引（Composite Index）：** 由多个列组成的索引，适用于涉及多个列的查询条件。复合索引能够加快多列条件查询的速度。

   - **覆盖索引（Covering Index）：** 索引包含了查询所需的所有信息，查询的结果可以直接从索引中获取，无需访问实际数据行。也就是只需要查一个 B+Tree 就能找到数据

   - **前缀索引**：

     - 前缀索引是指对字符类型字段的前几个字符建立的索引，而不是在整个字段上建立的索引，前缀索引可以建立在字段类型为 char、 varchar、binary、varbinary 的列上。

       使用前缀索引的目的是为了减少索引占用的存储空间，提升查询效率。

4. 按「字段个数」分类：**单列索引、联合索引**。

   - 联合索引：使用联合索引时，存在**最左匹配原则**，也就是按照最左优先的方式进行索引的匹配。

   - `(a, b, c)` 联合索引，是先按 a 排序，在 a 相同的情况再按 b 排序，在 b 相同的情况再按 c 排序。所以，**b 和 c 是全局无序，局部相对有序的**

   - **利用索引的前提是索引里的 key 是有序的**。

   - 

     联合索引的最左匹配原则**，在遇到范围查询（如 >、<）的时候，就会停止匹配**，也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。  这时可能会使用**索引下推优化**

     **注意，对于 >=、<=、BETWEEN、like 前缀匹配的范围查询，并不会停止匹配**

     - 注意为双向链表
   - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408261500854.png" alt="_E8_81_94_E5_90_88_E7_B4_A2_E5_BC_95.drawio" style="zoom:50%;" />



### 索引下推/回表

- 索引下推：通过在索引扫描阶段提前应用WHERE条件，减少回表次数，优化查询性能。

- **回表** 是在索引无法提供查询所需的所有字段时，数据库需要额外访问主表以获取完整数据的过程。

  



### 什么时候需要 / 不需要创建索引？

索引最大的好处是提高查询速度，但是索引也是有缺点的，比如：

- 需要占用物理空间，数量越大，占用空间越大；
- 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增大；
- 会降低表的增删改的效率，因为每次增删改索引，B+ 树为了维护索引有序性，都需要进行动态维护。



#### 什么时候适用索引？

- 字段有唯一性限制的，比如商品编码；
- 经常用于 `WHERE` 查询条件的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。
- 经常用于 `GROUP BY` 和 `ORDER BY` 的字段，这样在查询的时候就不需要再去做一次排序了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的。

#### 什么时候不需要创建索引？

- `WHERE` 条件，`GROUP BY`，`ORDER BY` 里用不到的字段，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的。
- 字段中存在大量重复数据，不需要创建索引，比如性别字段，只有男女，如果数据库表中，男女的记录分布均匀，那么无论搜索哪个值都可能得到一半的数据。在这些情况下，还不如不要索引，因为 MySQL 还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描。
- 表数据太少的时候，不需要创建索引；
- **经常更新的字段**不用创建索引，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于要维护 B+Tree的有序性，那么就需要频繁的重建索引，这个过程是会影响数据库性能的



### 优化索引

1. 前缀索引优化；

   - 使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。

     不过，前缀索引有一定的局限性，例如：

     - order by 就无法使用前缀索引；
     - 无法把前缀索引用作覆盖索引；

2. 覆盖索引优化；

3. 主键索引最好是自增的；

   - **如果我们使用自增主键**，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次**插入一条新记录，都是追加操作，不需要重新移动数据**，因此这种插入数据的方法效率非常高。

     **如果我们使用非自增主键**，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为**页分裂**。**页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率**。

     另外，主键字段的长度不要太大，因为**主键字段长度越小，意味着二级索引的叶子节点越小（二级索引的叶子节点存放的数据是主键值），这样二级索引占用的空间也就越小**

4. ### 索引最好设置为 NOT NULL

   - 第一原因：索引列存在 NULL 就会导致优化器在做索引选择的时候更加复杂，更加难以优化，因为可为 NULL 的列会使索引、索引统计和值比较都更复杂，比如进行索引统计时，count 会省略值为NULL 的行。
   - 第二个原因：NULL 值是一个没意义的值，但是它会占用物理空间，所以会带来的存储空间的问题，因为 InnoDB 存储记录的时候，如果表中存在允许为 NULL 的字段，那么[行格式 (opens new window)](https://xiaolincoding.com/mysql/base/row_format.html#innodb-行格式有哪些)中**至少会用 1 字节空间存储 NULL 值列**

5. 防止索引失效；

   - 索引失效的情况：

     1. 当我们使用左或者左右模糊匹配的时候，也就是 `like %xx` 或者 `like %xx%`这两种方式都会造成索引失效；

        1. 对于like %xx，当是**覆盖索引**时（所有字段都是索引），走的还是二级索引，因为二级索引树的记录东西很少，就只有「索引列+主键值」，而聚簇索引记录的东西会更多，比如聚簇索引中的叶子节点则记录了主键值、事务 id、用于事务和 MVCC 的回滚指针以及所有的剩余列。

     2. 当我们在查询条件中对索引列做了**计算、函数、类型转换**操作，这些情况下都会造成索引失效；

        - 因为索引保存的是索引字段的原始值，而不是表达式计算后的值，所以无法走索引
        - 从 MySQL 8.0 开始，索引特性增加了函数索引，即可以针对函数计算后的值建立一个索引，也就是说该索引的值是函数计算后的值，所以就可以通过扫描索引来查询数据。

     3. 索引隐式类型转换

        - **MySQL 在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较**。

          id字段是索引:
          
          ```sql
          select * from t_user where id < "1300000001";
          ```

           phone 字段为字符串，所以 MySQL 要会自动把字符串转为数字，所以这条语句相当于：
          
          ```sql
          select * from t_user where CAST(id AS signed int) < 1300000001;
          ```
          
          **CAST 函数是作用在了 id字段，而 id字段是索引，也就是对索引使用了函数！而前面我们也说了，对索引使用函数是会导致索引失效的**。 
          
          而且字符串比较和数字比较是结果不同的，例如"90">"100" ，如果id是整形，mysql会把100转成整形比较，那90<100   

     4. 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
     
     5. 在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效
     
        - 都设为索引

6. Using filesort ：当查询语句中包含 group by 操作，而且无法利用索引完成排序操作的时候， 这时不得不选择相应的排序算法进行，甚至可能会通过文件排序，效率是很低的，所以要避免这种问题的出现。

   Using temporary：使了用临时表保存中间结果，MySQL 在对查询结果排序时使用临时表，常见于排序 order by 和分组查询 group by。效率低，要避免这种问题的出现。



### shell命令

```SQL
创建索引：
创建单列索引：
CREATE INDEX index_name ON table_name (column_name DESC); //降序 （默认升序）
创建唯一索引：
CREATE unique INDEX index_name ON table_name (column_name DESC);
创建复合索引：
CREATE  INDEX index_name ON table_name (column1, column2); 
删除索引：
DROP INDEX index_name ON table_name;
修改索引：先删除旧的索引，然后再创建一个新的索引

ALTER TABLE mytable   //使用alert
ADD CONSTRAINT idx_unique_column1 UNIQUE (column1);  //constraint指定索引名
19) 查询至少订购了R1所订购的书籍的读者的编号
SELECT distinct rno
FROM od READ1
WHERE
NOT EXISTS (
SELECT *
FROM od READ2
WHERE READ2.rno='R1' AND /*因为这里选择了第二个，所以下面必须是第一个和第三个*/
NOT EXISTS (
SELECT *
FROM od READ3
WHERE
READ3.rno=READ1.rno AND
READ3.bno =READ2.bno
) )
```



## 事务

### 介绍

我们在执行执行一条“增删改”语句的时候，虽然没有输入 begin 开启事务和 commit 提交事务，但是 MySQL 会**隐式开启事务**来执行“增删改”语句的，执行完就自动提交事务的，这样就保证了执行完“增删改”语句后，我们可以及时在数据库表看到“增删改”的结果了。

执行一条语句是否自动提交事务，是由 `autocommit` 参数决定的，默认是开启。所以，执行一条 update 语句也是会使用事务的。

事务是由 MySQL 的引擎来实现的

### ACID

事务ACID 原则：

- **原子性（Atomicity）**：**一个事务中的所有操作，要么全部完成，要么全部不完成，**不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样，就好比买一件商品，购买成功时，则给商家付了钱，商品到手；购买失败时，则商品在商家手中，消费者的钱也没花出去。
- **隔离性（Isolation）**：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以**防止多个事务并发执行时由于交叉执行而导致数据的不一致**，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。也就是说，消费者购买商品这个事务，是不影响其他消费者购买的。
- **持久性（Durability）**：**事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。**
- **一致性（Consistency）**：**是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。**比如，用户 A 和用户 B 在银行分别有 800 元和 600 元，总共 1400 元，用户 A 给用户 B 转账 200 元，分为两个步骤，从 A 的账户扣除 200 元和对 B 的账户增加 200 元。一致性就是要求上述步骤操作后，最后的结果是用户 A 还有 600 元，用户 B 有 800 元，总共 1400 元，而不会出现用户 A 扣除了 200 元，但用户 B 未增加的情况（该情况，用户 A 和 B 均为 600 元，总共 1200 元）。

InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？

- 原子性是通过 undo log（回滚日志） 来保证的；
- 持久性是通过 redo log （重做日志）来保证的；
- 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的；
- 一致性则是通过持久性+原子性+隔离性来保证；



### 并行事务会引发什么问题？

- 脏读：读到其他事务未提交的数据；
- 不可重复读：前后读取的数据不一致；
- 幻读：前后读取的记录数量不一致。

严重性排序如下：

![d37bfa1678eb71ae7e33dc8f211d1ec1](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408270928687.png)



### 事务的隔离级别

- **读未提交（\*read uncommitted\*）**，指一个事务还没提交时，它做的变更就能被其他事务看到；
- **读提交（\*read committed\*）**，指一个事务提交之后，它做的变更才能被其他事务看到；
- **可重复读（\*repeatable read\*）**，指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，**MySQL InnoDB 引擎的默认隔离级别**；
- **串行化（\*serializable\* ）**；会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行；

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408270929249.png" alt="4e98ea2e60923b969790898565b4d643" style="zoom:50%;" />

四种隔离级别具体是如何实现的呢？

- 对于「读未提交」隔离级别的事务来说，因为可以读到未提交事务修改的数据，所以**直接读取**最新的数据就好了；

- 对于「读提交」和「可重复读」隔离级别的事务来说， Read View ==数据快照，

  「读提交」隔离级别是在「**每个语句执行前**」都会重新生成一个 Read View，

  「可重复读」隔离级别是「**启动事务时**」生成一个 Read View，然后整个事务期间都在用这个 Read View**。

  - 注意，执行「开始事务」命令，并不意味着启动了事务。在 MySQL 有两种开启事务的命令，分别是：

    - 第一种：begin/start transaction 命令；
      - 在执行这个命令后，执行了第一条 select 语句，才是事务真正启动的时机；

    - 第二种：start transaction with consistent snapshot 命令；
      - 执行了 start transaction with consistent snapshot 命令，就会马上启动事务。
    

- 对于「串行化」隔离级别的事务来说，通过加**读写锁**的方式来避免并行访问；



### 幻读

#### 避免幻读现象

MySQL InnoDB 引擎的默认隔离级别虽然是**「可重复读**」，但是它很大程度上**避免幻读现象**（并不是完全解决了），不建议将隔离级别升级为串行化，因为这会导致数据库并发时性能很差

- 针对**快照读**（普通 select 语句） ，是**通过 MVCC 方式解决了幻读**，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。

  在执行第一个查询语句后，会创建一个 Read View，**后续的查询语句利用这个 Read View，通过这个 Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的**，

- 针对**当前读**（select ... for update 等语句），是**通过 next-key lock（记录锁+间隙锁）方式解决了幻读**，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。



#### 仍会幻读

##### 1

在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成了一个 ReadView，之后事务 B 向表中新插入了一条 id = 5 的记录并提交。接着，事务 A 对 id = 5 这条记录进行了更新操作，在这个时刻，这条新记录的 trx_id 隐藏列的值就变成了事务 A 的事务 id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202410091020161.png" alt="_E5_B9_BB_E8_AF_BB_E5_8F_91_E7_94_9F.drawio" style="zoom:50%;" />

##### 2

- T1 时刻：事务 A 先执行「快照读语句」：select * from t_test where id > 100 得到了 3 条记录。
- T2 时刻：事务 B 往插入一个 id= 200 的记录并提交；
- T3 时刻：事务 A 再执行「当前读语句」 select * from t_test where id > 100 for update 就会得到 4 条记录，此时也发生了幻读现象。

即另一个事务插入了一个不存在的记录，**要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句**，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。

##### 3.delete操作





### Read View 

Read View 有四个重要的字段：

- m_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的**事务 id 列表**，注意是一个列表，**“活跃事务”指的就是，启动了但还没提交的事务**。
- min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 **id 最小的事务**，也就是 m_ids 的最小值。
- max_trx_id ：这个并不是 m_ids 的最大值，而是**创建 Read View 时当前数据库中应该给下一个事务的 id 值**，也就是全局事务中最大的事务 id 值 + 1；
- creator_trx_id ：指的是**创建该 Read View 的事务的事务 id**。

配合聚簇索引记录中的两个隐藏列。

- trx_id，当一个事务对某条聚簇索引记录进行改动时，就会**把该事务的事务 id 记录在 trx_id 隐藏列里**；
- roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后**这个隐藏列是个指针，指向每一个旧版本记录**，于是就可以通过它找到修改前的记录。



在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408270946208.png" alt="ReadView.drawio" style="zoom:50%;" />

一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：

- 如果记录的 trx_id 值小于 Read View 中的 `min_trx_id` 值，表示这个版本的记录是在创建 Read View **前**已经提交的事务生成的，所以该版本的记录对当前事务**可见**。
- 如果记录的 trx_id 值大于等于 Read View 中的 `max_trx_id` 值，表示这个版本的记录是在创建 Read View **后**才启动的事务生成的，所以该版本的记录对当前事务**不可见**。
- 如果记录的 trx_id 值在 Read View 的min_trx_id和max_trx_id之间，需要判断 trx_id 是否在 m_ids 列表中：
  - 如果记录的 trx_id **在** `m_ids` 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务**不可见**。
  - 如果记录的 trx_id **不在** `m_ids`列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务**可见**。

**这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）**

这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。而是沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录



## 锁

表锁和行锁是满足读读共享、读写互斥、写写互斥的。



### 全局锁

全局锁主要应用于做**全库逻辑备份**，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。

```sql
flush tables with read lock

释放全局锁，则要执行这条命令：
unlock tables
```

> 既然备份数据库数据的时候，使用全局锁会影响业务，那有什么其他方式可以避免？

有的，如果数据库的引擎支持的事务支持**可重复读的隔离级别**，那么在备份数据库之前先开启事务，会先创建 Read View，然后整个事务执行期间都在用这个 Read View，而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新操作。



### 表级锁

当执行**插入、更新、删除**操作，需要先对表加上「意向独占锁」，然后对该记录加独占锁。

1. 表锁；

   - ```sql
     //表级别的共享锁，也就是读锁；
     lock tables t_student read;
     
     //表级别的独占锁，也就是写锁；
     lock tables t_stuent write;
     
     unlock tables
     ```

2. 元数据锁（MDL）;

   - MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个表结构做了变更。MDL 是在事务提交后才会释放，这意味着**事务执行期间，MDL 是一直持有的**。

   - 对一张表进行 CRUD 操作时，加的是 **MDL 读锁**；

   - 对一张表做结构变更操作的时候，加的是 **MDL 写锁**；

   - > ABC，线程 C 因为申请不到 MDL 写锁，导致后续的申请**读锁**的查询操作也会被阻塞

     这是因为申请 MDL 锁的操作会形成一个队列，队列中**写锁获取优先级高于读锁**，一旦出现 MDL 写锁等待，会阻塞后续该表的所有 CRUD 操作。

     所以为了能安全的对表结构进行变更，在对表结构变更前，先要看看数据库中的长事务，是否有事务已经对表加上了 MDL 读锁，如果可以考虑 kill 掉这个长事务，然后再做表结构的变更。

3. 意向锁；

   - 当执行插入、更新、删除操作，需要先对表加上「意向独占锁」，然后对该记录加独占锁。
     - 意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（\*lock tables ... read\*）和独占表锁（\*lock tables ... write\*）发生冲突。
   - **意向锁的目的是为了快速判断表里是否有记录被加锁**，不然就需要遍历表里所有记录，
   - 在使用 InnoDB 引擎的表里对某些记录加上「共享锁」之前，需要先在表级别加上一个「意向共享锁」；
   - 在使用 InnoDB 引擎的表里对某些纪录加上「独占锁」之前，需要先在表级别加上一个「意向独占锁」；

4. AUTO-INC 锁

   - 主键通常都会设置成自增的

   - **执行完插入语句后就会立即释放**。

     -  在 MySQL 5.1.22 版本开始，InnoDB 存储引擎提供了一种**轻量级的锁**来实现自增。

       一样也是在插入数据的时候，会为被 `AUTO_INCREMENT` 修饰的字段加上轻量级锁，**然后给该字段赋值一个自增的值，就把这个轻量级锁释放了，而不需要等待整个插入语句执行完后才释放锁**。

       InnoDB 存储引擎提供了个 innodb_autoinc_lock_mode 的系统变量，是用来控制选择用 AUTO-INC 锁，还是轻量级的锁。

       - 当 innodb_autoinc_lock_mode = 0，就采用 AUTO-INC 锁，语句执行结束后才释放锁；
       - 当 innodb_autoinc_lock_mode = 2，就采用轻量级锁，申请自增主键后就释放锁，并不需要等语句执行后才释放。
       - 当 innodb_autoinc_lock_mode = 1：
         - 普通 insert 语句，自增锁在申请之后就马上释放；
         - 类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放；

     - **当 innodb_autoinc_lock_mode = 2 时，并且 binlog_format = row，既能提升并发性，又不会出现数据一致性问题**

       - 当 innodb_autoinc_lock_mode = 2 是性能最高的方式，但是当搭配 binlog 的日志格式是 statement 一起使用的时候，在「主从复制的场景」中会发生**数据不一致的问题**。





### 行级锁

#### 类型

InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁。

**加锁的对象是索引，加锁的基本单位是 next-key lock**，但是，**在能使用记录锁或者间隙锁就能避免幻读现象的场景下， next-key lock 就会退化成记录锁或间隙锁**。

对二级索引进行锁定读查询的时候，因为存在两个索引（二级索引和主键索引），所以两个索引都会加锁。



普通的 select 是不会加行级锁的，**普通的 select 语句是利用 MVCC 实现一致性读，是无锁的。**

加锁语句：

1. **update 和 delete 操作都会加行级锁，且锁的类型都是独占锁(X型锁)**。
2. 查询会加锁的语句称为**锁定读**。

```sql
//对读取的记录加共享锁
select ... lock in share mode;

//对读取的记录加独占锁
select ... for update;
```

上面这两条语句必须在一个事务中，**因为当事务提交了，锁就会被释放**，所以在使用这两条语句的时候，要加上 begin、start transaction 或者 set autocommit = 0。



行级锁的类型主要有三类：

- Record Lock，记录锁，也就是仅仅把一条记录锁上；

  - 记录锁是有 S 锁和 X 锁之分的

- Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；

  - **间隙锁是前开后开区间**
  - **间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的**

- Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。

  - **next-key lock 是前开后闭区间**，

  - **在能使用记录锁或者间隙锁就能避免幻读现象的场景下， next-key lock 就会退化成记录锁或间隙锁**

  - 有一点要注意的是，在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了，这是挺严重的问题。

  - ### 插入意向锁

    - 一个事务在插入一条记录的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock 也包含间隙锁）。

      如果有的话，插入操作就会发生**阻塞**，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻），在此期间会生成一个**插入意向锁**，表明有事务想在某个区间插入新记录，但是现在处于等待状态。

    - 插入意向锁名字虽然有意向锁，但是它并**不是意向锁，它是一种特殊的间隙锁，属于行级别锁**。

    - 如果说间隙锁锁住的是一个区间，那么「插入意向锁」锁住的就是一个点。因而从这个角度来说，插入意向锁确实是一种特殊的间隙锁。



#### 行级锁加锁策略

##### 等值查询

唯一索引等值查询：

- 当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会**退化成「记录锁」**。
- 当查询的记录是「不存在」的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会**退化成「间隙锁」**。

非唯一索引等值查询：

- 当查询的记录「存在」时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是非唯一索引等值查询的过程是一个扫描的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后**在扫描的过程中，对扫描到的二级索引记录加的是 next-key 锁，而对于第一个不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁**。
- 当查询的记录「不存在」时，**扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁**。



##### 范围查询

- 唯一索引在满足一些条件的时候，索引的 next-key lock 退化为间隙锁或者记录锁。
- 非唯一索引范围查询，索引的 next-key lock 不会退化为间隙锁和记录锁。

以避免幻读角度去分析，容易理解

还有一件很重要的事情，在线上在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，**如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了**，这是挺严重的问题。



唯一索引（主键索引）加锁的流程图如下。（*注意这个流程图是针对「主键索引」的，如果是二级索引的唯一索引，除了流程图中对二级索引的加锁规则之外，还会对查询到的记录的主键索引项加「记录锁」，流程图没有提示这一个点，所以在这里用文字补充说明下*）

![_E5_94_AF_E4_B8_80_E7_B4_A2_E5_BC_95_E5_8A_A0_E9_94_81_E6_B5_81_E7_A8_8B](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272009110.jpeg)

非唯一索引加锁的流程图：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272010214.jpeg" alt="_E9_9D_9E_E5_94_AF_E4_B8_80_E7_B4_A2_E5_BC_95_E5_8A_A0_E9_94_81_E6_B5_81_E7_A8_8B" style="zoom:67%;" />



##### update 没加索引会锁全表

**在 update 语句的 where 条件没有使用索引，就会全表扫描，于是就会对所有记录加上 next-key 锁（记录锁 + 间隙锁），相当于把整个表锁住了**。



那 update 语句的 where 带上索引就能避免全表记录加锁了吗

并不是。

**关键还得看这条语句在执行过程种，优化器最终选择的是索引扫描，还是全表扫描，如果走了全表扫描，就会对全表的记录加锁了**。



当我们要执行 update 语句的时候，确保 where 条件中带上了索引列，并且在测试机确认该语句是否走的是索引扫描，防止因为扫描全表，而对表中的所有记录加上锁。

我们可以打开 MySQL sql_safe_updates 参数，这样可以预防 update 操作时 where 条件没有带上索引列。

​	大致的意思是，当 sql_safe_updates 设置为 1 时。

update 语句必须满足如下条件之一才能执行成功：

- 使用 where，并且 where 条件中必须有索引列；
- 使用 limit；
- 同时使用 where 和 limit，此时 where 条件中可以没有索引列；

delete 语句必须满足以下条件能执行成功：

- 同时使用 where 和 limit，此时 where 条件中可以没有索引列；

如果发现即使在 where 条件中带上了列索引列，优化器走的还是全标扫描，这时我们就要使用 `force index([index_name])` 可以告诉优化器使用哪个索引。



#### Insert 语句是怎么加行级锁的？

Insert 语句在正常执行时是不会生成锁结构的，它是靠聚簇索引记录自带的 trx_id 隐藏列来作为**隐式锁**来保护记录的。

> 什么是隐式锁？

当事务需要加锁的时，如果这个锁不可能发生冲突，InnoDB会跳过加锁环节，这种机制称为隐式锁。

隐式锁是 InnoDB 实现的一种延迟加锁机制，其特点是只有在可能发生冲突时才加锁，从而减少了锁的数量，提高了系统整体性能。

隐式锁就是在 Insert 过程中不加锁，只有在特殊情况下，才会将隐式锁转换为显示锁，这里我们列举两个场景。

- 如果记录之间加有间隙锁，为了避免幻读，此时是不能插入记录的；

- 如果 Insert 的记录和已有记录存在唯一键冲突，此时也不能插入记录；

  - 如果主键索引重复，插入新记录的事务会给已存在的主键值重复的聚簇索引记录**添加 S 型记录锁**。

  - 如果唯一二级索引重复，插入新记录的事务都会给已存在的二级索引列值重复的二级索引记录**添加 S 型 next-key 锁**。



### 死锁

务 A 和事务 B 在执行完后 `select ... for update` 语句后都持有范围为`(1006,+∞]`的next-key 锁，而接下来的插入操作为了获取到插入意向锁，都在等待对方事务的间隙锁释放，于是就造成了循环等待，导致死锁。



如果两个事务分别向对方持有的间隙锁范围内插入一条记录，而插入操作为了获取到插入意向锁，都在等待对方事务的间隙锁释放，于是就造成了循环等待，满足了死锁的四个条件：**互斥、占有且等待、不可强占用、循环等待**，因此发生了死锁



#### 如何避免死锁？

死锁的四个必要条件：**互斥、不可强占用、占有且等待、循环等待**。只要系统发生死锁，这些条件必然成立，但是只要破坏任意一个条件就死锁就不会成立。

在数据库层面，有两种策略通过「打破循环等待条件」来解除死锁状态：

- **设置事务等待锁的超时时间**。当一个事务的等待时间超过该值后，就对这个事务进行回滚，于是锁就释放了，另一个事务就可以继续执行了。在 InnoDB 中，参数 `innodb_lock_wait_timeout` 是用来设置超时时间的，默认值时 50 秒。

  当发生超时后，就出现下面这个提示：

  <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281031415.png" alt="c296c1889f0101d335699311b4ef20a8" style="zoom:50%;" />

- **开启主动死锁检测**。主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑，默认就开启。

  当检测到死锁后，就会出现下面这个提示：

  <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281031748.png" alt="f380ef357d065498d8d54ad07f145e09" style="zoom:50%;" />

- 业务的角度来预防死锁，对订单做幂等性校验的目的是为了保证不会出现重复的订单，那我们可以直接将 order_no 字段设置为唯一索引列，利用它的唯一性来保证订单表不会出现重复的订单，不过有一点不好的地方就是在我们插入一个已经存在的订单记录时就会抛出异常。







## 日志

### 分类

更新语句的流程会涉及到 undo log（回滚日志）、redo log（重做日志） 、binlog （归档日志）这三种日志：

- **undo log（回滚日志）**：是 Innodb 存储引擎层生成的日志，实现了事务中的**原子性**，主要**用于事务回滚和 MVCC**。
- **redo log（重做日志）**：是 Innodb 存储引擎层生成的日志，实现了事务中的**持久性**，主要**用于掉电等故障恢复**；
- **binlog （归档日志）**：是 Server 层生成的日志，主要**用于数据备份和恢复以及主从复制**；



### undo log（回滚日志）

1.**实现事务回滚，保障事务的原子性**。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。

一条记录的每一次更新操作产生的 undo log 格式都有一个 roll_pointer 指针和一个 trx_id 事务id：

- 通过 trx_id 可以知道该记录是被哪个事务修改的；
- 通过 roll_pointer 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链；

**2.undo log 还有一个作用，通过 ReadView + undo log 实现 MVCC（多版本并发控制）**。

- **实现 MVCC（多版本并发控制）关键因素之一**。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。





### redo log（重做日志）

#### 介绍

- **实现事务的持久性，让 MySQL 有 crash-safe 的能力**，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失； 防止 Buffer Pool 中的脏页丢失

- **将写操作从「随机写」变成了「顺序写」**-------【循环写】，提升 MySQL 写入磁盘的性能。




redo log 是物理日志，记录了某个数据页做了什么修改，比如**对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新**，每当执行一个事务就会产生这样的一条或者多条物理日志。

**redo log 保证了事务四大特性中的持久性**。

redo log 和 undo log 区别在哪？

- undo log 记录了此次事务「**开始前**」的数据状态，记录的是更新**之前**的值；
- redo log 记录了此次事务「**完成后**」的数据状态，记录的是更新**之后**的值；

**事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务，**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281147997.png" alt="_E4_BA_8B_E5_8A_A1_E6_81_A2_E5_A4_8D" style="zoom:50%;" />

> 被修改 Undo 页面，需要记录对应 redo log 吗？

需要的。

开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。

不过，**在内存修改该 Undo 页面后，需要记录对应的 redo log**。





#### WAL（预写日志） 

Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。

**MySQL 的写操作并不是立刻写到磁盘上，而是先写日志（ redo log ），然后在合适的时间再写到磁盘上**。

WAL 技术的另外一个优点：**MySQL 的写操作从磁盘的「随机写」变成了「顺序写」**，提升语句的执行性能。



#### **redo log buffer**

每当产生一条 redo log 时，会先写入到 redo log buffer，后续在持久化到磁盘如下图：

redo log buffer 默认大小 16 MB，可以通过 `innodb_log_Buffer_size` 参数动态的调整大小，增大它的大小可以让 MySQL 处理「大事务」是不必写入磁盘，进而提升写 IO 性能

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281330020.webp" alt="redologbuf" style="zoom: 33%;" />



#### 刷盘时机

缓存在 redo log buffer 里的 redo log 还是在内存中，它什么时候刷新到磁盘？

主要有下面几个时机：

- MySQL 正常关闭时；
- 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘；
- InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。
- 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制
  - 数据安全性：参数 1 > 参数 2 > 参数 0
  - 写入性能：参数 0 > 参数 2> 参数 1
  - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281341597.png" alt="innodb_flush_log_at_trx_commit2.drawio" style="zoom:50%;" />





#### redo log 循环写

默认情况下， InnoDB 存储引擎有 1 个重做日志文件组( redo log Group），「重做日志文件组」由有 2 个 redo log 文件组成，这两个 redo 日志的文件名叫 ：`ib_logfile0` 和 `ib_logfile1` 。

在重做日志组中，每个 redo log File 的大小是固定且一致的，假设每个 redo log File 设置的上限是 1 GB，那么总共就可以记录 2GB 的操作。

重做日志文件组是以**循环写**的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。

所以 InnoDB 存储引擎会先写 ib_logfile0 文件，当 ib_logfile0 文件被写满的时候，会切换至 i

b_logfile1 文件，当 ib_logfile1 文件也被写满时，会切换回 ib_logfile0 文件。

![_E9_87_8D_E5_81_9A_E6_97_A5_E5_BF_97_E6_96_87_E4_BB_B6_E7_BB_84_E5_86_99_E5_85_A5_E8_BF_87_E7_A8_8B.drawio](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281503746.png)

- write pos 和 checkpoint 的移动都是顺时针方向；
- write pos ～ checkpoint 之间的部分（图中的红色部分），用来记录新的更新操作；
- check point ～ write pos 之间的部分（图中蓝色部分）：待落盘的脏数据页记录；

如果 write pos 追上了 checkpoint，就意味着 **redo log 文件满了，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞**（*因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要*），此时**会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针）**，然后 MySQL 恢复正常运行，继续执行新的更新操作。

所以，一次 checkpoint 的过程就是脏页刷新到磁盘中变成干净页，然后标记 redo log 哪些记录可以被覆盖的过程。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281504833.png" alt="checkpoint" style="zoom: 33%;" />





#### 整个数据库的数据被删除

> 如果不小心整个数据库的数据被删除了，能使用 redo log 文件恢复数据吗？

不可以使用 redo log 文件恢复，只能使用 binlog 文件恢复。

因为 redo log 文件是循环写，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。

binlog 文件保存的是全量的日志，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据



#### 事务没提交，redo log 持久化

缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。

也就是说，**事务没提交的时候，redo log 也是可能被持久化到磁盘的**。



有的同学可能会问，如果 mysql 崩溃了，还没提交事务的 redo log 已经被持久化磁盘了，mysql 重启后，数据不就不一致了？

这种情况 mysql 重启会进行回滚操作，因为事务没提交的时候，binlog 是还没持久化到磁盘的。

所以， **redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘。**



### binlog （归档日志）

 undo log 和 redo log 这两个日志都是 Innodb 存储引擎生成的。

MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog



#### redo log 和 binlog区别

这两个日志有四个区别。

*1、适用对象不同：*

- binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用；
- redo log 是 Innodb 存储引擎实现的日志；

*2、文件格式不同：*

- binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下：
  - STATEMENT：每一条**修改数据的 SQL** 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致；
  - ROW：**记录行数据最终被修改成什么样了**（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已；
  - MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式；
- redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；

*3、写入方式不同：*

- binlog 是**追加写**，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。
- redo log 是**循环写**，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。

*4、用途不同：*

- binlog 用于备份恢复、主从复制；
- redo log 用于掉电等故障恢复。



####  主从复制

这个过程一般是**异步**

MySQL 集群的主从复制过程梳理成 3 个阶段：

- **写入 Binlog**：主库写 binlog 日志，提交事务，并更新本地存储数据。
- **同步 Binlog**：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。
- **回放 Binlog**：回放 binlog，并更新存储引擎中的数据。

具体详细过程如下：

- MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。
- 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。
- 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。

> 从库是不是越多越好？

不是的。因为从库数量增加，从库连接上来的 I/O 线程也比较多，**主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽**。

所以在实际使用中，一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是一主多从的 MySQL 集群结构。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281552392.png" alt="_E4_B8_BB_E4_BB_8E_E5_A4_8D_E5_88_B6_E8_BF_87_E7_A8_8B.drawio" style="zoom:50%;" />

> MySQL 主从复制还有哪些模型？

主要有三种：

- **同步复制**：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。
- **异步复制**（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。
- **半同步复制**：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种**半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险**



#### binlog 刷盘时机

事务执行过程中，先把日志写到 binlog cache（Server 层的 cache），**事务提交的时候**，执行器把 binlog cache 里的完整事务写入到 **binlog 文件**中，并清空 binlog cache，再根据sync_binlog 参数来控制数据库的 **binlog 刷到磁盘上**的频率。



一个事务的 binlog 是不能被拆开的，因此无论这个事务有多大（比如有很多条语句），也要保证**一次性写入**。

MySQL 给每个线程分配了一片内存用于缓冲 binlog ，该内存叫 binlog cache，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。



<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281616634.jpeg" alt="img" style="zoom:50%;" />

虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件：

- 图中的 write，指的就是指把日志写入到 binlog 文件，但是并没有把数据持久化到磁盘，因为数据还缓存在文件系统的 page cache 里，write 的写入速度还是比较快的，因为不涉及磁盘 I/O。
- 图中的 fsync，才是将数据持久化到磁盘的操作，这里就会涉及磁盘 I/O，所以频繁的 fsync 会导致磁盘的 I/O 升高。



MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率：

- sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘；
- sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync；
- sync_binlog =N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。

在MySQL中系统默认的设置是 sync_binlog = 0，也就是不做任何强制性的磁盘刷新指令，这时候的性能是最好的，但是风险也是最大的。因为一旦主机发生异常重启，还没持久化到磁盘的数据就会丢失。

如果能容少量事务的 binlog 日志丢失的风险，为了提高写入的性能，一般会 sync_binlog 设置为 100~1000 中的某个数值。



### 两阶段提交

事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。

**MySQL 为了避免出现两份日志之间的逻辑不一致的问题，使用了「两阶段提交」来解决**，，**两阶段提交是以 binlog 写成功为事务提交成功的标识**

- **prepare 阶段**：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）；
- **commit 阶段**：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功



 MySQL 重启后会按顺序扫描 redo log 文件，碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID：

- **如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务**。对应时刻 A 崩溃恢复的情况。
- **如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务**。对应时刻 B 崩溃恢复的情况。

可以看到，**对于处于 prepare 阶段的 redo log，即可以提交事务，也可以回滚事务，**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408282007180.png" alt="_E4_B8_A4_E9_98_B6_E6_AE_B5_E6_8F_90_E4_BA_A4.drawio" style="zoom: 50%;" />



> 处于 prepare 阶段的 redo log 加上完整 binlog，重启就提交事务，MySQL 为什么要这么设计?

binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。

所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。



> 事务没提交的时候，redo log 会被持久化到磁盘吗？

会的。

事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。

也就是说，**事务没提交的时候，redo log 也是可能被持久化到磁盘的**。

有的同学可能会问，如果 mysql 崩溃了，还没提交事务的 redo log 已经被持久化磁盘了，mysql 重启后，数据不就不一致了？

放心，这种情况 mysql 重启会进行回滚操作，因为事务没提交的时候，binlog 是还没持久化到磁盘的。

所以， redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘



### 组提交

#### binlog 组提交

**MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数**，如果说 10 个事务依次排队刷盘的时间成本是 10，那么将这 10 个事务一次性一起刷盘的时间成本则近似于 1。

引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程：

- **flush 阶段**：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）；
- **sync 阶段**：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）；
- **commit 阶段**：各个事务按顺序做 InnoDB commit 操作；

上面的**每个阶段都有一个队列**，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281956156.png" alt="commit_4" style="zoom:33%;" />

对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再锁住提交事务的整个过程，可以看的出来，**锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率**。



#### redo log 组提交

这个要看 MySQL 版本，MySQL 5.6 没有 redo log 组提交，MySQL 5.7 有 redo log 组提交。

在 MySQL 5.6 的组提交逻辑中，每个事务各自执行 prepare 阶段，也就是各自将 redo log 刷盘，这样就没办法对 redo log 进行组提交。

所以在 MySQL 5.7 版本中，做了个改进，在 prepare 阶段不再让事务各自执行 redo log 刷盘操作，而是推迟到组提交的 flush 阶段，也就是说 prepare 阶段融合在了 flush 阶段。

这个优化是将 redo log 的刷盘延迟到了 flush 阶段之中，sync 阶段之前。通过延迟写 redo log 的方式，为 redolog 做了一次组写入，这样 binlog 和 redo log 都进行了优化。

接下来介绍每个阶段的过程，注意下面的过程针对的是“双 1” 配置（sync_binlog 和 innodb_flush_log_at_trx_commit 都配置为 1）。



#### 流程

- flush 阶段：队列的作用是**用于支撑 redo log 的组提交**。

- sync 阶段：**目的是为了组合更多事务的 binlog，然后再一起刷盘**，等待的时长由 `Binlog_group_commit_sync_delay` 参数控制

  - `binlog_group_commit_sync_delay= N`，表示在等待 N 微妙后，直接调用 fsync，将处于文件系统中 page cache 中的 binlog 刷盘，也就是将「 binlog 文件」持久化到磁盘。
  - `binlog_group_commit_sync_no_delay_count = N`，表示如果队列中的事务数达到 N 个，就忽视binlog_group_commit_sync_delay 的设置，直接调用 fsync，将处于文件系统中 page cache 中的 binlog 刷盘。

- commit 阶段：作用是承接 sync 阶段的事务，完成最后的引擎提交，使得 sync 可以尽早的处理下一组事务，最大化组提交的效率。

  





### MySQL 磁盘 I/O 很高，有什么优化的方法？

现在我们知道事务在提交的时候，需要将 binlog 和 redo log 持久化到磁盘，那么如果出现 MySQL 磁盘 I/O 很高的现象，我们可以通过控制以下参数，来 “延迟” binlog 和 redo log 刷盘的时机，从而降低磁盘 I/O 的频率：

- 设置组提交的两个参数： binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，延迟 binlog 刷盘的时机，从而减少 binlog 的刷盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但即使 MySQL 进程中途挂了，也没有丢失数据的风险，因为 binlog 早被写入到 page cache 了，只要系统没有宕机，缓存在 page cache 里的 binlog 就会被持久化到磁盘。
- 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000），表示每次提交事务都 write，但累积 N 个事务后才 fsync，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，主机掉电时会丢 N 个事务的 binlog 日志。
- 将 innodb_flush_log_at_trx_commit 设置为 2。表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，然后交由操作系统控制持久化到磁盘的时机。但是这样做的风险是，主机掉电的时候会丢数据



### update流程

具体更新一条记录 `UPDATE t_user SET name = 'xiaolin' WHERE id = 1;` 的流程如下:

1. 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录：
   - 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新；
   - 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。
2. 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样：
   - 如果一样的话就不进行后续更新流程；
   - 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作；
3. 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。
4. InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 **WAL 技术**，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。
5. 至此，一条记录更新完了。
6. 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。
7. 事务提交（为了方便说明，这里不说组提交的过程，只说两阶段提交）：
   - **prepare 阶段**：将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘；
   - **commit 阶段**：将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）；
8. 至此，一条更新语句执行完成



## 内存/缓冲

Innodb 存储引擎设计了一个**缓冲池（Buffer Pool）**，来提高数据库的读写性能。

默认配置下 Buffer Pool 只有 `128MB` 。可以通过调整 `innodb_buffer_pool_size` 参数来设置 Buffer Pool 的大小，一般建议设置成可用物理内存的 60%~80%

- 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。
- 当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘

MySQL 启动后，通过操作系统触发**缺页中断**，接着将虚拟地址和物理地址建立映射关系。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292222146.png" alt="bufferpool_E5_86_85_E5_AE_B9.drawio" style="zoom:50%;" />

为了更好的管理这些在 Buffer Pool 中的缓存页，InnoDB 为每一个缓存页都创建了一个**控制块**，控制块信息包括「缓存页的表空间、页号、缓存页地址、链表节点」等等。

控制块也是占有内存空间的，它是放在 Buffer Pool 的最前面，接着才是缓存页，如下图：

图中控制块和缓存页之间灰色部分称为碎片空间。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292223194.png" alt="_E7_BC_93_E5_AD_98_E9_A1_B5.drawio" style="zoom:50%;" />





Innodb 通过三种链表来管理缓页：

- Free List （空闲页链表），管理空闲页；

  头节点+N个控制块，Free 链表节点是一个一个的控制块，而每个控制块包含着对应缓存页的地址，所以相当于 Free 链表节点都对应一个空闲的缓存页。

  - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292231789.png" alt="freelist.drawio" style="zoom:50%;" />

- Flush List （脏页链表），管理脏页； 同Free List 

- LRU List，管理脏页+干净页，将最近且经常查询的数据缓存在其中，而不常查询的数据就淘汰出去。；

InnoDB 对 LRU 做了一些优化，我们熟悉的 LRU 算法通常是将最近查询的数据放到 LRU 链表的头部，而 InnoDB 做 2 点优化：

- 将 LRU 链表 分为**young 和 old 两个区域**，加入缓冲池的页，优先插入 old 区域；页被访问时，才进入 young 区域，目的是为了解决预读失效的问题。
- 当**「页被访问」且「 old 区域停留时间超过 `innodb_old_blocks_time` 阈值（默认为1秒）」**时，才会将页插入到 young 区域，否则还是插入到 old 区域，目的是为了解决批量数据访问，大量热数据淘汰的问题。
- <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408271642237.png" alt="lrutwo.drawio" style="zoom:50%;" />

可以通过调整 `innodb_old_blocks_pct` 参数，设置 young 区域和 old 区域比例。

在开启了慢 SQL 监控后，如果你发现「偶尔」会出现一些用时稍长的 SQL，这可因为脏页在刷新到磁盘时导致数据库性能抖动。如果在很短的时间出现这种现象，就需要调大 Buffer Pool 空间或 redo log 日志的大小。



> Undo 页是记录什么？

开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。

> 查询一条记录，就只需要缓冲一条记录吗？

不是的。

当我们查询一条记录时，InnoDB 是会把整个页的数据加载到 Buffer Pool 中，将页加载到 Buffer Pool 后，再通过页里的「页目录」去定位到某条具体的记录。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281046785.png" alt="_E7_BC_93_E5_86_B2_E6_B1_A0.drawio" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408281047455.png" alt="bufferpool_E5_86_85_E5_AE_B9.drawio" style="zoom:50%;" />



## EXPLAIN

在 MySQL 中，`EXPLAIN` 关键字用于分析 SQL 查询的执行计划，帮助开发者了解查询在数据库中的执行方式。通过使用 `EXPLAIN`，你可以查看查询的优化情况、使用的索引、扫描的行数、连接的顺序等信息，从而优化查询性能。

### `EXPLAIN` 的使用方法

`EXPLAIN` 可以放在 `SELECT`、`INSERT`、`UPDATE`、`DELETE`  语句前面。例如：

```sql
EXPLAIN SELECT * FROM users WHERE id = 1;
```

### 输出列解释

`EXPLAIN` 的输出结果通常包含以下几列（视 MySQL 版本可能有所不同）：

1. **id**：
   - 查询中每个 `SELECT` 语句的标识符，表示查询的执行顺序。较大的 `id` 表示优先执行。

2. **select_type**：
   - 描述查询的类型，常见值包括：
     - `SIMPLE`：简单查询，没有 `UNION` 或子查询。
     - `PRIMARY`：最外层查询。
     - `SUBQUERY`：子查询。
     - `DERIVED`：派生表查询（如子查询在 `FROM` 子句中）。

3. **table**：
   - 参与查询的表名或别名。

4. **type**：
   - 表示连接类型或访问方法，常见的类型从效率低到高排列：
     - `ALL`：全表扫描。
     - `INDEX`：扫描索引。
     - `RANGE`：索引范围扫描。
     - `REF`：使用非唯一索引的查询。
     - `EQ_REF`：使用唯一索引查询。
     - `CONST`/`SYSTEM`：常量查询，效率最高。

5. **possible_keys**：
   - 查询中可能使用的索引，表明哪些索引可以用于优化查询。

6. **key**：
   - 实际使用的索引，如果没有使用索引则为 `NULL`。

7. **key_len**：
   - 使用的索引长度，单位为字节，反映了 MySQL 实际用到了索引的哪个部分。

8. **ref**：
   - 显示索引中使用的列与查询中哪一列进行匹配。

9. **rows**：
   - 估计需要读取的行数，用于评估查询扫描的工作量。

10. **filtered**：
    - 表示查询条件过滤后的行百分比。例如，如果 `filtered=100`，表示没有行被过滤；如果是 `50`，则表示大约一半的行被过滤掉了。

11. **Extra**：
    - 显示额外的信息，可能的值包括：
      - `Using where`：查询中使用了 `WHERE` 过滤条件。
      - `Using index`：查询仅使用索引，未访问表数据（索引覆盖查询）。
      - `Using temporary`：查询中使用了临时表。
      - `Using filesort`：查询进行了文件排序，通常是因为缺乏合适的索引。

### 示例

以下是一个 `EXPLAIN` 查询的示例：

```sql
EXPLAIN SELECT * FROM orders WHERE user_id = 10 ORDER BY created_at DESC LIMIT 10;
```

输出结果可能如下：

| id   | select_type | table  | type | possible_keys | key     | key_len | ref   | rows | filtered | Extra                       |
| ---- | ----------- | ------ | ---- | ------------- | ------- | ------- | ----- | ---- | -------- | --------------------------- |
| 1    | SIMPLE      | orders | ref  | user_id_index | user_id | 4       | const | 100  | 100.00   | Using where; Using filesort |

### 优化建议

- **全表扫描（ALL）**：如果查询使用了 `ALL`，说明没有使用索引，可能需要添加合适的索引。
- **Using filesort**：通常意味着查询中有排序操作，但没有使用索引进行优化，可能需要添加索引。
- **Using temporary**：意味着查询使用了临时表，可能需要优化 `GROUP BY` 或 `ORDER BY` 的列。

`EXPLAIN` 是调优 MySQL 查询性能的关键工具，帮助你识别潜在的性能瓶颈并优化数据库结构和查询。



### 重点关注的字段及其意义：

#### 1. **type**（连接类型/访问方法）
   - **重要性**：这是最关键的字段之一，表示 MySQL 如何访问表中的数据。该字段值直接影响查询性能。
   - **常见值及其效率从低到高**：
     - `ALL`：全表扫描，通常效率较低，应避免。
     - `INDEX`：扫描整个索引，比全表扫描稍好，但仍需要遍历大量数据。
     - `RANGE`：索引范围扫描，查询效率较高，常用于范围条件 (`BETWEEN`、`>`)。
     - `REF`：使用非唯一索引查找，效率良好。
     - `EQ_REF`：使用唯一索引查找，查询性能非常好。
     - `CONST`/`SYSTEM`：只针对常量或系统表进行查找，效率最高。

   - **优化建议**：如果 `type` 显示为 `ALL` 或 `INDEX`，可能意味着查询没有使用合适的索引，应该考虑优化索引。

#### 2. **key**（实际使用的索引）
   - **重要性**：指示 MySQL 实际使用了哪个索引。如果为 `NULL`，说明查询没有使用索引，可能导致性能问题。
   - **优化建议**：如果 `key` 显示为 `NULL`，但 `possible_keys` 中有可用索引，可以检查查询条件是否能够利用该索引，或者是否需要添加索引。

#### 3. **rows**（扫描的行数）
   - **重要性**：MySQL 预估需要扫描多少行来满足查询条件。行数越多，查询的执行成本越高。
   - **优化建议**：尝试减少查询扫描的行数，通常可以通过增加或优化索引、改善 `WHERE` 子句条件来实现。

#### 4. **Extra**（额外信息）
   - **重要性**：这个字段提供了关于查询执行方式的额外信息，可能揭示性能问题。重要的值包括：
     - `Using where`：MySQL 在行检索后应用了 `WHERE` 条件，通常是正常现象。
     - `Using index`：查询只使用索引来满足请求，未访问表数据（索引覆盖查询），这种情况非常高效。
     - `Using temporary`：查询使用了临时表，通常出现在复杂的 `GROUP BY` 或 `ORDER BY` 查询中，可能会影响性能。
     - `Using filesort`：MySQL 需要额外的排序步骤，通常出现在没有合适索引的情况下，尤其是在 `ORDER BY` 或 `GROUP BY` 时。

   - **优化建议**：
     - **避免 `Using temporary` 和 `Using filesort`**：如果查询显示了这两项，意味着查询性能可能不佳，考虑通过添加索引来优化。

#### 5. **possible_keys**（可能使用的索引）
   - **重要性**：表示 MySQL 认为可以用于优化查询的索引列表。如果该字段为空，则表明查询可能没有合适的索引。
   - **优化建议**：如果有合适的索引但未被使用，可以检查查询条件是否正确匹配索引，或重新设计索引。

#### 6. **filtered**（过滤的行百分比）
   - **重要性**：表示查询条件在每一步过滤掉的行的百分比。数值越大，表示筛选后的行数越少。
   - **优化建议**：如果 `filtered` 的百分比很低，可能说明查询过滤效率不佳，应考虑重新设计 `WHERE` 条件或优化索引。

---

### 优化过程重点
- **关注 `type` 字段**：它是优化查询的关键，通常希望值为 `RANGE`、`REF`、`EQ_REF` 或 `CONST`。
- **查看 `key` 是否为 NULL**：如果查询没有使用索引，需检查是否可以通过优化索引来改善查询性能。
- **减少 `rows` 扫描数量**：扫描的行数越少，查询性能越高，通常通过添加索引或优化查询条件来实现。
- **避免 `Using temporary` 和 `Using filesort`**：这通常意味着查询效率不佳，应该尽量通过索引优化来避免。

通过重点分析这些字段，你可以识别查询的性能瓶颈并做出相应的优化。



### 原理

数据库的查询优化器在解析SQL语句后，会根据系统中保存的统计信息（比如表的行数、索引的分布、数据的分布情况等）以及一套预定义的成本模型，对各种可能的执行计划进行估算。这些成本模型包括磁盘I/O成本（例如顺序扫描的磁盘页数乘以相应的成本参数）和CPU处理成本等。

即使使用 `EXPLAIN` 命令时，数据库不会实际执行SQL语句，它仍然会：

1. **解析和重写查询**：将SQL语句解析为语法树，并根据规则进行优化（比如常量传播、子查询转换等）。
2. **获取统计信息**：从数据字典中提取当前表和索引的统计数据（例如行数、数据分布、索引基数等）。
3. **成本估算**：利用统计数据和成本参数（如 `seq_page_cost`、`cpu_tuple_cost` 等）计算出各种执行方案的预估成本和预计返回的行数。
4. **生成执行计划**：选择成本最低的方案并将其展示出来。

因此，`EXPLAIN` 命令展示的执行计划及成本估算，都是基于当前的统计信息和成本模型的虚拟计算结果，而不是查询真正执行时的实际数据。



## 分库分表

"只分库不分表"、"只分表不分库"、以及"既分库又分表"。

### 大表的判断标准

在数据库系统中，大表（Large Table）通常指行数（记录数）和数据量（磁盘占用）都较大的表。具体的判断标准可以从以下几个维度考虑：

- **行数（Row Count）**：单表行数是否达到数百万甚至上亿。通常行数超过 100 万可以认为是大表，但实际标准会因数据库和业务场景而异。
- **表空间大小（Table Size）**：单表占用的磁盘空间是否超过 1GB 或 10GB 以上（依据存储系统容量和性能来定）。
- **字段数（Column Count）**：字段过多的表（几十甚至上百个字段），因为字段多会影响查询和更新性能。
- **索引数量和大小**：索引过多或者索引大小超过一定值（如 100MB），也会被视为大表，因为索引维护和更新代价较高。
- **SQL 操作的开销**：查询、更新、删除某个表的数据是否引发明显的性能瓶颈（如查询耗时过长、锁等待）。

大表通常会导致以下问题：查询效率下降、索引失效、锁竞争严重等。因此对于大表的优化，通常包括分区（Partitioning）、分表（Sharding）、索引优化等方案。



### 分库

分库主要解决的是**并发量大**的问题。因为并发量一旦上来了，那么数据库就可能会成为瓶颈，因为数据库的连接数是有限的，虽然可以调整，但是也不是无限调整的。

比较典型的分库的场景就是我们在做微服务拆分的时候，就会按照业务边界，把各个业务的数据从一个单一的数据库中拆分开，分表把订单、物流、商品、会员等单独放到单独的数据库中。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409251023491.jpeg" alt="434c0db704e23c3174c624ea923c2bfbe4efb0" style="zoom:50%;" />

### 分表

分表其实主要解决的是**数据量大**的问题。

假如你的单表数据量非常大，因为并发不高，数据量连接可能还够，但是存储和查询的性能遇到了瓶颈了，你做了很多优化之后还是无法提升效率的时候，就需要考虑做分表了。

通过将数据拆分到多张表中，来减少单表的数据量，从而提升查询速度。

一般我们认为，单表行数超过 500 万行或者单表容量超过 2GB之后，才需要考虑做分库分表了，小于这个数据量，遇到性能问题先建议大家通过其他优化来解决。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409251023029.jpeg" alt="1220d7176ec8e4b0a38885a3e084fc261744d6" style="zoom:50%;" />



### 什么时候既分库又分表?

那么什么时候分库又分表呢，那就是既需要解决并发量大的问题，又需要解决数据量大的问题时候。通常情况下，高并发和数据量大的问题都是同时发生的，所以，我们会经常遇到分库分表需要同时进行的情况。

所以，当你的数据库链接也不够了，并且单表数据量也很大导致查询比较慢的时候，就需要做既分库又分表了。

### 横向拆分和纵向拆分

谈及到分库分表，那就要涉及到该如何做拆分的问题。

通常在做拆分的时候有两种分法，分别是横向拆分(水平拆分)和纵向拆分(垂直拆分)。假如我们有一张表，如果把这张表中某一条记录的多个字段，拆分到多张表中，这种就是纵向拆分。那如果把一张表中的不同的记录分别放到不同的表中，这种就是横向拆分。

横向拆分的结果是数据库表中的数据会分散到多张分表中，使得每一个单表中的数据的条数都有所下降。比如我们可以把不同的用户的订单分表拆分放到不同的表中。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409251024307.jpeg" alt="38dcbb856d9aedfb96d5951c14de7c572d5f04" style="zoom:50%;" />

纵向拆分的结果是数据库表中的数据的字段数会变少，使得每一个单表中的数据的存储有所下降。比如我可以把商品详情信息、价格信息、库存信息等等分别拆分到不同的表中。

针对不同的业务做拆分成多个数据库的这种情况，其实也是纵向拆分的一种。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409251025463.jpeg" alt="12127dc380e294645d7828fc0aa11bdb864830" style="zoom:50%;" />

### 分表字段的选择

在分库分表的过程中，我们需要有一个字段用来进行分表，比如按照用户分表、按照时间分表、按照地区分表。这里面的用户、时间、地区就是所谓的分表字段。

那么，在选择这个分表字段的时候，一定要注意，要根据实际的业务情况来做慎重的选择。

比如说我们要对交易订单进行分表的时候，我们可以选择的信息有很多，比如买家Id、卖家Id、订单号、时间、地区等等，具体应该如何选择呢?

通常，如果有特殊的诉求，比如按照月度汇总、地区汇总等以外，我们通常建议大家按照买家Id进行分表。因为这样可以避免一个关键的问题那就是——数据倾斜(热点数据)。



一个大的卖家可能会产生很多订单，比如像苏宁易购、当当等这种店铺，他每天在天猫产生的订单量就非常的大。如果按照卖家Id分表的话，那同一个卖家的很多订单都会分到同一张表。

那就会使得有一些表的数据量非常的大，但是有些表的数据量又很小，这就是发生了数据倾斜。这个卖家的数据就变成了热点数据，随着时间的增长，就会使得这个卖家的所有操作都变得异常缓慢。

需要注意的是，我们说按照买家Id做分表，保证的是**同一个买家的所有订单都在同一张表** ，并不是要给每个买家都单独分配一张表。

#### 卖家查询怎么办?

卖家查询的话，同样可以带卖家id过来，那么，我们可以有一个基于binlog、flink等准实时的同步一张卖家维度的分表，这张表只用来查询，来解决卖家查询的问题。

本质上就是**用空间换时间**的做法。

卖家表理论上是没有业务的写操作，只有读操作的。

所以，这个卖家库只需要有高性能的读就行了，那这样的话就可以有很多选择了，比如可以部署到一些配置不用那么高的机器、或者其实可以干脆就不用MYSQL，而是采用HBASE、PolarDB、Lindorm等数据库就可以了。这些数据库都是可以海量数据，并提供高性能查询的。

还有呢就是，大卖家一般都是可以识别的，提前针对大卖家，把他的订单，再按照一定的规则拆分到多张表中。因为只有读，没有写操作，所以拆分多张表也不用考虑事务的问题。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409251027053.jpeg" alt="67e626b48513c6615953404c03ca294b85deef" style="zoom:50%;" />

### 分表算法

选定了分表字段之后，如何基于这个分表字段来准确的把数据分表到某一张表中呢?

这就是分表算法要做的事情了，但是不管什么算法，我们都需要确保一个前提，那就是同一个分表字段，经过这个算法处理后，得到的结果一定是一致的，不可变的。

通常情况下，当我们对order表进行分表的时候，比如我们要分成128张表的话，那么得到的128表应该是:order_0000、order_0001、order_0002.....order_0126、order_0127

通常的分表算法有以下几种：

#### 直接取模

在分库分表时，我们是事先可以知道要分成多少个库和多少张表的，所以，比较简单的就是取模的方式。

比如我们要分成128张表的话，就用一个整数来对128取模就行了，得到的结果如果是0002，那么就把数据放到order_0002这张表中。

#### Hash取模

那如果分表字段不是数字类型，而是字符串类型怎么办呢?有一个办法就是哈希取模，就是先对这个分表字段取Hash，然后在再取模。

但是需要注意的是，Java中的hash方法得到的结果有可能是负数，需要考虑这种负数的情况。

#### 一致性Hash

前面两种取模方式都比较不错，可以使我们的数据比较均匀的分布到多张分表中。但是还是存在一个缺点。

那就是如果需要扩容二次分表，表的总数量发生变化时，就需要重新计算hash值，就需要涉及到数据迁移了。

为了解决扩容的问题，我们可以采用一致性哈希的方式来做分表。

一致性哈希可以按照常用的hash算法来将对应的key哈希到一个具有2^32次方个节点的空间中，形成成一个顺时针首尾相接的闭合的环形。所以当添加一台新的数据库服务器时，只有增加服务器的位置和逆时针方向第一台服务器之间的键会受影响。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409251028551.jpeg" alt="f6b531432b3fbbb2dbd90844da0206235dafd0" style="zoom:50%;" />

### 全局ID的生成

涉及到分库分表，就会引申出分布式系统中唯一主键ID的生成问题，因为在单表中我们可以用数据库主键来做唯一ID，但是如果做了分库分表，多张单表中的自增主键就一定会发生冲突。那就不具备全局唯一性了。



#### UUID

很多人对UUID都不陌生，它是可以做到全局唯一的，而且生成方式也简单，但是我们通常不推荐使用他做唯一ID，首先UUID太长了，其次字符串的查询效率也比较慢，而且没有业务含义，根本看不懂。

基于某个单表做自增主键

多张单表生成的自增主键会冲突，但是如果所有的表中的主键都从同一张表生成是不是就可以了。

所有的表在需要主键的时候，都到这张表中获取一个自增的ID。

这样做是可以做到唯一，也能实现自增，但是问题是这个单表就变成整个系统的瓶颈，而且也存在单点问题，一旦他挂了，那整个数据库就都无法写入了。



#### 基于多个单表+步长做自增主键

为了解决单个数据库做自曾主键的瓶颈及单点故障问题，我们可以引入多个表来一起生成就行了。

但是如何保证多张表里面生成的Id不重复呢?如果我们能实现以下的生成方式就行了：

实例1生成的ID从1000开始，到1999结束。

实例2生成的ID从2000开始，到2999结束。

实例3生成的ID从3000开始，到3999结束。

实例4生成的ID从4000开始，到4999结束。

![34f78dd71b3d64208c8187cd6380aec4577345](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409251029400.jpeg)

这样就能避免ID重复了，那如果第一个实例的ID已经用到1999了怎么办?那就生成一个新的起始值：

实例1生成的ID从5000开始，到5999结束。实例2生成的ID从6000开始，到6999结束。实例3生成的ID从7000开始，到7999结束。实例4生成的ID从8000开始，到8999结束。

我们把步长设置为1000，确保每一个单表中的主键起始值都不一样，并且比当前的最大值相差1000就行了。



#### 雪花算法

雪花算法也是比较常用的一种分布式ID的生成方式，它具有全局唯一、递增、高可用的特点。

雪花算法生成的主键主要由 4 部分组成，1bit符号位、41bit时间戳位、10bit工作进程位以及 12bit 序列号位。

时间戳占用41bit，精确到毫秒，总共可以容纳约69年的时间。

工作进程位占用10bit，其中高位5bit是数据中心ID，低位5bit是工作节点ID，做多可以容纳1024个节点。

序列号占用12bit，每个节点每毫秒0开始不断累加，最多可以累加到4095，一共可以产生4096个ID。

所以，一个雪花算法可以在同一毫秒内最多可以生成1024 X 4096 = 4194304个唯一的ID



### 分库分表带来的问题

分库分表之后，会带来很多问题。

首先，做了分库分表之后，所有的读和写操作，都需要带着分表字段，这样才能知道具体去哪个库、哪张表中去查询数据。如果不带的话，就得支持全表扫描。

但是，单表的时候全表扫描比较容易，但是做了分库分表之后，就没办法做扫表的操作了，如果要扫表的话就要把所有的物理表都要扫一遍。

还有，一旦我们要从多个数据库中查询或者写入数据，就有很多事情都不能做了，比如跨库事务就是不支持的。



所以，分库分表之后就会带来因为不支持事务而导致的数据一致性的问题。

其次，做了分库分表之后，以前单表中很方便的分页查询、排序等等操作就都失效了。因为我们不能跨多表进行分页、排序。

总之，分库分表虽然能解决一些大数据量、高并发的问题，但是同时也会带来一些新的问题。所以，在做数据库优化的时候，还是建议大家优先选择其他的优化方式，最后再考虑分库分表。

------



# redis

现在为7.4版本

## 性质

Redis 是一种基于内存的数据库，对数据的读写操作都是在内存中完成，因此**读写速度非常快**，常用于**缓存，消息队列、分布式锁等场景**。

Redis 提供了多种数据类型来支持不同的业务场景，比如 String(字符串)、Hash(哈希)、 List (列表)、Set(集合)、Zset(有序集合)、Bitmaps（位图）、HyperLogLog（基数统计）、GEO（地理信息）、Stream（流），并且对数据类型的操作都是**原子性**的，因为执行命令由单线程负责的，不存在并发竞争的问题。

除此之外，Redis 还支持**事务 、持久化、Lua 脚本、多种集群方案（主从复制模式、哨兵模式、切片机群模式）、发布/订阅模式，内存淘汰机制、过期删除机制**等等。

**具备「高性能」和「高并发」两种特性**。

单台设备的 Redis 的 QPS（Query Per Second，每秒钟处理完请求的次数） 是 MySQL 的 10 倍，Redis 单机的 QPS 能轻松破 10w，而 MySQL 单机的 QPS 很难破 1w。



<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202407111545933.png" alt="202407090216013" style="zoom: 33%;" />



## 线程模型

​	**Redis 程序并不是单线程的**，Redis 在启动的时候，是会**启动后台线程**（BIO）的

Redis 单线程指的是「接收客户端请求->解析请求 ->进行数据读写等操作->发送数据给客户端」这个过程是由一个线程（主线程）来完成的

Redis 6.0 版本之后，Redis 在启动的时候，默认情况下会**额外创建 6 个线程**（*这里的线程数不包括主线程*）：

- Redis-server ： Redis的主线程，主要负责执行命令；
- bio_close_file、bio_aof_fsync、bio_lazy_free：三个后台线程，分别异步处理**关闭文件任务、AOF刷盘任务、释放内存任务**；
- io_thd_1、io_thd_2、io_thd_3：三个 I/O 线程，io-threads 默认是 4 ，所以会启动 3（4-1）个 I/O 多线程，用来分担 Redis 网络 I/O 的压力



- BIO_CLOSE_FILE，关闭文件任务队列：当队列有任务后，后台线程会调用 close(fd) ，将文件关闭；
- BIO_AOF_FSYNC，AOF刷盘任务队列：当 AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封装成一个任务，也放到队列中。当发现队列有任务后，后台线程会调用 fsync(fd)，将 AOF 文件刷盘，
- BIO_LAZY_FREE，lazy free 任务队列：当队列有任务后，后台线程会 free(obj) 释放对象 / free(dict) 删除数据库所有对象 / free(skiplist) 释放跳表对象

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181202799.png" alt="_E5_90_8E_E5_8F_B0_E7_BA_BF_E7_A8_8B" style="zoom: 33%;" />



**单线程的 Redis 吞吐量可以达到 10W/每秒**

采用单线程（网络 I/O 和执行命令）那么快，有如下几个原因：

- Redis 的大部分操作**都在内存中完成**，并且采用了高效的数据结构，**CPU 并不是制约 Redis 性能表现的瓶颈所在**，更多情况下是受到内存大小和网络I/O的限制，既然 CPU 不是瓶颈，那么自然就采用单线程的解决方案了；
- Redis 采用单线程模型可以**避免了多线程之间的竞争**，省去了多线程切换带来的时间和性能上的开销，而且也不会导致死锁问题。
- Redis 采用了 **I/O 多路复用机制**处理大量的客户端 Socket 请求



在 **Redis 6.0 版本之后**，也采用了**多个 I/O 线程来处理网络请求**，这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上。

但是对于**命令的执行，Redis 仍然使用单线程**来处理

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181209371.png" alt="redis_E5_8D_95_E7_BA_BF_E7_A8_8B_E6_A8_A1_E5_9E_8B.drawio" style="zoom: 33%;" />

## 启动

```Bash
#redis-server用于启动和管理Redis服务器实例，而redis-cli用于与服务器进行交互和执行操作。
redis-server
redis-cli  / redis-cli --raw 以原始的形式来显示内容 //如中文

redis-server --version   查看版本
redis-cli --version
```

### 安装

在 CentOS 上安装 Redis 可以通过 yum 包管理器轻松完成。以下是在 CentOS 上下载和安装 Redis 的步骤：

1. **更新包列表**：首先，打开终端并更新您的 CentOS 包列表，以确保您获得最新的软件包信息：

```Bash
sudo yum update
```

1. **安装 Redis**：一旦包列表更新完毕，您可以使用 yum 安装 Redis。运行以下命令：

```Bash
sudo yum install redis
```

1. yum 将自动下载并安装 Redis 及其相关依赖项。
2. **启动 Redis 服务**：安装完成后，您可以启动 Redis 服务并将其设置为在系统引导时自动启动。运行以下命令：

```Bash
sudo systemctl start redis
sudo systemctl enable redis
```

1. 这将启动 Redis 服务并将其配置为在系统启动时自动运行。
2. **验证 Redis 安装**：您可以运行以下命令来验证 Redis 是否正在运行：

```Bash
redis-cli ping
```

1. 如果 Redis 正在运行，将返回 "PONG"。

现在，Redis 已成功安装和启动在您的 CentOS 系统上。您可以使用 `redis-cli` 工具或相应的客户端来与 Redis 进行交互。

请注意，这只是一个基本的 Redis 安装。如果需要更多高级配置，如设置密码、修改配置文件等，您可以编辑 Redis 的配置文件 `/etc/redis.conf` 并重启服务以应用更改。



### 切换为后台服务

1. 打开命令提示符：按下Windows键 + R，输入"cmd"并按下Enter键，将打开命令提示符窗口。
2. 导航到Redis安装目录：在命令提示符中，使用`cd`命令导航到Redis的安装目录。例如：

```Plaintext
mathematicaCopy code
cd D:\Environment\Redis-x64-3.0.504
```

1. 这将使命令提示符处于Redis安装目录下。
2. 启动Redis服务器作为后台服务：执行以下命令启动Redis服务器作为后台服务：

```Plaintext
cssCopy code
redis-server --service-install redis.windows.conf --service-name RedisService
```

1. 这将使用redis.windows.conf配置文件并将服务名称设置为"RedisService"来安装Redis服务器作为后台服务。
2. 启动Redis服务：执行以下命令来启动Redis服务：

```Plaintext
cssCopy code
redis-server --service-start --service-name RedisService
```

1. 这将启动Redis服务并将其设置为在后台运行。

现在，Redis服务器已被设置为后台服务，并且会在系统启动时自动启动。您可以通过执行`redis-cli`命令连接到Redis服务器，或通过在服务列表中启动/停止RedisService服务来管理Redis服务器。

1. 打开命令行终端。
2. 运行 `redis-cli` 命令进入 Redis 命令行界面。
3. 在 Redis 命令行界面中，输入 `shutdown` 命令，并按下回车键。
4. Redis 会执行关闭操作并退出后台程序。

请注意，这种方法要求你已经安装并正确配置了 Redis，并且能够访问 Redis 服务器。如果你无法使用命令行终端或无法访问 Redis 服务器，可以考虑使用其他方法来停止 Redis 后台程序，例如通过操作系统的任务管理器或使用 Redis 的管理工具。

另外，如果你是通过在后台运行 Redis 的方式启动的，可以使用以下命令来停止 Redis 后台程序：

```Plaintext
redis-cli shutdown
```





## 数据类型

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181127802.png" alt="key" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181127507.png" alt="_E4_BA_94_E7_A7_8D_E6_95_B0_E6_8D_AE_E7_B1_BB_E5_9E_8B" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181132079.png" alt="9fa26a74965efbf0f56b707a03bb9b7f" style="zoom:50%;" />

### 应用场景

- String 类型的应用场景：缓存对象、常规计数、分布式锁、共享 session 信息等。

  <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011812404.jpeg" alt="img" style="zoom: 50%;" />

- List 类型的应用场景：消息队列（但是有两个问题：1. 生产者需要自行实现全局唯一 ID；2. 不能以消费组形式消费数据）等。

- Hash 类型：缓存对象、购物车等。

  - 购物车：
  - 添加商品：`HSET cart:{用户id} {商品id} 1`
  - 添加数量：`HINCRBY cart:{用户id} {商品id} 1`
  - 商品总数：`HLEN cart:{用户id}`
  - 删除商品：`HDEL cart:{用户id} {商品id}`
  - 获取购物车所有商品：`HGETALL cart:{用户id}`

- Set 类型：聚合计算（并集、交集、差集）场景，比如点赞、共同关注、抽奖活动等。

- Zset 类型：排序场景，比如排行榜、电话和姓名排序等。

Redis 后续版本又支持四种数据类型，它们的应用场景如下：

- BitMap（2.2 版新增）：二值状态统计的场景，比如签到、判断用户登陆状态、连续签到用户总数等；
- HyperLogLog（2.8 版新增）：适合对计算精度不高，但工作量大的工作 （例如统计网站的独立访问者数量、统计搜索引擎中的不同搜索词数量、实时日志分析等。）
- GEO（3.2 版新增）：存储地理位置信息的场景，比如滴滴叫车；
- Stream（5.0 版新增）：消息队列，相比于基于 List 类型实现的消息队列，有这两个特有的特性：自动生成全局唯一消息ID，支持以消费组形式消费数据。



### string

#### 介绍

String 是最基本的 key-value 结构，key 是唯一标识，value 是具体的值，value其实不仅是**字符串**， 也可以是**数字（整数或浮点数）**，value 最多可以容纳的数据长度是 `512M`。

#### 底层

String 类型的底层的数据结构实现主要是  int 和**SDS（简单动态字符串**）。 SDS 和我们认识的 C 字符串不太一样，之所以没有使用 C 语言的字符串表示，因为 SDS 相比于 C 的原生字符串：

- **SDS 不仅可以保存文本数据，还可以保存二进制数据**。因为 SDS 使用 len 属性的值而不是空字符来判断字符串是否结束，并且 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 buf[] 数组里的数据。所以 SDS 不光能存放文本数据，而且能保存图片、音频、视频、压缩文件这样的二进制数据。
- **SDS 获取字符串长度的时间复杂度是 O(1)**。因为 C 语言的字符串并不记录自身长度，所以获取长度的复杂度为 O(n)；而 SDS 结构里用 len 属性记录了字符串长度，所以复杂度为 O(1)。
- **Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出**。因为 SDS 在拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容，所以不会导致缓冲区溢出的问题。

字符串对象的内部编码（encoding）有 3 种 ：**int、raw和 embstr**。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011747592.png" alt="string_E7_BB_93_E6_9E_84" style="zoom: 33%;" />

1. 如果一个字符串对象保存的是**整数值**，并且这个整数值可以用`long`类型来表示，那么字符串对象会将整数值保存在字符串对象结构的`ptr`属性里面（将`void*`转换成 long），并将字符串对象的编码设置为`int`。

   <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011754262.png" alt="int" style="zoom: 33%;" />

2. 如果字符串对象保存的是一个字符串，并且这个字符申的长度**小于等于** 32 字节（redis 2.+版本），那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串，并将对象的编码设置为`embstr`， `embstr`编码是专门用于保存短字符串的一种优化编码方式：

   - redis 2.+ 是 32 字节
   - redis 3.0-4.0 是 39 字节
   - redis 5.0 是 44 字节

   <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011754519.png" alt="embstr" style="zoom: 33%;" />

3. 如果字符串对象保存的是一个字符串，并且这个字符串的长度大于 32 字节（redis 2.+版本），那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串，并将对象的编码设置为`raw`：

   <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011755585.png" alt="raw" style="zoom: 33%;" />

`embstr`和`raw`编码都会使用`SDS`来保存值，但不同之处在于`embstr`会通过一次内存分配函数来分配一块连续的内存空间来保存`redisObject`和`SDS`，而`raw`编码会通过调用两次内存分配函数来分别分配两块空间来保存`redisObject`和`SDS`。Redis这样做会有很多好处：

- `embstr`编码将创建字符串对象所需的内存分配次数从 `raw` 编码的两次降低为一次；
- 释放 `embstr`编码的字符串对象同样只需要调用一次内存释放函数；
- 因为`embstr`编码的字符串对象的所有数据都保存在一块连续的内存里面可以更好的利用 CPU 缓存提升性能。

但是 embstr 也有缺点的：

- 如果字符串的长度增加需要重新分配内存时，整个redisObject和sds都需要重新分配空间，所以**embstr编码的字符串对象实际上是只读的**。当我们对embstr编码的字符串对象执行任何修改命令（例如append）时，程序会先将对象的编码从embstr转换成raw，然后再执行修改命令



#### shell操作

```shell
# 设置 key-value 类型的值
> SET name lin
OK
# 根据 key 获得对应的 value
> GET name
"lin"
# 所有key
>keys all
# 判断某个 key 是否存在
> EXISTS name
(integer) 1
# 返回 key 所储存的字符串值的长度
> STRLEN name
(integer) 3
# 删除某个 key 对应的值
> DEL name
(integer) 1
```

批量设置 :

```shell
# 批量设置 key-value 类型的值
> MSET key1 value1 key2 value2 
OK
# 批量获取多个 key 对应的 value
> MGET key1 key2 
1) "value1"
2) "value2"
```

计数器（字符串的内容为整数的时候可以使用）：

```shell
# 设置 key-value 类型的值
> SET number 0
OK
# 将 key 中储存的数字值增一
> INCR number
(integer) 1
# 将key中存储的数字值加 10
> INCRBY number 10
(integer) 11
# 将 key 中储存的数字值减一
> DECR number
(integer) 10
# 将key中存储的数字值键 10
> DECRBY number 10
(integer) 0
```

过期（默认为永不过期）：

```bash
# 设置 key 在 60 秒后过期（该方法是针对已经存在的key设置过期时间）
> EXPIRE name  60 
(integer) 1
# 查看数据还有多久过期
> TTL name 
(integer) 51

#设置 key-value 类型的值，并设置该key的过期时间为 60 秒
> SET key  value EX 60
OK
> SETEX key  60 value
OK
```

不存在就插入：

```shell
# 不存在就插入（not exists）
>SETNX key value
(integer) 1
```



### list

####  介绍

List 列表是简单的字符串列表，**按照插入顺序排序**，可以从头部或尾部向 List 列表添加元素。

列表的最大长度为 `2^32 - 1`，也即每个列表支持超过 `40 亿`个元素。

#### 底层

###### 旧

List 类型的底层数据结构是由**双向链表或压缩列表**实现的：

- 如果列表的元素个数小于 `512` 个（默认值，可由 `list-max-ziplist-entries` 配置），列表每个元素的值都小于 `64` 字节（默认值，可由 `list-max-ziplist-value` 配置），Redis 会使用**压缩列表**作为 List 类型的底层数据结构；
- 如果列表的元素不满足上面的条件，Redis 会使用**双向链表**作为 List 类型的底层数据结构；

###### 新3.2 版本之后

**在 Redis 3.2 版本之后，List 数据类型底层数据结构就只由 quicklist 实现了，替代了双向链表和压缩列表**



#### shell操作

可重复 有插入顺序

```Bash
# 将一个或多个值value插入到key列表的表头(最左边)，最后的值在最前面
LPUSH key value [value ...] 
# 将一个或多个值value插入到key列表的表尾(最右边)
RPUSH key value [value ...]
# 移除并返回key列表的头元素
LPOP key     
# 移除并返回key列表的尾元素
RPOP key 

# 返回列表key中指定区间内的元素，区间以偏移量start和stop指定，从0开始
LRANGE key start stop
lrange key #所有

llen key #长度
ltrim 123 1 3 #只保留1-3 之间的元素

# 从key列表表头弹出一个元素，没有就阻塞timeout秒，如果timeout=0则一直阻塞
BLPOP key [key ...] timeout
# 从key列表表尾弹出一个元素，没有就阻塞timeout秒，如果timeout=0则一直阻塞
BRPOP key [key ...] timeout
```

#### 消息队列

消息队列在存取消息时，必须要满足三个需求，分别是**消息保序、处理重复的消息和保证消息可靠性**。

- 消息保序：使用 LPUSH + RPOP；
- 阻塞读取：使用 BRPOP；
- 重复消息处理：生产者自行实现全局唯一 ID；
- 消息的可靠性：使用 BRPOPLPUSH



### Hash

#### 介绍

Hash 是一个键值对（key - value）集合，其中 value 的形式如： `value=[{field1，value1}，...{fieldN，valueN}]`。Hash 特别适合用于存储对象。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011830582.png" alt="hash" style="zoom: 33%;" />

#### 底层实现

###### 旧

Hash 类型的底层数据结构是由**压缩列表或哈希表**实现的：

- 如果哈希类型元素个数小于 `512` 个（默认值，可由 `hash-max-ziplist-entries` 配置），所有值小于 `64` 字节（默认值，可由 `hash-max-ziplist-value` 配置）的话，Redis 会使用**压缩列表**作为 Hash 类型的底层数据结构；
- 如果哈希类型元素不满足上面条件，Redis 会使用**哈希表**作为 Hash 类型的 底层数据结构。

###### 新Redis 7.0 

**在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了**

#### shell操作

类似表，存储键值对

**存储对象**

```Bash
# 存储一个哈希表key的键值
HSET key field value   
# 获取哈希表key对应的field键值
HGET key field

# 在一个哈希表key中存储多个键值对
HMSET key field value [field value...] 
# 批量获取哈希表key中多个field键值
HMGET key field [field ...]       
# 删除哈希表key中的field键值
HDEL key field [field ...]    

# 返回哈希表key中field的数量
HLEN key       
# 返回哈希表key中所有的键值
HGETALL key 

# 为哈希表key中field键的值加上增量n
HINCRBY key field n 

hexists person age
0
```



### Set

#### 介绍

Set 类型是一个无序并唯一的键值集合，它的存储顺序不会按照插入的先后顺序进行存储。

一个集合最多可以存储 `2^32-1` 个元素。概念和数学中个的集合基本类似，可以交集，并集，差集等等

#### 内部实现

Set 类型的底层数据结构是由**哈希表或整数集合**实现的：

- 如果集合中的元素都是整数且元素个数小于 `512` （默认值，`set-maxintset-entries`配置）个，Redis 会使用**整数集合**作为 Set 类型的底层数据结构；
- 如果集合中的元素不满足上面条件，则 Redis 使用**哈希表**作为 Set 类型的底层数据结构。

#### shell

不可重复 无序 命令都以s开头

```Bash
# 往集合key中存入元素，元素存在则忽略，若key不存在则新建
SADD key member [member ...]
# 从集合key中删除元素
SREM key member [member ...] 
# 获取集合key中所有元素
SMEMBERS key
# 获取集合key中的元素个数
SCARD key

# 判断member元素是否存在于集合key中
SISMEMBER key member

# 从集合key中随机选出count个元素，元素不从key中删除
SRANDMEMBER key [count]
# 从集合key中随机选出count个元素，元素从key中删除
SPOP key [count]
 
 
 Set 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计算，会导致 Redis 实例阻塞。.
 #集合运算
 SINTER key [key ...] #返回一个集合的全部成员，多个集合则返回交集。
 SUNION        key [key ...] #返回一个集合的全部成员，多个集合则返回并集。
 SDIFF        key [key ...] #返回一个集合的全部成员，多个集合则返回差集。
 
 # 交集运算
SINTER key [key ...]
# 将交集结果存入新集合destination中
SINTERSTORE destination key [key ...]

# 并集运算
SUNION key [key ...]
# 将并集结果存入新集合destination中
SUNIONSTORE destination key [key ...]

# 差集运算
SDIFF key [key ...]
# 将差集结果存入新集合destination中
SDIFFSTORE destination key [key ...]
```

#### 应用场景

##### 点赞

Set 类型可以保证一个用户只能点一个赞，这里举例子一个场景，key 是文章id，value 是用户id。

`uid:1` 、`uid:2`、`uid:3` 三个用户分别对 article:1 文章点赞了。

```shell
# uid:1 用户对文章 article:1 点赞
> SADD article:1 uid:1
(integer) 1
# uid:2 用户对文章 article:1 点赞
> SADD article:1 uid:2
(integer) 1
# uid:3 用户对文章 article:1 点赞
> SADD article:1 uid:3
(integer) 1
```

`uid:1` 取消了对 article:1 文章点赞。

```text
> SREM article:1 uid:1
(integer) 1
```

获取 article:1 文章所有点赞用户 :

```shell
> SMEMBERS article:1
1) "uid:3"
2) "uid:2"
```

获取 article:1 文章的点赞用户数量：

```shell
> SCARD article:1
(integer) 2
```

判断用户 `uid:1` 是否对文章 article:1 点赞了：

```shell
> SISMEMBER article:1 uid:1
(integer) 0  # 返回0说明没点赞，返回1则说明点赞了
```

##### 共同关注

Set 类型支持交集运算，所以可以用来计算共同关注的好友、公众号等。

key 可以是用户id，value 则是已关注的公众号的id。

`uid:1` 用户关注公众号 id 为 5、6、7、8、9，`uid:2` 用户关注公众号 id 为 7、8、9、10、11。

```shell
# uid:1 用户关注公众号 id 为 5、6、7、8、9
> SADD uid:1 5 6 7 8 9
(integer) 5
# uid:2  用户关注公众号 id 为 7、8、9、10、11
> SADD uid:2 7 8 9 10 11
(integer) 5
```

`uid:1` 和 `uid:2` 共同关注的公众号：

```shell
# 获取共同关注
> SINTER uid:1 uid:2
1) "7"
2) "8"
3) "9"
```

给 `uid:2` 推荐 `uid:1` 关注的公众号：

```shell
> SDIFF uid:1 uid:2
1) "5"
2) "6"
```

验证某个公众号是否同时被 `uid:1` 或 `uid:2` 关注:

```shell
> SISMEMBER uid:1 5
(integer) 1 # 返回0，说明关注了
> SISMEMBER uid:2 5
(integer) 0 # 返回0，说明没关注
```

##### 抽奖活动

存储某活动中中奖的用户名 ，Set 类型因为有去重功能，可以保证同一个用户不会中奖两次。

key为抽奖活动名，value为员工名称，把所有员工名称放入抽奖箱 ：

```shell
>SADD lucky Tom Jerry John Sean Marry Lindy Sary Mark
(integer) 5
```

如果允许重复中奖，可以使用 SRANDMEMBER 命令。

```shell
# 抽取 1 个一等奖：
> SRANDMEMBER lucky 1
1) "Tom"
# 抽取 2 个二等奖：
> SRANDMEMBER lucky 2
1) "Mark"
2) "Jerry"
# 抽取 3 个三等奖：
> SRANDMEMBER lucky 3
1) "Sary"
2) "Tom"
3) "Jerry"
```

如果不允许重复中奖，可以使用 SPOP 命令。

```shell
# 抽取一等奖1个
> SPOP lucky 1
1) "Sary"
# 抽取二等奖2个
> SPOP lucky 2
1) "Jerry"
2) "Mark"
# 抽取三等奖3个
> SPOP lucky 3
1) "John"
2) "Sean"
3) "Lindy"
```



### SortedSet（ZSet）

#### 介绍

Zset 类型（有序集合类型）相比于 Set 类型多了一个排序属性 score（分值），对于有序集合 ZSet 来说，每个存储元素相当于有两个值组成的，一个是有序集合的元素值，一个是排序值。

有序集合保留了集合不能有重复成员的特性（分值可以重复），但不同的是，有序集合中的元素可以排序。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011845811.png" alt="zset" style="zoom:50%;" />

#### 底层

##### 旧

Zset 类型的底层数据结构是由**压缩列表或跳表**实现的：

- 如果有序集合的元素个数小于 `128` 个，并且每个元素的值小于 `64` 字节时，Redis 会使用**压缩列表**作为 Zset 类型的底层数据结构；
- 如果有序集合的元素不满足上面的条件，Redis 会使用**跳表**作为 Zset 类型的底层数据结构；

##### 新Redis 7.0 

在Redis 7.0 中，压缩列表数据结构已经废弃了，交由 **listpack** 数据结构来实现了



#### shell

以z开头 每个元素关联一个浮点类型的分数

```Bash
# 往有序集合key中加入带分值元素
ZADD key score member [[score member]...]   
# 往有序集合key中删除元素
ZREM key member [member...]                 
# 返回有序集合key中元素member的分值
ZSCORE key member
# 返回有序集合key中元素个数
ZCARD key 

# 为有序集合key中元素member的分值加上increment
ZINCRBY key increment member 


zrank name 张三 #输出的是升序的下标
2 
zrevrank name 张三 #输出排名 0开始
0

# 正序获取有序集合key从start下标到stop下标的元素
ZRANGE key start stop [WITHSCORES]
# 倒序获取有序集合key从start下标到stop下标的元素
ZREVRANGE key start stop [WITHSCORES]

# 返回有序集合中指定分数区间内的成员，分数由低到高排序。
ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]

# 返回指定成员区间内的成员，按字典正序排列, 分数必须相同。
ZRANGEBYLEX key min max [LIMIT offset count]
# 返回指定成员区间内的成员，按字典倒序排列, 分数必须相同
ZREVRANGEBYLEX key max min [LIMIT offset count]


Zset 运算操作（相比于 Set 类型，ZSet 类型没有支持差集运算）：
# 并集计算(相同元素分值相加)，numberkeys一共多少个key，WEIGHTS每个key对应的分值乘积
ZUNIONSTORE destkey numberkeys key [key...] 
# 交集计算(相同元素分值相加)，numberkeys一共多少个key，WEIGHTS每个key对应的分值乘积
ZINTERSTORE destkey numberkeys key [key...]
```

#### 应用场景

Zset 类型（Sorted Set，有序集合） 可以根据元素的权重来排序，我们可以自己来决定每个元素的权重值。比如说，我们可以根据元素插入 Sorted Set 的时间确定权重值，先插入的元素权重小，后插入的元素权重大。

在面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，可以优先考虑使用 Sorted Set。

##### [#](https://xiaolincoding.com/redis/data_struct/command.html#排行榜)排行榜

有序集合比较典型的使用场景就是排行榜。例如学生成绩的排名榜、游戏积分排行榜、视频播放排名、电商系统中商品的销量排名等。

我们以博文点赞排名为例，小林发表了五篇博文，分别获得赞为 200、40、100、50、150。

```shell
# arcticle:1 文章获得了200个赞
> ZADD user:xiaolin:ranking 200 arcticle:1
(integer) 1
# arcticle:2 文章获得了40个赞
> ZADD user:xiaolin:ranking 40 arcticle:2
(integer) 1
# arcticle:3 文章获得了100个赞
> ZADD user:xiaolin:ranking 100 arcticle:3
(integer) 1
# arcticle:4 文章获得了50个赞
> ZADD user:xiaolin:ranking 50 arcticle:4
(integer) 1
# arcticle:5 文章获得了150个赞
> ZADD user:xiaolin:ranking 150 arcticle:5
(integer) 1
```

文章 arcticle:4 新增一个赞，可以使用 ZINCRBY 命令（为有序集合key中元素member的分值加上increment）：

```shell
> ZINCRBY user:xiaolin:ranking 1 arcticle:4
"51"
```

查看某篇文章的赞数，可以使用 ZSCORE 命令（返回有序集合key中元素个数）：

```shell
> ZSCORE user:xiaolin:ranking arcticle:4
"50"
```

获取小林文章赞数最多的 3 篇文章，可以使用 ZREVRANGE 命令（倒序获取有序集合 key 从start下标到stop下标的元素）：

```shell
# WITHSCORES 表示把 score 也显示出来
> ZREVRANGE user:xiaolin:ranking 0 2 WITHSCORES
1) "arcticle:1"
2) "200"
3) "arcticle:5"
4) "150"
5) "arcticle:3"
6) "100"
```

获取小林 100 赞到 200 赞的文章，可以使用 ZRANGEBYSCORE 命令（返回有序集合中指定分数区间内的成员，分数由低到高排序）：

```shell
> ZRANGEBYSCORE user:xiaolin:ranking 100 200 WITHSCORES
1) "arcticle:3"
2) "100"
3) "arcticle:5"
4) "150"
5) "arcticle:1"
6) "200"
```

##### [#](https://xiaolincoding.com/redis/data_struct/command.html#电话、姓名排序)电话、姓名排序

使用有序集合的 `ZRANGEBYLEX` 或 `ZREVRANGEBYLEX` 可以帮助我们实现电话号码或姓名的排序，我们以 `ZRANGEBYLEX` （返回指定成员区间内的成员，按 key 正序排列，分数必须相同）为例。

**注意：不要在分数不一致的 SortSet 集合中去使用 ZRANGEBYLEX和 ZREVRANGEBYLEX 指令，因为获取的结果会不准确。**

*1、电话排序*

我们可以将电话号码存储到 SortSet 中，然后根据需要来获取号段：

```shell
> ZADD phone 0 13100111100 0 13110114300 0 13132110901 
(integer) 3
> ZADD phone 0 13200111100 0 13210414300 0 13252110901 
(integer) 3
> ZADD phone 0 13300111100 0 13310414300 0 13352110901 
(integer) 3
```

获取所有号码:

```shell
> ZRANGEBYLEX phone - +
1) "13100111100"
2) "13110114300"
3) "13132110901"
4) "13200111100"
5) "13210414300"
6) "13252110901"
7) "13300111100"
8) "13310414300"
9) "13352110901"
```

获取 132 号段的号码：

```shell
> ZRANGEBYLEX phone [132 (133
1) "13200111100"
2) "13210414300"
3) "13252110901"
```

获取132、133号段的号码：

```shell
> ZRANGEBYLEX phone [132 (134
1) "13200111100"
2) "13210414300"
3) "13252110901"
4) "13300111100"
5) "13310414300"
6) "13352110901"
```

*2、姓名排序*

```shell
> zadd names 0 Toumas 0 Jake 0 Bluetuo 0 Gaodeng 0 Aimini 0 Aidehua 
(integer) 6
```

获取所有人的名字:

```shell
> ZRANGEBYLEX names - +
1) "Aidehua"
2) "Aimini"
3) "Bluetuo"
4) "Gaodeng"
5) "Jake"
6) "Toumas"
```

获取名字中大写字母A开头的所有人：

```shell
> ZRANGEBYLEX names [A (B
1) "Aidehua"
2) "Aimini"
```

获取名字中大写字母 C 到 Z 的所有人：

```shell
> ZRANGEBYLEX names [C [Z
1) "Gaodeng"
2) "Jake"
3) "Toumas"
```



### 位图Bitmap

#### 介绍

Bitmap，即位图，是一串连续的二进制数组（0和1），可以通过偏移量（offset）定位元素。BitMap通过最小的单位bit来进行`0|1`的设置，表示某个元素的值或者状态，时间复杂度为O(1)。

由于 bit 是计算机中最小的单位，使用它进行储存将非常节省空间，特别适合一些数据量大且使用**二值统计的场景**。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409012306089.png" alt="bitmap" style="zoom:50%;" />

#### 底层

Bitmap 本身是用 String 类型作为底层数据结构实现的一种统计二值状态的数据类型。

String 类型是会保存为二进制的字节数组，所以，Redis 就把字节数组的每个 bit 位利用起来，用来表示一个元素的二值状态，你可以把 Bitmap 看作是一个 bit 数组

#### shell

Redis位图（Bitmap）是一种数据结构，它用于表示和操作位（0或1）的集合或数组。Redis中的位图是一个非常紧凑的数据结构，通常用于处理大量 布尔型数据**/**二进制数据，如**用户的在线状态、用户的行为记录、标记用户是否喜欢某个内容**等等。

以下是Redis位图的主要特点和用途：

1. **位操作**：Redis位图支持位操作，允许你在位级别上执行操作，如设置位、清除位、翻转位等。这使得位图非常适合于处理包含大量二进制标志的数据集。
2. **紧凑的存储**：Redis位图使用非常紧凑的内存表示，因为它存储在位级别上。这使得它非常适合处理大规模的二进制数据。
3. **原子操作**：Redis位图支持原子操作，这意味着你可以在多个客户端同时并发访问和更新位图，而不用担心数据一致性问题。
4. **位计数**：Redis提供了位计数功能，可以方便地统计位图中值为1的位的数量。这对于计算集合的基数非常有用。
5. **位图索引**：你可以使用位图的索引来访问和修改位图中的特定位。
6. **位图运算**：Redis支持位图之间的运算，如与、或、异或等操作，这使得位图可以用于处理集合的交集、并集等运算。

下面是一些常见的Redis位图命令：

- `SETBIT key offset value`：设置位图中指定偏移量的位的值。
- `GETBIT key offset`：获取位图中指定偏移量的位的值。
- `BITCOUNT key [start end]`：统计位图中值为1的位的数量。
- `BITOP operation destkey key [key ...]`：对多个位图执行位操作（与、或、异或等）并将结果存储到目标位图中。

```Bash
#redis中的bitmap本质是字符串，可以使用字符串的方式来设置
127.0.0.1:6379> set dianzan "\xF0" #巧妙使用十六进制表示二进制 11110000
OK

# 设置值，其中value只能是 0 和 1
SETBIT key offset value

# 获取值
GETBIT key offset

# 获取指定范围内值为 1 的个数
# start 和 end 以字节为单位
BITCOUNT key start end
127.0.0.1:6379> bitcount dianzan # 统计位图中值为1的位的数量。
4


# BitMap间的运算
# operations 位移操作符，枚举值
  AND 与运算 &
  OR 或运算 |
  XOR 异或 ^
  NOT 取反 ~
# result 计算的结果，会存储在该key中
# key1 … keyn 参与运算的key，可以有多个，空格分割，not运算只能一个key
# 当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 0。返回值是保存到 destkey 的字符串的长度（以字节byte为单位），和输入 key 中最长的字符串长度相等。
BITOP [operations] [result] [key1] [keyn…]

# 返回指定key中第一次出现指定value(0/1)的位置
BITPOS [key] [value]
```

#### 应用场景

Bitmap 类型非常适合二值状态统计的场景，这里的二值状态就是指集合元素的取值就只有 0 和 1 两种，在记录海量数据时，Bitmap 能够有效地节省内存空间。

##### [#](https://xiaolincoding.com/redis/data_struct/command.html#签到统计)签到统计

在签到打卡的场景中，我们只用记录签到（1）或未签到（0），所以它就是非常典型的二值状态。

签到统计时，每个用户一天的签到用 1 个 bit 位就能表示，一个月（假设是 31 天）的签到情况用 31 个 bit 位就可以，而一年的签到也只需要用 365 个 bit 位，根本不用太复杂的集合类型。

假设我们要统计 ID 100 的用户在 2022 年 6 月份的签到情况，就可以按照下面的步骤进行操作。

第一步，执行下面的命令，记录该用户 6 月 3 号已签到。

```shell
SETBIT uid:sign:100:202206 2 1
```

第二步，检查该用户 6 月 3 日是否签到。

```shell
GETBIT uid:sign:100:202206 2 
```

第三步，统计该用户在 6 月份的签到次数。

```shell
BITCOUNT uid:sign:100:202206
```

这样，我们就知道该用户在 6 月份的签到情况了。

> 如何统计这个月首次打卡时间呢？

Redis 提供了 `BITPOS key bitValue [start] [end]`指令，返回数据表示 Bitmap 中第一个值为 `bitValue` 的 offset 位置。

在默认情况下， 命令将检测整个位图， 用户可以通过可选的 `start` 参数和 `end` 参数指定要检测的范围。所以我们可以通过执行这条命令来获取 userID = 100 在 2022 年 6 月份**首次打卡**日期：

```text
BITPOS uid:sign:100:202206 1
```

需要注意的是，因为 offset 从 0 开始的，所以我们需要将返回的 value + 1 。

##### [#](https://xiaolincoding.com/redis/data_struct/command.html#判断用户登陆态)判断用户登陆态

Bitmap 提供了 `GETBIT、SETBIT` 操作，通过一个偏移值 offset 对 bit 数组的 offset 位置的 bit 位进行读写操作，需要注意的是 offset 从 0 开始。

只需要一个 key = login_status 表示存储用户登陆状态集合数据， 将用户 ID 作为 offset，在线就设置为 1，下线设置 0。通过 `GETBIT`判断对应的用户是否在线。 5000 万用户只需要 6 MB 的空间。

假如我们要判断 ID = 10086 的用户的登陆情况：

第一步，执行以下指令，表示用户已登录。

```shell
SETBIT login_status 10086 1
```

第二步，检查该用户是否登陆，返回值 1 表示已登录。

```text
GETBIT login_status 10086
```

第三步，登出，将 offset 对应的 value 设置成 0。

```shell
SETBIT login_status 10086 0
```

##### [#](https://xiaolincoding.com/redis/data_struct/command.html#连续签到用户总数)连续签到用户总数

如何统计出这连续 7 天连续打卡用户总数呢？

我们把每天的日期作为 Bitmap 的 key，userId 作为 offset，若是打卡则将 offset 位置的 bit 设置成 1。

key 对应的集合的每个 bit 位的数据则是一个用户在该日期的打卡记录。

一共有 7 个这样的 Bitmap，如果我们能对这 7 个 Bitmap 的对应的 bit 位做 『与』运算。同样的 UserID offset 都是一样的，当一个 userID 在 7 个 Bitmap 对应对应的 offset 位置的 bit = 1 就说明该用户 7 天连续打卡。

结果保存到一个新 Bitmap 中，我们再通过 `BITCOUNT` 统计 bit = 1 的个数便得到了连续打卡 7 天的用户总数了。

Redis 提供了 `BITOP operation destkey key [key ...]`这个指令用于对一个或者多个 key 的 Bitmap 进行位元操作。

- `operation` 可以是 `and`、`OR`、`NOT`、`XOR`。当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 `0` 。空的 `key` 也被看作是包含 `0` 的字符串序列。

假设要统计 3 天连续打卡的用户数，则是将三个 bitmap 进行 AND 操作，并将结果保存到 destmap 中，接着对 destmap 执行 BITCOUNT 统计，如下命令：

```shell
# 与操作
BITOP AND destmap bitmap:01 bitmap:02 bitmap:03
# 统计 bit 位 =  1 的个数
BITCOUNT destmap
```

即使一天产生一个亿的数据，Bitmap 占用的内存也不大，大约占 12 MB 的内存（10^8/8/1024/1024），7 天的 Bitmap 的内存开销约为 84 MB。同时我们最好给 Bitmap 设置过期时间，让 Redis 删除过期的打卡数据，节省内存。





### 消息队列Stream

#### 介绍

Redis Stream 是 Redis 5.0 版本新增加的数据类型，Redis 专门为消息队列设计的数据类型。用于处理实时事件流或日志数据。它提供了一种有序、持久化、可扩展的方式来存储和处理消息流。

在 Redis 5.0 Stream 没出来之前，消息队列的实现方式都有着各自的缺陷，例如：

- 发布订阅模式，不能持久化也就无法可靠的保存消息，并且对于离线重连的客户端不能读取历史消息的缺陷；
- List 实现消息队列的方式不能重复消费，一个消息消费完就会被删除，而且生产者需要自行实现全局唯一 ID。

基于以上问题，Redis 5.0 便推出了 Stream 类型也是此版本最重要的功能，用于完美地实现消息队列，它支持消息的**持久化、支持自动生成全局唯一 ID、支持 ack 确认消息的模式、支持消费组模式**等，让消息队列更加的稳定和可靠

1. **消息**：Redis Stream中的数据单元称为消息。每个消息都有一个唯一的ID，用于标识消息在Stream中的位置。
2. **Stream名称**：每个Stream都有一个唯一的名称，用于标识它。Stream名称是一个字符串。
3. **偏移量（Offset）**：偏移量用于表示消息在Stream中的位置。每个消息都有一个关联的偏移量，用于定位消息。
4. **消费者组**：消费者组是一组消费者的集合，它们共同协作来消费Stream中的消息。每个消费者组有一个唯一的名称。
5. **消费者**：消费者是消费者组中的单个成员，用于处理Stream中的消息。消费者可以在不同的消费者组中存在。
6. **消息生产和插入**：使用`XADD`命令可以向Stream中插入新消息，每个消息都有一个唯一的ID。ID可以由Redis自动生成，也可以由客户端指定。
7. **消息消费和确认**：使用`XREADGROUP`命令可以从Stream中读取消息，并将其分配给消费者组中的消费者。消费者需要使用`XACK`命令来确认已经处理了某条消息。
8. **持久性**：Stream中的消息是持久性的，它们存储在内存中，并且可以配置成在内存满时将消息写入磁盘，以防止数据丢失。
9. **有序性**：消息在Stream中按照插入的顺序有序存储，并且每个消息都有一个唯一的ID。这使得Stream非常适合处理有序事件流。
10. **多消费者支持**：多个消费者可以同时订阅同一个Stream，并且每个消费者可以处理不同的消息。



#### 底层



#### 和Kafka比较

以下是 Redis Streams 相对于 Kafka 的一些劣处：

##### 1. **持久性和可靠性**
- **数据持久性**：Kafka 是一个持久化的消息系统，所有消息都可以持久化到磁盘，保证在系统崩溃时不会丢失。而 Redis Streams 虽然支持持久化，但仍以内存为主，数据可能因为故障或重启而丢失。
- **消息确认机制**：Kafka 提供内置的消息确认机制，确保消息至少被消费一次（at-least-once）或仅被消费一次（exactly-once）。Redis Streams 需要开发者手动实现确认机制，这可能增加出错的可能性。

##### 2. **扩展性和可伸缩性**
- **水平扩展能力**：Kafka 设计为分布式系统，能够轻松水平扩展，支持高并发的消息生产和消费。Redis Streams 虽然可以通过主从复制和分片实现一定的扩展，但在高并发写入和数据量大时，可能会面临性能瓶颈。
- **数据分区**：Kafka 使用分区的概念来分散负载和实现并行处理，每个分区可以独立消费，提高吞吐量。Redis Streams 没有原生的分区支持，无法充分利用多个消费者并行处理消息。

##### 3. **消息路由和处理能力**
- **复杂路由和过滤**：Kafka 支持基于主题的消息路由，可以根据不同的主题将消息发送到不同的消费者组。Redis Streams 的消息路由功能有限，缺乏复杂的过滤和主题机制。
- **消费模型**：Kafka 的消费者组模型允许多个消费者共享负载，Redis Streams 需要手动管理消费者组和消息的分配，增加了开发复杂性。

##### 消息堆积

- 消息可堆积。

*2、Redis Stream 消息可堆积吗？*

Redis 的数据都存储在内存中，这就意味着一旦发生消息积压，则会导致 Redis 的内存持续增长，如果超过机器内存上限，就会面临被 OOM 的风险。

所以 Redis 的 Stream 提供了可以指定队列最大长度的功能，就是为了避免这种情况发生。

当指定队列最大长度时，队列长度超过上限后，旧消息会被删除，只保留固定长度的新消息。这么来看，Stream 在消息积压时，如果指定了最大长度，还是有可能丢失消息的。



所以，能不能将 Redis 作为消息队列来使用，关键看你的业务场景：

- 如果你的业务场景足够简单，对于数据丢失不敏感，而且消息积压概率比较小的情况下，把 Redis 当作队列是完全可以的。
- 如果你的业务有海量消息，消息积压的概率比较大，并且不能接受数据丢失，那么还是用专业的消息队列中间件吧。



##### 4. **数据保留策略**
- **消息保留机制**：Kafka 支持配置消息保留策略（基于时间或大小），允许消息在一段时间内保留，便于后续消费和分析。Redis Streams 的消息过期和删除策略比较简单，可能会导致重要数据被提前丢弃。
- **回溯消费**：Kafka 允许消费者回溯到任意偏移量进行消息消费，而 Redis Streams 的消费策略较为简单，难以支持复杂的回溯需求。

##### 5. **监控和管理工具**
- **监控和管理功能**：Kafka 有丰富的监控和管理工具（如 Kafka Manager、Confluent Control Center），可以监控消息流、消费者状态等。Redis 的管理和监控工具相对简单，缺乏针对 Streams 的专用监控工具。
- **可视化支持**：Kafka 的生态系统提供了更强大的可视化工具，帮助开发者和运维人员更好地理解和管理消息流，而 Redis 的可视化支持较弱。

##### 6. **运维复杂性**
- **集群管理**：Kafka 集群的管理和配置较为复杂，但提供了强大的功能和灵活性。相对而言，虽然 Redis Streams 也可以配置为集群，但其分布式特性不如 Kafka 强大，运维成本可能更高。
- **故障恢复**：Kafka 提供了丰富的故障恢复机制（如副本机制），能在节点故障时自动恢复，而 Redis Streams 的故障恢复和数据复制机制相对简单。

##### 7. **社区和生态系统**
- **生态系统支持**：Kafka 拥有丰富的生态系统（如 Kafka Connect、Kafka Streams、KSQL 等），可用于集成、数据流处理和实时分析。Redis Streams 的生态系统相对较小，缺乏类似的工具和集成能力。



#### shell

**以x开头**

Stream 消息队列操作命令：

- XADD：插入消息，保证有序，可以自动生成全局唯一 ID；
- XLEN ：查询消息长度；
- XREAD：用于读取消息，可以按 ID 读取数据；
- XDEL ： 根据消息 ID 删除消息；
- DEL ：删除整个 Stream；
- XRANGE ：读取区间消息
- XREADGROUP：按消费组形式读取消息； **不同消费组的消费者可以消费同一条消息（但是有前提条件，创建消息组的时候，不同消费组指定了相同位置开始读取消息）**
- XPENDING 和 XACK：
  - XPENDING 命令可以用来查询每个消费组内所有消费者「已读取、但尚未确认」的消息；
  - XACK 命令用于向消息队列确认消息处理已完成





```Bash
127.0.0.1:6379> xadd kkk * name lisi #*表示自动生成序号 
"1696606173120-0"  #  时间戳-序号
127.0.0.1:6379> xadd kkk 1696606185991-0 age 14
"1696606185991-0"
127.0.0.1:6379> xlen kkk #长度
(integer) 2
127.0.0.1:6379> xrange kkk - + #遍历   - + 表示所有 
1) 1) "1696606173120-0"
   2) 1) "name"
      2) "lisi"
2) 1) "1696606185991-0"
   2) 1) "age"
      2) "14"
127.0.0.1:6379> xdel kkk 1696606173120-0 #《1 删除
(integer) 1
127.0.0.1:6379> xtrim kkk maxlen 0  # 《1删除
(integer) 1


127.0.0.1:6379> xread count 2 block 1000 streams kkk 0 #读取消息 2个 如果没有就阻塞1000毫秒  从0开始读取 
1) 1) "kkk"
   2) 1) 1) "1696606185991-1"
         2) 1) "age"
            2) "14"
      2) 1) "1696606749261-0"
         2) 1) "name"
            2) "zhangsan"
127.0.0.1:6379> xrange kkk - +  #没消失，可以多次读取
1) 1) "1696606185991-1"
   2) 1) "age"
      2) "14"
2) 1) "1696606749261-0"
   2) 1) "name"
      2) "zhangsan"

#创建消费者组
127.0.0.1:6379> xadd kkk * name lisi
1696835318114-0
127.0.0.1:6379> xgroup create kkk group1 0  #偏移量为0
OK

127.0.0.1:6379> xinfo groups kkk  #查看消费者组的信息
name
group1
consumers
0
pending
0
last-delivered-id
0-0
entries-read

lag
3
#在消费者组里创建消费者
127.0.0.1:6379> xgroup createconsumer kkk group1 consumer1 
1
127.0.0.1:6379> xgroup createconsumer kkk group1 consumer2
1
#从Stream中读取消息并将其分配给名为 "group1" 的消费者组中的 "consumer1" 消费者
127.0.0.1:6379> xreadgroup group group1 consumer1 count 2 block 1000 streams kkk > # > 要读取Stream中的所有消息。
kkk
1696606185991-1
age
14
1696606749261-0
name
zhangsan
```





#### 应用场景

##### 消息队列

基于 Stream 实现的消息队列就说到这里了，小结一下：

- 消息保序：XADD/XREAD
- 阻塞读取：XREAD block
- 重复消息处理：Stream 在使用 XADD 命令，会自动生成全局唯一 ID；
- 消息可靠性：内部使用 PENDING List 自动保存消息，使用 XPENDING 命令查看消费组已经读取但是未被确认的消息，消费者使用 XACK 确认消息；
- 支持消费组形式消费数据



保证消费者在发生故障或宕机再次重启后，仍然可以读取未处理完的消息：
Streams 会自动使用内部队列（也称为 PENDING List）留存消费组里每个消费者读取的消息，直到消费者使用 XACK 命令通知 Streams“消息已经处理完成”。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409020913781.png" alt="_E6_B6_88_E6_81_AF_E7_A1_AE_E8_AE_A4" style="zoom: 50%;" />

如果消费者没有成功处理消息，它就不会给 Streams 发送 XACK 命令，消息仍然会留存。此时，**消费者可以在重启后，用 XPENDING 命令查看已读取、但尚未确认处理完成的消息**。











### HyperLogLog

####  介绍

Redis HyperLogLog 是 Redis 2.8.9 版本新增的数据类型，是一种用于「统计基数」的数据集合类型，基数统计就是指统计一个集合中不重复的元素个数。但要注意，HyperLogLog 是统计规则是基于概率完成的，不是非常准确，标准误算率是 0.81%。

所以，简单来说 HyperLogLog **提供不精确的去重计数**。

HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的内存空间总是固定的、并且是很小的。

在 Redis 里面，**每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 `2^64` 个不同元素的基数**，和元素越多就越耗费内存的 Set 和 Hash 类型相比，HyperLogLog 就非常节省空间。

这什么概念？举个例子给大家对比一下。

用 Java 语言来说，一般 long 类型占用 8 字节，而 1 字节有 8 位，即：1 byte = 8 bit，即 long 数据类型最大可以表示的数是：`2^63-1`。对应上面的`2^64`个数，假设此时有`2^63-1`这么多个数，从 `0 ~ 2^63-1`，按照`long`以及`1k = 1024 字节`的规则来计算内存总数，就是：`((2^63-1) * 8/1024)K`，这是很庞大的一个数，存储空间远远超过`12K`，而 `HyperLogLog` 却可以用 `12K` 就能统计完。

#### 内部实现

HyperLogLog 的实现涉及到很多数学问题：[HyperLogLog (opens new window)](https://en.wikipedia.org/wiki/HyperLogLog)。



#### shell

用来做基数统计的算法（集合中每个元素只统计一次） **相当于stl 的set**

占用内存小，但有误差



以PF开头

```Bash
127.0.0.1:6379> pfadd course  git docker redis #创建一个集合
1
127.0.0.1:6379> pfcount course
3
127.0.0.1:6379> pfadd course1 go
1
127.0.0.1:6379> pfmerge result course course1 #合并两个集合
OK
127.0.0.1:6379> pfcount result
4

127.0.0.1:6379> pfadd course redis #添加失败
0
```



HyperLogLog 命令很少，就三个。

```shell
# 添加指定元素到 HyperLogLog 中
PFADD key element [element ...]

# 返回给定 HyperLogLog 的基数估算值。
PFCOUNT key [key ...]

# 将多个 HyperLogLog 合并为一个 HyperLogLog
PFMERGE destkey sourcekey [sourcekey ...]
```

#### 应用场景

##### 百万级网页 UV 计数

Redis HyperLogLog 优势在于只需要花费 12 KB 内存，就可以计算接近 2^64 个元素的基数，和元素越多就越耗费内存的 Set 和 Hash 类型相比，HyperLogLog 就非常节省空间。

所以，非常适合统计百万级以上的网页 UV 的场景。

在统计 UV 时，你可以用 PFADD 命令（用于向 HyperLogLog 中添加新元素）把访问页面的每个用户都添加到 HyperLogLog 中。

```shell
PFADD page1:uv user1 user2 user3 user4 user5
```

接下来，就可以用 PFCOUNT 命令直接获得 page1 的 UV 值了，这个命令的作用就是返回 HyperLogLog 的统计结果。

```shell
PFCOUNT page1:uv
```

不过，有一点需要你注意一下，HyperLogLog 的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是 0.81%。

这也就意味着，你使用 HyperLogLog 统计的 UV 是 100 万，但实际的 UV 可能是 101 万。虽然误差率不算大，但是，如果你需要精确统计结果的话，最好还是继续用 Set 或 Hash 类型。





### 地理位置Geospatial

#### 介绍

Redis GEO 是 Redis 3.2 版本新增的数据类型，主要用于存储地理位置信息，并对存储的信息进行操作。

在日常生活中，我们越来越依赖搜索“附近的餐馆”、在打车软件上叫车，这些都离不开基于位置信息服务（Location-Based Service，LBS）的应用。LBS 应用访问的数据是和人或物关联的一组经纬度信息，而且要能查询相邻的经纬度范围，GEO 就非常适合应用在 LBS 服务的场景中

#### 底层

GEO 本身并没有设计新的底层数据结构，而是直接使用了 Sorted Set 集合类型。

GEO 类型使用 GeoHash 编码方法实现了经纬度到 Sorted Set 中元素权重分数的转换，这其中的两个关键机制就是「对二维地图做区间划分」和「对区间进行编码」。一组经纬度落在某个区间后，就用区间的编码值来表示，并把编码值作为 Sorted Set 元素的权重分数。

这样一来，我们就可以把经纬度保存到 Sorted Set 中，利用 Sorted Set 提供的“按权重进行有序范围查找”的特性，实现 LBS 服务中频繁使用的“搜索附近”的需求

#### shell

都以geo开头

```Bash
127.0.0.1:6379> geoadd city 116.105 39.904 北京 #添加城市  经纬度 城市名
1
127.0.0.1:6379> geoadd city 121.472 31.231 上海
1

127.0.0.1:6379> geopos city 北京 #查看城市
116.1049988865852356
39.90399988166036138
127.0.0.1:6379> geodist city 北京 上海  #计算距离
1079428.6014  
127.0.0.1:6379> GEODIST city 北京 上海 KM #距离改成km
1079.4286
127.0.0.1:6379> geosearch city frommember 上海 byradius  3000 KM  #以上海为圆心 3000km为半径 查找城市
上海
北京

# 存储指定的地理空间位置，可以将一个或多个经度(longitude)、纬度(latitude)、位置名称(member)添加到指定的 key 中。
GEOADD key longitude latitude member [longitude latitude member ...]

# 从给定的 key 里返回所有指定名称(member)的位置（经度和纬度），不存在的返回 nil。
GEOPOS key member [member ...]

# 返回两个给定位置之间的距离。
GEODIST key member1 member2 [m|km|ft|mi]

# 根据用户给定的经纬度坐标来获取指定范围内的地理位置集合。
GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key]
```

#### 应用场景

##### 滴滴叫车

这里以滴滴叫车的场景为例，介绍下具体如何使用 GEO 命令：GEOADD 和 GEORADIUS 这两个命令。

假设车辆 ID 是 33，经纬度位置是（116.034579，39.030452），我们可以用一个 GEO 集合保存所有车辆的经纬度，集合 key 是 cars:locations。

执行下面的这个命令，就可以把 ID 号为 33 的车辆的当前经纬度位置存入 GEO 集合中：

```shell
GEOADD cars:locations 116.034579 39.030452 33
```

当用户想要寻找自己附近的网约车时，LBS 应用就可以使用 GEORADIUS 命令。

例如，LBS 应用执行下面的命令时，Redis 会根据输入的用户的经纬度信息（116.054579，39.030452 ），查找以这个经纬度为中心的 5 公里内的车辆信息，并返回给 LBS 应用。

```shell
GEORADIUS cars:locations 116.054579 39.030452 5 km ASC COUNT 10
```





### 位域

#### 介绍

#### 底层

#### shell

Redis的位域（Bitfield）是一个命令，用于处理Redis字符串值的位级别操作。位域允许你在Redis中执行各种位操作，如设置位、获取位、翻转位等，而不需要处理整个字符串。

以下是Redis位域的主要特点和用途：

1. **位级别操作**：Redis位域允许你在位级别上执行操作，而不是对整个字符串进行操作。这可以在处理大量二进制数据时提供更高的灵活性。
2. **原子操作**：Redis位域的操作是原子的，这意味着多个客户端可以同时并发访问和更新位域，而不用担心数据一致性问题。
3. **位数和偏移量**：位域操作通常需要指定从哪个位开始，以及要操作多少位。这可以帮助你精确定位操作的范围。
4. **位数值设置**：Redis位域允许你设置位的值，例如，将一个或多个位设置为1或0。这对于存储标志、开关或状态信息非常有用。
5. **位数值获取**：你可以获取位的值，以确定特定位是否为1或0。
6. **位域运算**：Redis提供了位域之间的位运算，如与、或、异或等操作，这对于处理多个位域非常有用。

以下是一些常见的Redis位域命令：

- `BITFIELD`：执行位域操作的通用命令，可以用于设置、获取、翻转位等操作。
- `SETBIT key offset value`：设置位域中指定偏移量的位的值。
- `GETBIT key offset`：获取位域中指定偏移量的位的值。
- `BITOP operation destkey key [key ...]`：对多个位域执行位操作（与、或、异或等）并将结果存储在目标位域中。
- `BITCOUNT key [start end]`：统计位域中值为1的位的数量。

Redis位域常用于处理二进制标志、位掩码、权限管理、布隆过滤器等场景。它是一种强大的工具，用于处理位级别的操作，特别适合在Redis中存储和处理大规模的二进制数据。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202407111546582.png" alt="202407090217360" style="zoom:50%;" />



### Redis 实现延迟队列

延迟队列是指把当前要做的事情，往后推迟一段时间再做。延迟队列的常见使用场景有以下几种：

- 在淘宝、京东等购物平台上下单，超过一定时间未付款，订单会自动取消；
- 打车的时候，在规定时间没有车主接单，平台会取消你的单并提醒你暂时没有车主接单；
- 点外卖的时候，如果商家在10分钟还没接单，就会自动取消订单；

在 Redis 可以使用有序集合（ZSet）的方式来实现延迟消息队列的，ZSet 有一个 Score 属性可以用来存储延迟执行的时间。

使用 zadd score1 value1 命令就可以一直往内存中生产消息。再利用 zrangebysocre 查询符合条件的所有待处理的任务， 通过循环执行队列任务即可。

![_E5_BB_B6_E8_BF_9F_E9_98_9F_E5_88_97](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408191526148.png)

### 发布订阅模式

缺点：信息无法持久化，无法记录历史消息

```Bash
subscribe lmy #订阅lmy
subscribe
lmy
1

message #收到信息
lmy
123
#发布lmy   新开终端
publish lmy 123
1
```

> 补充：Redis 发布/订阅机制为什么不可以作为消息队列？

发布订阅机制存在以下缺点，都是跟丢失数据有关：

1. 发布/订阅机制没有基于任何数据类型实现，所以不具备「数据持久化」的能力，也就是发布/订阅机制的相关操作，不会写入到 RDB 和 AOF 中，当 Redis 宕机重启，发布/订阅机制的数据也会全部丢失。
2. 发布订阅模式是“发后既忘”的工作模式，如果有订阅者离线重连之后不能消费之前的历史消息。
3. 当消费端有一定的消息积压时，也就是生产者发送的消息，消费者消费不过来时，如果超过 32M 或者是 60s 内持续保持在 8M 以上，消费端会被强行断开，这个参数是在配置文件中设置的，默认值是 `client-output-buffer-limit pubsub 32mb 8mb 60`。

所以，发布/订阅机制只适合即时通讯的场景，比如[构建哨兵集群 (opens new window)](https://xiaolincoding.com/redis/cluster/sentinel.html#哨兵集群是如何组成的)的场景采用了发布/订阅机制



## 数据结构

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181132079.png" alt="9fa26a74965efbf0f56b707a03bb9b7f" style="zoom:50%;" />

### redis

Redis 是使用了一个「哈希表」保存所有键值对，哈希表的最大好处就是让我们可以用 O(1) 的时间复杂度来快速查找到键值对。哈希表其实就是一个数组，数组中的元素叫做哈希桶。

Redis 的哈希桶是怎么保存键值对数据的呢？

哈希桶存放的是指向键值对数据的指针（dictEntry*），这样通过指针就能找到键值对数据，然后因为键值对的值可以保存字符串对象和集合数据类型的对象，所以键值对的数据结构中并不是直接保存值本身，而是保存了 void * key 和 void * value 指针，分别指向了实际的键对象和值对象，这样一来，即使值是集合数据，也可以通过 void * value 指针找到。



<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409020931735.png" alt="3c386666e4e7638a07b230ba14b400fe" style="zoom:50%;" />

- redisDb 结构，表示 Redis 数据库的结构，结构体里存放了指向了 dict 结构的指针；
- dict 结构，结构体里存放了 2 个哈希表，正常情况下都是用「哈希表1」，「哈希表2」只有在 rehash 的时候才用，具体什么是 rehash，我在本文的哈希表数据结构会讲；
- ditctht 结构，表示哈希表的结构，结构里存放了哈希表数组，数组中的每个元素都是指向一个哈希表节点结构（dictEntry）的指针；
- dictEntry 结构，表示哈希表节点的结构，结构里存放了 **void * key 和 void * value 指针， *key 指向的是 String 对象，而 \*value 则可以指向 String 对象，也可以指向集合类型的对象，比如 List 对象、Hash 对象、Set 对象和 Zset 对象**。

void * key 和 void * value 指针指向的是 **Redis 对象**，Redis 中的每个对象都由 redisObject 结构表示，如下图：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409020931425.png" alt="58d3987af2af868dca965193fb27c464" style="zoom: 50%;" />

- type，标识该对象是什么类型的对象（String 对象、 List 对象、Hash 对象、Set 对象和 Zset 对象）；
- encoding，标识该对象使用了哪种底层的数据结构；
- **ptr，指向底层数据结构的指针**。



### SDS

Redis 是用 C 语言实现的，但是它没有直接使用 C 语言的 char* 字符数组来实现字符串，而是自己封装了一个名为简单动态字符串（simple dynamic string，SDS） 的数据结构来表示字符串

C 语言的字符串不足之处以及可以改进的地方：

- 获取字符串长度的时间复杂度为 O（N）；
- 字符串的结尾是以 “\0” 字符标识，字符串里面不能包含有 “\0” 字符，因此不能保存二进制数据；
- 字符串操作函数不高效且不安全，比如有缓冲区溢出的风险，有可能会造成程序运行终止；



![516738c4058cdf9109e40a7812ef4239](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409020937293.png)

- **len，记录了字符串长度**。这样获取字符串长度的时候，只需要返回这个成员变量值就行，时间复杂度只需要 O（1）。

- **alloc，分配给字符数组的空间长度**。这样在修改字符串的时候，可以通过 `alloc - len` 计算出剩余的空间大小，可以用来判断空间是否满足修改需求，如果不满足的话，就会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出的问题。

  - 在扩容 SDS 空间之前，SDS API 会优先检查未使用空间是否足够，如果不够的话，API 不仅会为 SDS 分配修改所必须要的空间，还会给 SDS 分配额外的「未使用空间」。

- **flags，用来表示不同类型的 SDS**。一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64，后面在说明区别之处。

  - **之所以 SDS 设计不同类型的结构体，是为了能灵活保存不同大小的字符串，从而有效节省内存空间**。比如，在保存小字符串时，结构头占用空间也比较少。

    **使用了专门的编译优化来节省内存空间**，即在 struct 声明了 `__attribute__ ((packed))` ，它的作用是：告诉编译器取消结构体在编译过程中的优化对齐，按照实际占用字节数进行对齐。

- **buf[]，字符数组，用来保存实际数据**。不仅可以保存字符串，也可以保存任意格式的**二进制数据**。

总的来说，Redis 的 SDS 结构在原本字符数组之上，增加了三个元数据：len、alloc、flags，用来解决 C 语言字符串的缺陷



### 链表

#### 链表的优势与缺陷

Redis 的链表实现优点如下：

- listNode 链表节点的结构里带有 prev 和 next 指针，**获取某个节点的前置节点或后置节点的时间复杂度只需O(1)，而且这两个指针都可以指向 NULL，所以链表是无环链表**；
- list 结构因为提供了表头指针 head 和表尾节点 tail，所以**获取链表的表头节点和表尾节点的时间复杂度只需O(1)**；
- list 结构因为提供了链表节点数量 len，所以**获取链表中的节点数量的时间复杂度只需O(1)**；
- listNode 链表节使用 void* 指针保存节点值，并且可以通过 list 结构的 dup、free、match 函数指针为节点设置该节点类型特定的函数，因此**链表节点可以保存各种不同类型的值**；

链表的缺陷也是有的：

- 链表每个节点之间的内存都是不连续的，意味着**无法很好利用 CPU 缓存**。能很好利用 CPU 缓存的数据结构就是数组，因为数组的内存是连续的，这样就可以充分利用 CPU 缓存来加速访问。
- 还有一点，保存一个链表节点的值都需要一个链表节点结构头的分配，**内存开销较大**。



#### 链表节点结构设计

先来看看「链表节点」结构的样子：

```c
typedef struct listNode {
    //前置节点
    struct listNode *prev;
    //后置节点
    struct listNode *next;
    //节点的值
    void *value;
} listNode;
```

有前置节点和后置节点，可以看的出，这个是一个双向链表。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409020946905.png" alt="4fecbf7f63c73ec284a4821e0bfe2843" style="zoom:50%;" />

#### 链表结构设计

 listNode 结构体基础上又封装了 list 这个数据结构，这样操作起来会更方便，链表结构如下：

```c
typedef struct list {
    //链表头节点
    listNode *head;
    //链表尾节点
    listNode *tail;
    //节点值复制函数
    void *(*dup)(void *ptr);
    //节点值释放函数
    void (*free)(void *ptr);
    //节点值比较函数
    int (*match)(void *ptr, void *key);
    //链表节点数量
    unsigned long len;
} list;
```

list 结构为链表提供了链表头指针 head、链表尾节点 tail、链表节点数量 len、以及可以自定义实现的 dup、free、match 函数。

举个例子，下面是由 list 结构和 3 个 listNode 结构组成的链表。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409020946787.png" alt="cadf797496816eb343a19c2451437f1e" style="zoom:50%;" />



### 压缩列表

被设计成一种内存紧凑型的数据结构，占用一块连续的内存空间，不仅可以利用 CPU 缓存，而且会针对不同长度的数据，进行相应编码，这种方法可以有效地节省内存开销。

但是，压缩列表的缺陷也是有的：

- 不能保存过多的元素，否则查询效率就会降低；
- 新增或修改某个元素时，压缩列表占用的内存空间需要重新分配，甚至可能引发连锁更新的问题。

因此，Redis 对象（List 对象、Hash 对象、Zset 对象）包含的元素数量较少，或者元素值不大的情况才会使用压缩列表作为底层数据结构。



#### 压缩列表结构设计

压缩列表是 Redis 为了节约内存而开发的，它是**由连续内存块组成的顺序型数据结构**，有点类似于数组。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021014484.png" alt="a3b1f6235cf0587115b21312fe60289c" style="zoom:50%;" />

压缩列表在表头有三个字段：

- ***zlbytes***，记录整个压缩列表占用对**内存字节数；**
- ***zltail***，记录压缩列表「尾部」节点距离起始地址由多少字节，也就是**列表尾的偏移量**；
- ***zllen***，记录压缩列表包含的**节点数量**；
- ***zlend***，标记压缩列表的**结束点**，固定值 0xFF（十进制255）。

在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段（zllen）的长度直接定位，复杂度是 O(1)。而**查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了，因此压缩列表不适合保存过多的元素**。



压缩列表节点包含三部分内容：

- ***prevlen***，记录了「前一个节点」的长度，目的是为了实现从后向前遍历；
- ***encoding***，记录了当前节点实际数据的「类型和长度」，类型主要有两种：字符串和整数。
- ***data***，记录了当前节点的实际数据，类型和长度都由 `encoding` 决定；

压缩列表里的每个节点中的 prevlen 属性都记录了「前一个节点的长度」，而且 prevlen 属性的空间大小跟前一个节点长度值有关，比如：

- 如果**前一个节点的长度小于 254 字节**，那么 prevlen 属性需要用 **1 字节的空间**来保存这个长度值；
- 如果**前一个节点的长度大于等于 254 字节**，那么 prevlen 属性需要用 **5 字节的空间**来保存这个长度值；



encoding 属性的空间大小跟数据是字符串还是整数，以及字符串的长度有关

- 如果**当前节点的数据是整数**，则 encoding 会使用 **1 字节的空间**进行编码，也就是 encoding 长度为 1 字节。通过 encoding 确认了整数类型，就可以确认整数数据的实际大小了，比如如果 encoding 编码确认了数据是 int16 整数，那么 data 的长度就是 int16 的大小。
- 如果**当前节点的数据是字符串，根据字符串的长度大小**，encoding 会使用 **1 字节/2字节/5字节的空间**进行编码，encoding 编码的前两个 bit 表示数据的类型，后续的其他 bit 标识字符串数据的实际长度，即 data 的长度

#### 连锁更新

压缩列表除了查找复杂度高的问题，还有一个问题。

**压缩列表新增某个元素或修改某个元素时，如果空间不不够，压缩列表占用的内存空间就需要重新分配。而当新插入的元素较大时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起「连锁更新」问题，导致每个元素的空间都要重新分配，造成访问压缩列表性能的下降**。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021016169.png" alt="d1a6deff4672580609c99a5b06bf3429" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021016950.png" alt="1f0e5ae7ab749078cadda5ba0ed98eac" style="zoom:50%;" />



### 哈希表

哈希表是一种保存键值对（key-value）的数据结构。

哈希表中的每一个 key 都是独一无二的

哈希表优点在于，它**能以 O(1) 的复杂度快速查询数据**。

但是存在的风险也是有，在哈希表大小固定的情况下，随着数据不断增多，那么**哈希冲突**的可能性也会越高。

**Redis 采用了「链式哈希」来解决哈希冲突**，在不扩容哈希表的前提下，将具有相同哈希值的数据串起来，形成链接起，以便这些数据在表中仍然可以被查询到。

#### 哈希表结构设计

Redis 的哈希表结构如下：

```c
typedef struct dictht {
    //哈希表数组
    dictEntry **table;
    //哈希表大小
    unsigned long size;  
    //哈希表大小掩码，用于计算索引值
    unsigned long sizemask;
    //该哈希表已有的节点数量
    unsigned long used;
} dictht;
```

可以看到，哈希表是一个数组（dictEntry **table），数组的每个元素是一个指向「哈希表节点（dictEntry）」的指针。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021036132.png" alt="dc495ffeaa3c3d8cb2e12129b3423118" style="zoom:50%;" />



哈希表节点的结构如下：

```c
typedef struct dictEntry {
    //键值对中的键
    void *key;
  
    //键值对中的值
    union {
        void *val;
        uint64_t u64;
        int64_t s64;
        double d;
    } v;
    //指向下一个哈希表节点，形成链表
    struct dictEntry *next;
} dictEntry;
```

dictEntry 结构里不仅包含指向键和值的指针，还包含了指向下一个哈希表节点的指针，这个指针可以将多个哈希值相同的键值对链接起来，以此来解决哈希冲突的问题，这就是链式哈希。

另外dictEntry 结构里键值对中的值是一个「联合体 v」定义的，因此，键值对中的值可以是一个指向实际值的指针，或者是一个无符号的 64 位整数或有符号的 64 位整数或double 类的值。
这么做的好处是可以节省内存空间，因为当「值」是整数或浮点数时，就可以将值的数据内嵌在 dictEntry 结构里，无需再用一个指针指向实际的值，从而节省了内存空间

#### rehash

哈希表结构设计的这一小节，我给大家介绍了 Redis 使用 dictht 结构体表示哈希表。不过，在实际使用哈希表时，Redis 定义一个 dict 结构体，这个结构体里定义了**两个哈希表（ht[2]）**。

```c
typedef struct dict {
    …
    //两个Hash表，交替使用，用于rehash操作
    dictht ht[2]; 
    …
} dict;
```

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021045512.png" alt="2fedbc9cd4cb7236c302d695686dd478" style="zoom:50%;" />

随着数据逐步增多，触发了 rehash 操作，这个过程分为三步：

- 给「哈希表 2」 分配空间，一般会比「哈希表 1」 大一倍（两倍的意思）；
- 将「哈希表 1 」的数据迁移到「哈希表 2」 中；
- 迁移完成后，「哈希表 1 」的空间会被释放，并把「哈希表 2」 设置为「哈希表 1」，然后在「哈希表 2」 新创建一个空白的哈希表，为下次 rehash 做准备。

**如果「哈希表 1 」的数据量非常大，那么在迁移至「哈希表 2 」的时候，因为会涉及大量的数据拷贝，此时可能会对 Redis 造成阻塞，无法服务其他请求**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021046733.png" alt="cabce0ce7e320bc9d9b5bde947b6811b" style="zoom:50%;" />

#### 渐进式 rehash

为了避免 rehash 在数据迁移过程中，因拷贝数据的耗时，影响 Redis 性能的情况，所以 Redis 采用了**渐进式 rehash**，也就是将数据的迁移的工作不再是一次性迁移完成，而是分多次迁移。

渐进式 rehash 步骤如下：

- 给「哈希表 2」 分配空间；
- **在 rehash 进行期间，每次哈希表元素进行新增、删除、查找或者更新操作时，Redis 除了会执行对应的操作之外，还会顺序将「哈希表 1 」中索引位置上的所有 key-value 迁移到「哈希表 2」 上**；
- 随着处理客户端发起的哈希表操作请求数量越多，最终在某个时间点会把「哈希表 1 」的所有 key-value 迁移到「哈希表 2」，从而完成 rehash 操作。

这样就巧妙地把一次性大量数据迁移工作的开销，分摊到了多次处理请求的过程中，避免了一次性 rehash 的耗时操作。

在进行渐进式 rehash 的过程中，会有两个哈希表，所以在渐进式 rehash 进行期间，哈希表元素的删除、查找、更新等操作都会在这两个哈希表进行。

比如，查找一个 key 的值的话，先会在「哈希表 1」 里面进行查找，如果没找到，就会继续到哈希表 2 里面进行找到。

另外，在渐进式 rehash 进行期间，新增一个 key-value 时，会被保存到「哈希表 2 」里面，而「哈希表 1」 则不再进行任何添加操作，这样保证了「哈希表 1 」的 key-value 数量只会减少，随着 rehash 操作的完成，最终「哈希表 1 」就会变成空表。

#### rehash 触发条件

rehash 的触发条件跟**负载因子（load factor）**有关系。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021046349.png" alt="85f597f7851b90d6c78bb0d8e39690fc" style="zoom: 67%;" />

触发 rehash 操作的条件，主要有两个：

- **当负载因子大于等于 1 ，并且 Redis 没有在执行 bgsave 命令或者 bgrewiteaof 命令，也就是没有执行 RDB 快照或没有进行 AOF 重写的时候，就会进行 rehash 操作。**
- **当负载因子大于等于 5 时，此时说明哈希冲突非常严重了，不管有没有有在执行 RDB 快照或 AOF 重写，都会强制进行 rehash 操作**



### 整数集合

整数集合是 Set 对象的底层实现之一。当一个 Set 对象只包含整数值元素，并且元素数量不大时，就会使用整数集这个数据结构作为底层实现。

#### 整数集合结构设计

整数集合本质上是一块连续内存空间，它的结构定义如下：

```c
typedef struct intset {
    //编码方式
    uint32_t encoding;
    //集合包含的元素数量
    uint32_t length;
    //保存元素的数组
    int8_t contents[];
} intset;
```

保存元素的容器是一个 contents 数组，虽然 contents 被声明为 int8_t 类型的数组，但是实际上 contents 数组并不保存任何 int8_t 类型的元素，contents 数组的真正类型取决于 intset 结构体里的 encoding 属性的值。比如：

- 如果 encoding 属性值为 INTSET_ENC_INT16，那么 contents 就是一个 int16_t 类型的数组，数组中每一个元素的类型都是 int16_t；
- 如果 encoding 属性值为 INTSET_ENC_INT32，那么 contents 就是一个 int32_t 类型的数组，数组中每一个元素的类型都是 int32_t；
- 如果 encoding 属性值为 INTSET_ENC_INT64，那么 contents 就是一个 int64_t 类型的数组，数组中每一个元素的类型都是 int64_t；

不同类型的 contents 数组，意味着数组的大小也会不同。

#### 整数集合的升级操作

整数集合升级的好处是**节省内存资源**。

**不支持降级操作**，一旦对数组进行了升级，就会一直保持升级后的状态。

整数集合会有一个升级规则，就是当我们将一个新元素加入到整数集合里面，如果新元素的类型（int32_t）比整数集合现有所有元素的类型（int16_t）都要长时，整数集合需要先进行升级，也就是按新元素的类型（int32_t）扩展 contents 数组的空间大小，然后才能将新元素加入到整数集合里，当然升级的过程中，也要维持整数集合的有序性。

整数集合升级的过程不会重新分配一个新类型的数组，而是在原本的数组上扩展空间，然后在将每个元素按间隔类型大小分割，如果 encoding 属性值为 INTSET_ENC_INT16，则每个元素的间隔就是 16 位。

举个例子，假设有一个整数集合里有 3 个类型为 int16_t 的元素。

![5dbdfa7cfbdd1d12a4d9458c6c90d472](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021059224.png)

现在，往这个整数集合中加入一个新元素 65535，这个新元素需要用 int32_t 类型来保存，所以整数集合要进行升级操作，首先需要为 contents 数组扩容，**在原本空间的大小之上再扩容多 80 位（4x32-3x16=80），这样就能保存下 4 个类型为 int32_t 的元素**。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021059270.png" alt="e2e3e19fc934e70563fbdfde2af39a2b" style="zoom:50%;" />

扩容完 contents 数组空间大小后，需要将之前的三个元素转换为 int32_t 类型，并将转换后的元素放置到正确的位上面，并且需要维持底层数组的有序性不变，整个转换过程如下：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021100661.png" alt="e84b052381e240eeb8cc97d6b729968b" style="zoom:50%;" />



### 跳表

Redis 只有 Zset 对象的底层实现用到了跳表，跳表的优势是能支持平均 O(logN) 复杂度的节点查找。

zset 结构体里有两个数据结构：一个是跳表，一个是哈希表。这样的好处是既能进行高效的范围查询，也能进行高效单点查询。

```c
typedef struct zset {
    dict *dict;
    zskiplist *zsl;
} zset;
```

Zset 对象在执行数据插入或是数据更新的过程中，会依次在跳表和哈希表中插入或更新相应的数据，从而保证了跳表和哈希表中记录的信息一致。

Zset 对象能支持范围查询（如 ZRANGEBYSCORE 操作），这是因为它的数据结构设计采用了跳表，而又能以常数复杂度获取元素权重（如 ZSCORE 操作），这是因为它同时采用了哈希表进行索引。

可能很多人会奇怪，为什么我开头说 Zset 对象的底层数据结构是「压缩列表」或者「跳表」，而没有说哈希表呢？

Zset 对象在使用跳表作为数据结构的时候，是使用由「哈希表+跳表」组成的 struct zset，但是我们讨论的时候，都会说跳表是 Zset 对象的底层数据结构，而不会提及哈希表，是因为 struct zset 中的哈希表只是用于以常数复杂度获取元素权重，大部分操作都是跳表实现的。



#### 跳表结构设计

链表在查找元素的时候，因为需要逐一查找，所以查询效率非常低，时间复杂度是O(N)，于是就出现了跳表。**跳表是在链表基础上改进过来的，实现了一种「多层」的有序链表**，这样的好处是能快读定位数据。

下图展示了一个层级为 3 的跳表。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021129080.png" alt="2ae0ed790c7e7403f215acb2bd82e884" style="zoom:50%;" />

图中头节点有 L0~L2 三个头指针，分别指向了不同层级的节点，然后每个层级的节点都通过指针连接起来：

- L0 层级共有 5 个节点，分别是节点1、2、3、4、5；
- L1 层级共有 3 个节点，分别是节点 2、3、5；
- L2 层级只有 1 个节点，也就是节点 3 。

如果我们要在链表中查找节点 4 这个元素，只能从头开始遍历链表，需要查找 4 次，而使用了跳表后，只需要查找 2 次就能定位到节点 4，因为可以在头节点直接从 L2 层级跳到节点 3，然后再往前遍历找到节点 4。

可以看到，这个查找过程就是在多个层级上跳来跳去，最后定位到元素。当数据量很大时，跳表的查找复杂度就是 O(logN)。

那跳表节点是怎么实现多层级的呢？这就需要看「跳表节点」的数据结构了，如下：

```c
typedef struct zskiplistNode {
    //Zset 对象的元素值
    sds ele;
    //元素权重值
    double score;
    //后向指针
    struct zskiplistNode *backward;
  
    //节点的level数组，保存每层上的前向指针和跨度
    struct zskiplistLevel {
        struct zskiplistNode *forward;
        unsigned long span;
    } level[];
} zskiplistNode;
```

Zset 对象要同时保存**「元素」和「元素的权重」**，对应到跳表节点结构里就是 sds 类型的 ele 变量和 double 类型的 score 变量。每个跳表节点都有一个后向指针（struct zskiplistNode *backward），指向前一个节点，目的是为了方便从跳表的尾节点开始访问节点，这样倒序查找时很方便。

跳表是一个带有层级关系的链表，而且每一层级可以包含多个节点，每一个节点通过指针连接起来，实现这一特性就是靠跳表节点结构体中的**zskiplistLevel 结构体类型的 level 数组**。

level 数组中的每一个元素代表跳表的一层，也就是由 zskiplistLevel 结构体表示，比如 leve[0] 就表示第一层，leve[1] 就表示第二层。zskiplistLevel 结构体里定义了「指向下一个跳表节点的指针」和「跨度」，跨度时用来记录两个节点之间的距离。

比如，下面这张图，展示了各个节点的跨度。

![3_E5_B1_82_E8_B7_B3_E8_A1_A8-_E8_B7_A8_E5_BA_A6](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021129558.png)

第一眼看到跨度的时候，以为是遍历操作有关，实际上并没有任何关系，遍历操作只需要用前向指针（struct zskiplistNode *forward）就可以完成了。

**跨度实际上是为了计算这个节点在跳表中的排位**。具体怎么做的呢？因为跳表中的节点都是按序排列的，那么计算某个节点排位的时候，从头节点点到该结点的查询路径上，将沿途访问过的所有层的跨度累加起来，得到的结果就是目标节点在跳表中的排位。

举个例子，查找图中节点 3 在跳表中的排位，从头节点开始查找节点 3，查找的过程只经过了一个层（L2），并且层的跨度是 3，所以节点 3 在跳表中的排位是 3。

另外，图中的头节点其实也是 zskiplistNode 跳表节点，只不过头节点的后向指针、权重、元素值都没有用到，所以图中省略了这部分。

问题来了，由谁定义哪个跳表节点是头节点呢？这就介绍「跳表」结构体了，如下所示：

```c
typedef struct zskiplist {
    struct zskiplistNode *header, *tail;
    unsigned long length;
    int level;
} zskiplist;
```

跳表结构里包含了：

- 跳表的头尾节点，便于在O(1)时间复杂度内访问跳表的头节点和尾节点；
- 跳表的长度，便于在O(1)时间复杂度获取跳表节点的数量；
- 跳表的最大层数，便于在O(1)时间复杂度获取跳表中层高最大的那个节点的层数量；

#### 跳表节点查询过程

**查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层**。在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，共有两个判断条件：

- 如果当前节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。
- 如果当前节点的权重「等于」要查找的权重时，并且当前节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。

如果上面两个条件都不满足，或者下一个节点为空时，跳表就会使用目前遍历到的节点的 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。

举个例子，下图有个 3 层级的跳表。

![3_E5_B1_82_E8_B7_B3_E8_A1_A8-_E8_B7_A8_E5_BA_A6.drawio](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021130188.png)

如果要查找「元素：abcd，权重：4」的节点，查找的过程是这样的：

- 先从头节点的最高层开始，L2 指向了「元素：abc，权重：3」节点，这个节点的权重比要查找节点的小，所以要访问该层上的下一个节点；
- 但是该层的下一个节点是空节点（ leve[2]指向的是空节点），于是就会跳到「元素：abc，权重：3」节点的下一层去找，也就是 leve[1];
- 「元素：abc，权重：3」节点的 leve[1] 的下一个指针指向了「元素：abcde，权重：4」的节点，然后将其和要查找的节点比较。虽然「元素：abcde，权重：4」的节点的权重和要查找的权重相同，但是当前节点的 SDS 类型数据「大于」要查找的数据，所以会继续跳到「元素：abc，权重：3」节点的下一层去找，也就是 leve[0]；
- 「元素：abc，权重：3」节点的 leve[0] 的下一个指针指向了「元素：abcd，权重：4」的节点，该节点正是要查找的节点，查询结束。

#### 跳表节点层数设置

跳表的相邻两层的节点数量的比例会影响跳表的查询性能。

举个例子，下图的跳表，第二层的节点数量只有 1 个，而第一层的节点数量有 6 个。

![2802786ab4f52c1e248904e5cef33a74](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021130741.png)

这时，如果想要查询节点 6，那基本就跟链表的查询复杂度一样，就需要在第一层的节点中依次顺序查找，复杂度就是 O(N) 了。所以，为了降低查询复杂度，我们就需要维持相邻层结点数间的关系。

**跳表的相邻两层的节点数量最理想的比例是 2:1，查找复杂度可以降低到 O(logN)**。

下图的跳表就是，相邻两层的节点数量的比例是 2 : 1。

![cdc14698f629c74bf5a239cc8a611aeb](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021131240.png)

> 那怎样才能维持相邻两层的节点数量的比例为 2 : 1 呢？

如果采用新增节点或者删除节点时，来调整跳表节点以维持比例的方法的话，会带来额外的开销。

Redis 则采用一种巧妙的方法是，**跳表在创建节点的时候，随机生成每个节点的层数**，并没有严格维持相邻两层的节点数量比例为 2 : 1 的情况。

具体的做法是，**跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数**。

这样的做法，相当于每增加一层的概率不超过 25%，层数越高，概率越低，层高最大限制是 64。

虽然我前面讲解跳表的时候，图中的跳表的「头节点」都是 3 层高，但是其实**如果层高最大限制是 64，那么在创建跳表「头节点」的时候，就会直接创建 64 层高的头节点**。

如下代码，创建跳表时，头节点的 level 数组有 ZSKIPLIST_MAXLEVEL个元素（层），节点不存储任何 member 和 score 值，level 数组元素的 forward 都指向NULL， span值都为0。

```c
/* Create a new skiplist. */
zskiplist *zslCreate(void) {
    int j;
    zskiplist *zsl;

    zsl = zmalloc(sizeof(*zsl));
    zsl->level = 1;
    zsl->length = 0;
    zsl->header = zslCreateNode(ZSKIPLIST_MAXLEVEL,0,NULL);
    for (j = 0; j < ZSKIPLIST_MAXLEVEL; j++) {
        zsl->header->level[j].forward = NULL;
        zsl->header->level[j].span = 0;
    }
    zsl->header->backward = NULL;
    zsl->tail = NULL;
    return zsl;
}
```

其中，ZSKIPLIST_MAXLEVEL 定义的是最高的层数，Redis 7.0 定义为 32，Redis 5.0 定义为 64，Redis 3.0 定义为 32。



#### 为什么用跳表而不用平衡树？

这里插一个常见的面试题：为什么 Zset 的实现用跳表而不用平衡树（如 AVL树、红黑树等）？

- **从内存占用上来比较，跳表比平衡树更灵活一些**。平衡树每个节点包含 2 个指针（分别指向左右子树），而跳表每个节点包含的指针数目平均为 1/(1-p)，具体取决于参数 p 的大小。如果像 Redis里的实现一样，取 p=1/4，那么平均每个节点包含 1.33 个指针，比平衡树更有优势。
- **在做范围查找的时候，跳表比平衡树操作要简单**。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在跳表上进行范围查找就非常简单，只需要在找到小值之后，对第 1 层链表进行若干步的遍历就可以实现。
- **从算法实现难度上来比较，跳表比平衡树要简单得多**。平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而跳表的插入和删除只需要修改相邻节点的指针，操作简单又快速。





### quicklist

在 Redis 3.0 之前，List 对象的底层数据结构是双向链表或者压缩列表。然后在 Redis 3.2 的时候，List 对象的底层改由 quicklist 数据结构实现。

其实 quicklist 就是「双向链表 + 压缩列表」组合，因为一个 quicklist 就是一个链表，而链表中的每个元素又是一个压缩列表。

在前面讲压缩列表的时候，我也提到了压缩列表的不足，虽然压缩列表是通过紧凑型的内存布局节省了内存开销，但是因为它的结构设计，如果保存的元素数量增加，或者元素变大了，压缩列表会有「连锁更新」的风险，一旦发生，会造成性能下降。

quicklist 解决办法，**通过控制每个链表节点中的压缩列表的大小或者元素个数，来规避连锁更新的问题。因为压缩列表元素越少或越小，连锁更新带来的影响就越小，从而提供了更好的访问性能。**

#### quicklist 结构设计

quicklist 的结构体跟链表的结构体类似，都包含了表头和表尾，区别在于 quicklist 的节点是 quicklistNode。

```c
typedef struct quicklist {
    //quicklist的链表头
    quicklistNode *head;      //quicklist的链表头
    //quicklist的链表尾
    quicklistNode *tail; 
    //所有压缩列表中的总元素个数
    unsigned long count;
    //quicklistNodes的个数
    unsigned long len;       
    ...
} quicklist;
```

接下来看看，quicklistNode 的结构定义：

```c
typedef struct quicklistNode {
    //前一个quicklistNode
    struct quicklistNode *prev;     //前一个quicklistNode
    //下一个quicklistNode
    struct quicklistNode *next;     //后一个quicklistNode
    //quicklistNode指向的压缩列表
    unsigned char *zl;              
    //压缩列表的的字节大小
    unsigned int sz;                
    //压缩列表的元素个数
    unsigned int count : 16;        //ziplist中的元素个数 
    ....
} quicklistNode;
```

可以看到，quicklistNode 结构体里包含了前一个节点和下一个节点指针，这样每个 quicklistNode 形成了一个双向链表。但是链表节点的元素不再是单纯保存元素值，而是保存了一个压缩列表，所以 quicklistNode 结构体里有个指向压缩列表的指针 *zl。

我画了一张图，方便你理解 quicklist 数据结构。

![f46cbe347f65ded522f1cc3fd8dba549](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021131364.png)



在向 quicklist 添加一个元素的时候，不会像普通的链表那样，直接新建一个链表节点。而是会检查插入位置的压缩列表是否能容纳该元素，如果能容纳就直接保存到 quicklistNode 结构里的压缩列表，如果不能容纳，才会新建一个新的 quicklistNode 结构。

quicklist 会控制 quicklistNode 结构里的压缩列表的大小或者元素个数，来规避潜在的连锁更新的风险，但是这并没有完全解决连锁更新的问题。





### listpack

quicklist 虽然通过控制 quicklistNode 结构里的压缩列表的大小或者元素个数，来减少连锁更新带来的性能影响，但是并没有完全解决连锁更新的问题。

因为 quicklistNode 还是用了压缩列表来保存元素，压缩列表连锁更新的问题，来源于它的结构设计，所以要想彻底解决这个问题，需要设计一个新的数据结构。

于是，Redis 在 5.0 新设计一个数据结构叫 listpack，目的是替代压缩列表，它最大特点是 **listpack 中每个节点不再包含前一个节点的长度**了，压缩列表每个节点正因为需要保存前一个节点的长度字段，就会有连锁更新的隐患。



#### listpack 结构设计

listpack 采用了压缩列表的很多优秀的设计，比如还是用一块连续的内存空间来紧凑地保存数据，并且为了节省内存的开销，listpack 节点会采用不同的编码方式保存不同大小的数据。

我们先看看 listpack 结构：

![4d2dc376b5fd68dae70d9284ae82b73a](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021132830.png)

listpack 头包含两个属性，分别记录了 listpack 总字节数和元素数量，然后 listpack 末尾也有个结尾标识。图中的 listpack entry 就是 listpack 的节点了。

每个 listpack 节点结构如下：

![c5fb0a602d4caaca37ff0357f05b0abf](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021132758.png)

主要包含三个方面内容：

- encoding，定义该元素的编码类型，会对不同长度的整数和字符串进行编码；
- data，实际存放的数据；
- len，encoding+data的总长度；

可以看到，**listpack 没有压缩列表中记录前一个节点长度的字段了，listpack 只记录当前节点的长度，当我们向 listpack 加入一个新元素的时候，不会影响其他节点的长度字段的变化，从而避免了压缩列表的连锁更新问题**。

> 压缩列表的entry为什么要保存prevlen呢？listpack改成len之后不会影响功能吗？

压缩列表的 entry 保存 prevlen 是为了实现节点从后往前遍历，知道前一个节点的长度，就可以计算前一个节点的偏移量。

listpack 一样可以支持从后往前遍历的。详细的算法可以看：https://github.com/antirez/listpack/blob/master/listpack.c 里的lpDecodeBacklen函数，lpDecodeBacklen 函数就可以从当前列表项起始位置的指针开始，向左逐个字节解析，得到前一项的 entry-len 值。

```c
/* Decode the backlen and returns it. If the encoding looks invalid (more than
 * 5 bytes are used), UINT64_MAX is returned to report the problem. */
uint64_t lpDecodeBacklen(unsigned char *p) {
    uint64_t val = 0;
    uint64_t shift = 0;
    do {
        val |= (uint64_t)(p[0] & 127) << shift;
        if (!(p[0] & 128)) break;
        shift += 7;
        p--;
        if (shift > 28) return UINT64_MAX;
    } while(1);
    return val;
}
```



## 事务

Redis 事务可以一次执行多个命令， 并且带有以下三个重要的保证：

- 批量操作在发送 EXEC 命令前被放入队列缓存。
- 收到 EXEC 命令后进入事务执行，事务中任意命令执行失败，其余的命令依然被执行。
- 在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中。

一个事务从开始到执行会经历以下三个阶段：

- 开始事务。
- 命令入队。
- 执行事务。

单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 **Redis 事务的执行并不是原子性的，不支持事务回滚。**

事务可以理解为一个打包的批量执行脚本，但批量指令并非原子化的操作，中间某条指令的失败不会导致前面已做指令的回滚，也不会造成后续的指令不做。

### 命令

下表列出了 redis 事务的相关命令：

| 序号 | 命令及描述                                                   |
| ---- | ------------------------------------------------------------ |
| 1    | [DISCARD](https://www.runoob.com/redis/transactions-discard.html) 取消事务，放弃执行事务块内的所有命令。 |
| 2    | [EXEC](https://www.runoob.com/redis/transactions-exec.html) 执行所有事务块内的命令。 |
| 3    | [MULTI](https://www.runoob.com/redis/transactions-multi.html) 标记一个事务块的开始。 |
| 4    | [UNWATCH](https://www.runoob.com/redis/transactions-unwatch.html) 取消 WATCH 命令对所有 key 的监视。 |
| 5    | [WATCH key [key ...\]](https://www.runoob.com/redis/transactions-watch.html) 监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 |

### 实例

```Bash
以下是一个事务的例子， 它先以 **MULTI** 开始一个事务， 然后将多个命令入队到事务中， 最后由 **EXEC** 命令触发事务， 一并执行事务中的所有命令：
redis 127.0.0.1:6379> MULTI
OK
redis 127.0.0.1:6379> SET book-name "Mastering C++ in 21 days"
QUEUED
redis 127.0.0.1:6379> GET book-name
QUEUED
redis 127.0.0.1:6379> SADD tag "C++" "Programming" "Mastering Series"
QUEUED
redis 127.0.0.1:6379> SMEMBERS tag
QUEUED

redis 127.0.0.1:6379> EXEC
1) OK
2) "Mastering C++ in 21 days"
3) (integer) 3
4) 1) "Mastering Series"
   2) "C++"
   3) "Programming"
   
   
#执行不是原子性的
redis 127.0.0.1:7000> multi
OK
redis 127.0.0.1:7000> set a aaa
QUEUED
redis 127.0.0.1:7000> set b bbb
QUEUED
redis 127.0.0.1:7000> set c ccc
QUEUED
redis 127.0.0.1:7000> exec
1) OK
2) OK
3) OK
```



## 持久化

允许将数据保存到磁盘上以进行持久化。

**RDB**：适合做定期的全量备份，适合冷备份和对数据丢失不敏感的场景。

**AOF**：适合实时性要求高、不能容忍数据丢失的场景，但性能上比 RDB 要差。

**混合持久化**：结合了两者的优点，适合需要数据恢复速度快、数据丢失少的场景，尤其是生产环境。**对数据完整性要求高，同时又需要快速恢复**的场景。适合高并发、数据量较大的生产环境，既需要减少 I/O 开销，又需要在发生故障时保证尽量少丢失数据。



### **AOF 持久化**

**（Append-Only File）：**

AOF 持久化是将 Redis 命令追加到一个日志文件中的机制。这个日志文件包含了可以恢复整个数据集的操作，而不仅仅是数据快照。AOF 文件以文本形式保存，通常以 "**.aof**" 扩展名结尾。AOF 持久化适用于恢复最近的数据更为重要的场景。

- 配置 AOF 持久化：在 Redis 配置文件中设置 "appendonly" 指令，例如：

```Plaintext
appendonly yes
appendfilename "appendonly.aof"
```

#### AOF 持久化工作流程

1. 当 Redis 执行写操作时（例如 SET、DEL、INCR 等），它会将相应的命令追加到 AOF 日志文件中。

   - ### AOF 记录的内容：

     - **写操作**：如 `SET`、`DEL`、`INCR`、`LPUSH` 等命令。
     - **影响数据结构的操作**：如 `HSET`、`SADD`、`ZADD` 等。

     ### 不记录的内容：

     - **读操作**：如 `GET`、`LRANGE`、`HGET` 等。
     - **非数据变更操作**：如 `INFO`、`PING`、`TIME` 等命令。

2. Redis 将命令写入 AOF 缓冲区，然后异步将缓冲区的内容写入 AOF 文件。
3. 在 Redis 服务器重启时，它会将 AOF 文件中的命令按顺序重新执行，从而将数据还原到内存中。

#### AOF 持久化的优势

- 更精确的数据恢复：由于 AOF 文件记录了每个写操作，可以更精确地还原数据。
- 恢复时间短：Redis 重启时，只需要按顺序重新执行 AOF 文件中的命令，通常比加载 RDB 文件更快。
- 操作可读性：AOF 文件以文本形式记录命令，方便查看和调试。
- 自动压缩：Redis 支持 AOF 文件的自动压缩，以减小文件大小。



Reids 是**先执行写操作命令后，才将该命令记录到 AOF 日志**里的，这么做其实有两个好处。

- **避免额外的检查开销**：因为如果先将写操作命令记录到 AOF 日志里，再执行该命令的话，如果当前的命令语法有问题，那么如果不进行命令语法检查，该错误的命令记录到 AOF 日志里后，Redis 在使用日志恢复数据时，就可能会出错。
- **不会阻塞当前写操作命令的执行**：因为当写操作命令执行成功后，才会将命令记录到 AOF 日志。

当然，这样做也会带来风险：

- **数据可能会丢失：** 执行写操作命令和记录日志是两个过程，那当 Redis 在还没来得及将命令写入到硬盘时，服务器发生宕机了，这个数据就会有丢失的风险。
- **可能阻塞其他操作：** 由于写操作命令执行成功后才记录到 AOF 日志，所以不会阻塞当前命令的执行，但因为 AOF 日志也是在主线程中执行，所以当 Redis 把日志文件写入磁盘的时候，还是会阻塞后续的操作无法执行。



#### 写回策略

具体内核缓冲区的数据什么时候写入到硬盘，由内核决定

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408182019329.png" alt="4eeef4dd1bedd2ffe0b84d4eaa0dbdea-20230309232249413" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408182019354.jpeg" alt="img" style="zoom:50%;" />

三种策略只是在控制 `fsync()` 函数的调用时机。

- Always 策略就是每次写入 AOF 文件数据后，就执行 fsync() 函数；
- Everysec 策略就会创建一个**异步任务**来执行 fsync() 函数；
- No 策略就是永不执行 fsync() 函数;



####  **AOF 重写机制**

当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。

AOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到「新的 AOF 文件」，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件。



**重写 AOF 过程是由后台子进程 \*bgrewriteaof\* 来完成的**，这么做可以达到两个好处：

- 子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程；
- 子进程带有主进程的数据副本，这里使用子进程而不是线程，因为如果是使用线程，多线程之间会共享内存，那么在修改共享内存数据的时候，需要通过加锁来保证数据的安全，而这样就会降低性能。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读的方式，而当父子进程任意一方修改了该共享内存，就会发生「写时复制」，于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。

重写过程中，主进程依然可以正常处理命令，同时将写命令写入到 **「AOF 缓冲区」和 「AOF 重写缓冲区」**。

当子进程完成 AOF 重写工作（*扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志*）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的。

主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作：

- 将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致；

- 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。


![watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70-20230309231944807](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021557861.png)





### **RDB 持久化**

**（Snapshotting）：**

RDB 持久化是将 Redis 数据快照保存到磁盘的机制。这个快照包含了一个时间点的所有数据，可以定期生成或手动触发。RDB 文件是二进制文件，通常以 "**.rdb**" 扩展名结尾。RDB 持久化非常适合用于备份和灾难恢复。您可以配置 Redis 定期将数据保存为 RDB 快照。

Redis 的快照是**全量快照**，如果频率太频繁，可能会对 Redis 性能产生影响。

#### 配置 RDB 持久化

1.自动快照：在 Redis 配置文件中设置 "save" 指令

```Plaintext
vim  redis.config
save 900 1
save 300 10
save 60 10000
```

这将指示 Redis 在900秒内，如果至少有1个键发生变化，就执行一次持久化操作。

2.手动触发：执行 Redis 命令 `SAVE` 或 `BGSAVE` 来手动触发 RDB 文件的生成。

`SAVE` 命令会阻塞 Redis 服务器，直到快照生成完毕，
`BGSAVE` 命令会在后台生成快照，不会阻塞服务器。

​	fork进程，执行 bgsave 过程中，Redis 依然**可以继续处理操作命令**，**写时复制技术**

bgsave 快照过程中，如果主线程修改了共享数据，**发生了写时复制后，RDB 快照保存的是原本的内存数据**，而主线程刚修改的数据，是没办法在这一时间写入 RDB 文件的，只能交由下一次的 bgsave 快照。

#### **RDB 文件加载**

RDB 文件加载是 Redis 数据的恢复机制。当 Redis 服务器启动时，它会尝试加载最新的 RDB 文件，将数据还原到内存中。这是一种非常快速的启动方式，因为它只需要加载一个**二进制文件**，而不需要重新执行所有的写操作。





### **混合持久化方式**

您可以选择使用 RDB、AOF 或两者同时进行持久化，具体取决于您的需求：

- **只使用 RDB：** 如果您对数据的定期备份较为满意，并且不需要对每个写操作进行记录，可以仅使用 RDB 持久化。
- **只使用 AOF：** 如果数据完整性和精确的数据恢复是关键，可以只使用 AOF 持久化。
- **同时使用 RDB 和 AOF：** 这是推荐的方式。RDB 提供了定期备份和快速恢复，而 AOF 提供了精确的操作记录和可读性。

混合持久化工作在 **AOF 日志重写过程**，当开启了混合持久化时，在 **AOF 重写日志时**，fork 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。

也就是说，使用了混合持久化，**AOF 文件**的**前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据**。

**混合持久化优点：**

- 混合持久化结合了 RDB 和 AOF 持久化的优点，开头为 RDB 的格式，使得 Redis 可以更快的启动，同时结合 AOF 的优点，有减低了大量数据丢失的风险。

**混合持久化缺点：**

- AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差；
- 兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了



###  持久化时，如何处理过期键

RDB 文件分为两个阶段，RDB 文件生成阶段和加载阶段。

- **RDB 文件生成阶段**：从内存状态持久化成 RDB（文件）的时候，会对 key 进行过期检查，**过期的键「不会」被保存到新的 RDB 文件中**
- RDB 加载阶段
  - **如果 Redis 是「主服务器」运行模式的话，在载入 RDB 文件时，程序会对文件中保存的键进行检查，过期键「不会」被载入到数据库中**。所以过期键不会对载入 RDB 文件的主服务器造成影响；
  - **如果 Redis 是「从服务器」运行模式的话，在载入 RDB 文件时，不论键是否过期都会被载入到数据库中**。但由于主从服务器在进行数据同步时，从服务器的数据会被清空。所以一般来说，过期键对载入 RDB 文件的从服务器也不会造成影响。

AOF 文件分为两个阶段，AOF 文件写入阶段和 AOF 重写阶段。

- **AOF 文件写入阶段**：当 Redis 以 AOF 模式持久化时，**如果数据库某个过期键还没被删除，那么 AOF 文件会保留此过期键，当此过期键被删除后，Redis 会向 AOF 文件追加一条 DEL 命令来显式地删除该键值**。
- **AOF 重写阶段**：执行 AOF 重写时，会对 Redis 中的键值对进行检查，**已过期的键不会被保存到重写后的 AOF 文件中**，因此不会对 AOF 重写造成任何影响



### Redis 大 Key 对持久化有什么影响？



#### 大 Key 对 AOF 日志的影响

**当使用 Always 策略的时候，如果写入是一个大 Key，主线程在执行 fsync() 函数的时候，阻塞的时间会比较久，因为当写入的数据量很大的时候，数据同步到硬盘这个过程是很耗时的**。

当使用 Everysec 策略的时候，由于是异步执行 fsync() 函数，所以大 Key 持久化的过程（数据同步磁盘）不会影响主线程。

当使用 No 策略的时候，由于永不执行 fsync() 函数，所以大 Key 持久化的过程不会影响主线程



#### 大 Key 对 AOF 重写和 RDB 的影响

AOF 重写机制和 RDB 快照（bgsave 命令）的过程，都会分别通过 `fork()` 函数创建一个子进程来处理任务。会有两个阶段会导致阻塞父进程（主线程）：

- 创建子进程的途中，由于要复制父进程的页表等数据结构，阻塞的时间跟页表的大小有关，页表越大，阻塞的时间也越长；
- 创建完子进程后，如果父进程修改了共享数据中的大 Key，就会发生写时复制，这期间会拷贝物理内存，由于大 Key 占用的物理内存会很大，那么在复制物理内存这一过程，就会比较耗时，所以有可能会阻塞父进程。

> 大 key 除了会影响持久化之外，还会有以下的影响：

- 客户端超时阻塞。由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。
- 引发网络阻塞。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。
- 阻塞工作线程。如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。
- 内存分布不均。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多。

> 如何避免大 Key 呢？

最好在设计阶段，就把大 key 拆分成一个一个小 key。或者，定时检查 Redis 是否存在大 key ，如果该大 key 是可以删除的，不要使用 DEL 命令删除，因为该命令删除过程会阻塞主线程，而是用 unlink 命令（Redis 4.0+）删除大 key，因为该命令的删除过程是异步的，不会阻塞主线程。







## key过期删除与内存淘汰

### key过期删除策略

「**惰性删除+定期删除**」

惰性删除策略的做法是，**不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。**

定期删除策略的做法是，**每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。**

​	Redis 的定期删除的流程：

1. 从过期字典中随机抽取 20 个 key；
2. 检查这 20 个 key 是否过期，并删除已过期的 key；
3. 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。

​	Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。

![_E8_BF_87_E6_9C_9F_E5_88_A0_E9_99_A4_E7_AD_96_E7_95_A5](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021622858.png)



### **内存淘汰机制**

Redis 内存淘汰策略共有八种，这八种策略大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略。配置项为 maxmemory

#### ***1、不进行数据淘汰的策略***

**noeviction**（**Redis3.0之后，默认的内存淘汰策略**） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，而是不再提供服务，直接返回错误。

#### ***2、进行数据淘汰的策略***

针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。 

在设置了过期时间的数据中进行淘汰：

- **volatile-random**：随机淘汰设置了过期时间的任意键值；
- **volatile-ttl**：优先淘汰更早过期的键值。
- **volatile-lru**（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值；
- **volatile-lfu**（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值；

在所有数据范围内进行淘汰：

- **allkeys-random**：随机淘汰任意键值;
- **allkeys-lru**：淘汰整个键值中最久未使用的键值；
- **allkeys-lfu**（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值

![_E5_86_85_E5_AD_98_E6_B7_98_E6_B1_B0_E7_AD_96_E7_95_A5](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021622526.png)



#### **LRU**

 全称是 Least Recently Used 翻译为**最近最少使用**，会选择淘汰最近最少使用的数据。

传统 LRU 算法的实现是基于「链表」结构，链表中的元素按照操作顺序从前往后排列，最新操作的键会被移动到表头，当需要内存淘汰时，只需要删除链表尾部的元素即可，因为链表尾部的元素就代表最久未被使用的元素。

Redis 并没有使用这样的方式实现 LRU 算法，因为传统的 LRU 算法存在两个问题：

- 需要用链表管理所有的缓存数据，这会带来额外的空间开销；

- 当有数据被访问时，需要在链表上把该数据移动到头端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。

  

> Redis 是如何实现 LRU 算法的？

Redis 实现的是一种**近似 LRU 算法**，目的是为了更好的节约内存，它的**实现方式是在 Redis 的对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间**。

当 Redis 进行内存淘汰时，会使用**随机采样的方式来淘汰数据**，它是随机取 5 个值（此值可配置），然后**淘汰最久没有使用的那个**。

Redis 实现的 LRU 算法的优点：

- 不用为所有的数据维护一个大链表，节省了空间占用；
- 不用在每次数据访问时都移动链表项，提升了缓存的性能；

但是 LRU 算法有一个问题，**无法解决缓存污染问题**，比如应用一次读取了大量的数据，而这些数据只会被读取这一次，那么这些数据会留存在 Redis 缓存中很长一段时间，造成缓存污染。



#### LFU 

通过**实现 LFU 算法**来避免「缓存污染」而导致缓存命中率下降的问题

全称是 Least Frequently Used 翻译为**最近最不常用的**，LFU 算法是根据数据访问次数来淘汰数据的，它的核心思想是“如果数据过去被访问多次，那么将来被访问的频率也更高”。

所以， LFU 算法会记录每个数据的访问次数。当一个数据被再次访问时，就会增加该数据的访问次数。这样就解决了偶尔被访问一次之后，数据留存在缓存中很长一段时间的问题，相比于 LRU 算法也更合理一些。

> Redis 是如何实现 LFU 算法的？

LFU 算法相比于 LRU 算法的实现，多记录了「数据的访问频次」的信息。Redis 对象的结构如下：

```c
typedef struct redisObject {
    ...
      
    // 24 bits，用于记录对象的访问信息
    unsigned lru:24;  
    ...
} robj;
```

Redis 对象头中的 lru 字段，在 LRU 算法下和 LFU 算法下使用方式并不相同。

**在 LRU 算法中**，Redis 对象头的 24 bits 的 lru 字段是用来记录 key 的访问时间戳，因此在 LRU 模式下，Redis可以根据对象头中的 lru 字段记录的值，来比较最后一次 key 的访问时间长，从而淘汰最久未被使用的 key。

**在 LFU 算法中**，Redis对象头的 24 bits 的 lru 字段被分成两段来存储，高 16bit 存储 ldt(Last Decrement Time)，用来记录 key 的访问时间戳；低 8bit 存储 logc(Logistic Counter)，用来记录 key 的访问频次。



#### 设置 Redis 最大运行内存

在配置文件 redis.conf 中，可以通过参数 `maxmemory <bytes>` 来设定最大运行内存，只有在 Redis 的运行内存达到了我们设置的最大运行内存，才会触发内存淘汰策略。 不同位数的操作系统，maxmemory 的默认值是不同的：

- 在 64 位操作系统中，maxmemory 的默认值是 0，表示没有内存大小限制，那么不管用户存放多少数据到 Redis 中，Redis 也不会对可用内存进行检查，直到 Redis 实例因内存不足而崩溃也无作为。
- 在 32 位操作系统中，maxmemory 的默认值是 3G，因为 32 位的机器最大只支持 4GB 的内存，而系统本身就需要一定的内存资源来支持运行，所以 32 位操作系统限制最大 3 GB 的可用内存是非常合理的，这样可以避免因为内存不足而导致 Redis 实例崩溃



## 缓存设计

![061e2c04e0ebca3425dd75dd035b6b7b](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409022041722.png)

### 缓存雪崩

当**大量缓存数据在同一时间过期（失效）或者 Redis 故障宕机**时，如果此时有大量的用户请求，都无法在 Redis 中处理，于是全部请求都直接访问数据库，从而导致数据库的压力骤增，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃，这就是**缓存雪崩**的问题。

#### 大量数据同时过期

针对大量数据同时过期而引发的缓存雪崩问题，常见的应对方法有下面这几种：

- 均匀设置过期时间；
- 互斥锁；
- 后台更新缓存；

#### Redis 故障宕机

针对 Redis 故障宕机而引发的缓存雪崩问题，常见的应对方法有下面这几种：

- 服务熔断或请求限流机制；
- 构建 Redis 缓存高可靠集群；



### 缓存击穿

如果缓存中的**某个热点数据过期**了，此时大量的请求访问了该热点数据，就无法从缓存中读取，直接访问数据库，数据库很容易就被高并发的请求冲垮，这就是**缓存击穿**

可以认为缓存击穿是缓存雪崩的一个子集

- 互斥锁方案（Redis 中使用 setNX 方法设置一个状态位，表示这是一种锁定状态），保证同一时间只有一个业务线程请求缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。
- 不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间；





### 缓存穿透

当用户访问的数据，**既不在缓存中，也不在数据库中**，导致请求在访问缓存时，发现缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据，没办法构建缓存数据，来服务后续的请求。那么当有大量这样的请求到来时，数据库的压力骤增，这就是**缓存穿透**

缓存穿透的发生一般有这两种情况：

- 业务误操作，缓存中的数据和数据库中的数据都被误删除了，所以导致缓存和数据库中都没有数据；
- 黑客恶意攻击，故意大量访问某些读取不存在数据的业务；

应对缓存穿透的方案，常见的方案有三种。

- **非法请求的限制**：当有大量恶意请求访问不存在的数据的时候，也会发生缓存穿透，因此在 API 入口处我们要判断求请求参数是否合理，请求参数是否含有非法值、请求字段是否存在，如果判断出是恶意请求就直接返回错误，避免进一步访问缓存和数据库。
- **设置空值或者默认值**：当我们线上业务发现缓存穿透的现象时，可以针对查询的数据，在缓存中设置一个空值或者默认值，这样后续请求就可以从缓存中读取到空值或者默认值，返回给应用，而不会继续查询数据库。
- **使用布隆过滤器快速判断数据是否存在，避免通过查询数据库来判断数据是否存在**：我们可以在写入数据库数据时，使用布隆过滤器做个标记，然后在用户请求到来时，业务线程确认缓存失效后，可以通过查询布隆过滤器快速判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在，即使发生了缓存穿透，大量请求只会查询 Redis 和布隆过滤器，而不会查询数据库，保证了数据库能正常运行，Redis 自身也是支持布隆过滤器的。



#### 布隆过滤器

布隆过滤器由「初始值都为 0 的位图数组」和「 N 个哈希函数」两部分组成。当我们在写入数据库数据时，在布隆过滤器里做个标记，这样下次查询数据是否在数据库时，只需要查询布隆过滤器，如果查询到数据没有被标记，说明不在数据库中。

**当应用要查询数据 x 是否数据库时，通过布隆过滤器只要查到位图数组的第 1、4、6 位置的值是否全为 1，只要有一个为 0，就认为数据 x 不在数据库中**。

布隆过滤器由于是基于哈希函数实现查找的，高效查找的同时**存在哈希冲突的可能性**，比如数据 x 和数据 y 可能都落在第 1、4、6 位置，而事实上，可能数据库中并不存在数据 y，存在误判的情况。

所以，**查询布隆过滤器说数据存在，并不一定证明数据库中存在这个数据，但是查询到数据不存在，数据库中一定就不存在这个数据**。



### 缓存更新策略

常见的缓存更新策略共有3种：

- Cache Aside（旁路缓存）策略；

  - **适合读多写少的场景，不适合写多的场景**

  - **写策略的步骤：**

    - 先更新数据库中的数据，再删除缓存中的数据。

    **读策略的步骤：**

    - 如果读取的数据命中了缓存，则直接返回数据；
    - 如果读取的数据没有命中缓存，则从数据库中读取数据，然后将数据写入到缓存，并且返回给用户。

  - 先更新数据库，再删除缓存也是会出现数据不一致性的问题，但是在实际中，这个问题出现的概率并不高。**因为缓存的写入通常要远远快于数据库的写入**

    - 频繁地清理，这样会对缓存的命中率有一些影响，如果业务对缓存命中率有严格的要求，那么可以考虑两种解决方案：
      - 一种做法是在更新数据时也更新缓存，只是在更新缓存前先加一个分布式锁，因为这样在同一时间只允许一个线程更新缓存，就不会产生并发问题了。当然这么做对于写入的性能会有一些影响；
      - 另一种做法同样也是在更新数据时更新缓存，只是给缓存加一个较短的过期时间，这样即使出现缓存不一致的情况，缓存的数据也会很快过期，对业务的影响也是可以接受。

- Read/Write Through（读穿 / 写穿）策略；

  - ***1、Read Through 策略***

    先查询缓存中数据是否存在，如果存在则直接返回，如果不存在，则由缓存组件负责从数据库查询数据，并将结果写入到缓存组件，最后缓存组件将数据返回给应用。

    ***2、Write Through 策略***

    当有数据更新的时候，先查询要写入的数据在缓存中是否已经存在：

    - 如果缓存中数据已经存在，则更新缓存中的数据，并且由缓存组件同步更新到数据库中，然后缓存组件告知应用程序更新完成。
    - 如果缓存中数据不存在，直接更新数据库，然后返回；

  - 在我们开发过程中相比 Cache Aside 策略要少见一些，原因是我们经常使用的分布式缓存组件，无论是 Memcached 还是 Redis 都不提供写入数据库和自动加载数据库中的数据的功能。而我们在使用本地缓存的时候可以考虑使用这种策略。

- Write Back（写回）策略；

  - Write Back（写回）策略在更新数据的时候，只更新缓存，同时将缓存数据设置为脏的，然后立马返回，并不会更新数据库。对于数据库的更新，会通过批量异步更新的方式进行。

    实际上，Write Back（写回）策略也不能应用到我们常用的数据库和缓存的场景中，因为 Redis 并没有异步更新数据库的功能。

  - **适合写多的场景**，**但是带来的问题是，数据不是强一致性的，而且会有数据丢失的风险**



### 大 key 

大 key 并不是指 key 的值很大，而是 **key 对应的 value 很大**。

一般而言，下面这两种情况被称为大 key：

- String 类型的值大于 10 KB；

- Hash、List、Set、ZSet 类型的元素的个数超过 5000个；

- 通常以Key的大小和Key中成员的数量来综合判定，例如：

  - Key本身的数据量过大：一个String类型的Key，它的值为5 MB。
  - Key中的成员数过多：一个ZSET类型的Key，它的成员数量为10,000个。
  - Key中成员的数据量过大：一个Hash类型的Key，它的成员数量虽然只有2,000个但这些成员的Value（值）总大小为100 MB。
  
  
  
  

#### 问题

- **客户端超时阻塞**。由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。
- **引发网络阻塞**。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。
- **阻塞工作线程**。如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。
- **内存分布不均**。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多，QPS 也会比较大。
- Redis内存达到**maxmemory**参数定义的上限引发操作阻塞或**重要的Key被逐出**，甚至引发内存溢出（Out Of Memory）。





#### 产生的原因

- 大key
  - 在不适用的场景下使用Redis，易造成Key的value过大，如使用String类型的Key存放大体积二进制文件型数据；
  - 业务上线前规划设计不足，没有对Key中的成员进行合理的拆分，造成个别Key中的成员数量过多；
  - 未定期清理无效数据，造成如HASH类型Key中的成员持续不断地增加；
  - 使用LIST类型Key的业务消费侧发生代码故障，造成对应Key的成员只增不减。



#### 处理方案

1. 对大Key进行拆分

   例如将含有数万成员的一个HASH Key拆分为多个HASH Key，并确保每个Key的成员数量在合理范围。在Redis集群架构中，拆分大Key能对数据分片间的内存平衡起到显著作用。

2. 对大Key进行清理

   将不适用Redis能力的数据存至其它存储，并在Redis中删除此类数据。

   **说明**

   - Redis 4.0及之后版本：您可以通过**UNLINK**命令安全地删除大Key甚至特大Key，该命令能够以非阻塞的方式，逐步地清理传入的Key。
   - Redis 4.0之前的版本：建议先通过**SCAN**命令读取部分数据，然后进行删除，避免一次性删除大量key导致Redis阻塞。

3. 监控Redis的内存水位

   您可以通过监控系统设置合理的Redis内存报警阈值进行提醒，例如Redis内存使用率超过70%、Redis的内存在1小时内增长率超过20%等。通过此类监控手段，可以提前规避许多问题，例如LIST数据类型的消费程序故障造成对应Key的列表数量持续增长，将告警转变为预警从而避免故障的发生，更多信息，请参见[报警设置](https://help.aliyun.com/zh/redis/user-guide/alert-settings#concept-sj5-m2z-5db)。

4. 对过期数据进行定期清理

   堆积大量过期数据会造成大Key的产生，例如在HASH数据类型中以增量的形式不断写入大量数据而忽略了数据的时效性。可以通过定时任务的方式对失效数据进行清理。

   **说明**

   在清理HASH数据时，建议通过**HSCAN**命令配合**HDEL**命令对失效数据进行清理，避免清理大量数据造成Redis阻塞。

   

### 热key

通常以其接收到的Key被请求频率来判定，例如：

- QPS集中在特定的Key：Redis实例的总QPS（每秒查询率）为10,000，而其中一个Key的每秒访问量达到了7,000。
- 带宽使用率集中在特定的Key：对一个拥有上千个成员且总大小为1 MB的HASH Key每秒发送大量的**HGETALL**操作请求。
- CPU使用时间占比集中在特定的Key：对一个拥有数万个成员的Key（ZSET类型）每秒发送大量的**ZRANGE**操作请求。



#### 问题

- 占用大量的CPU资源，影响其他请求并导致整体性能降低。
- 集群架构下，产生访问倾斜，即某个数据分片被大量访问，而其他数据分片处于空闲状态，可能引起该数据分片的连接数被耗尽，新的连接建立请求被拒绝等问题。
- 在抢购或秒杀场景下，可能因商品对应库存Key的请求量过大，超出Redis处理能力造成超卖。
- 热Key的请求压力数量超出Redis的承受能力易造成缓存击穿，即大量请求将被直接指向后端的存储层，导致存储访问量激增甚至宕机，从而影响其他业务。

#### 产生的原因

- 预期外的访问量陡增，如突然出现的爆款商品、访问量暴涨的热点新闻、直播间某主播搞活动带来的大量刷屏点赞、游戏中某区域发生多个工会之间的战斗涉及大量玩家等。

#### 处理方案

1. 在Redis集群架构中对热Key进行复制

   在Redis集群架构中，由于热Key的迁移粒度问题，无法将请求分散至其他数据分片，导致单个数据分片的压力无法下降。此时，可以将对应热Key进行复制并迁移至其他数据分片，例如将热Key foo复制出3个内容完全一样的Key并名为foo2、foo3、foo4，将这三个Key迁移到其他数据分片来解决单个数据分片的热Key压力。

   **说明**

   该方案的缺点在于需要联动修改代码，同时带来了数据一致性的挑战（由原来更新一个Key演变为需要更新多个Key），仅建议该方案用来解决临时棘手的问题。

2. 使用读写分离架构

   如果热Key的产生来自于读请求，您可以将实例改造成读写分离架构来降低每个数据分片的读请求压力，甚至可以不断地增加从节点。但是读写分离架构在增加业务代码复杂度的同时，也会增加Redis集群架构复杂度。不仅要为多个从节点提供转发层（如Proxy，LVS等）来实现负载均衡，还要考虑从节点数量显著增加后带来故障率增加的问题。Redis集群架构变更会为监控、运维、故障处理带来了更大的挑战。

   然而，阿里云Redis服务以开箱即用的方式提供服务。在业务发生变化时，您仅需通过变配的方式调整实例架构来轻松应对，例如将主从架构转变为读写分离架构、将读写分构架构转变为集群架构，以及将社区版转变为支持大量高级特性的Tair（企业版）等，更多信息，请参见[变更实例配置](https://help.aliyun.com/zh/redis/user-guide/change-the-configurations-of-an-instance#concept-mgf-z25-tdb)。

   **说明**

   读写分离架构同样存在缺点，在请求量极大的场景下，读写分离架构会产生不可避免的延迟，此时会有读取到脏数据的问题。因此，在读、写压力都较大且对数据一致性要求很高的场景下，读写分离架构并不是最优方案。

3. 互斥锁方案（Redis 中使用 setNX 方法设置一个状态位，表示这是一种锁定状态），保证同一时间只有一个业务线程请求缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。

4. 不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间；









### 快速找出大Key和热Key

Redis提供多种方案帮助您轻松找出大Key与热Key。

|      |      |      |
| ---- | ---- | ---- |
|      |      |      |

| **方法**                                                     | **优缺点**                                                   | **说明**                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [实时Top Key统计](https://help.aliyun.com/zh/redis/user-guide/use-the-real-time-key-statistics-feature#task-2096542)（推荐） | 优点：准确性高、对性能几乎无影响。缺点：展示的Key数量有一定限制，但能满足常规场景下的需求。 | 可实时展示实例中的大Key和热Key信息，同时支持查看4天内大Key和热Key的历史信息。该功能可帮助您掌握Key在内存中的占用、Key的访问频次等信息，溯源分析问题，为您的优化操作提供数据支持。 |
| [离线全量Key分析](https://help.aliyun.com/zh/redis/user-guide/offline-key-analysis#concept-ufz-byl-jgb) | 优点：可对历史备份数据进行分析，对线上服务无影响。缺点：时效性差，RDB文件较大时耗时较长。 | 对Redis的RDB备份文件进行定制化的分析，帮助您发现实例中的大Key，掌握Key在内存中的占用和分布、Key过期时间等信息，为您的优化操作提供数据支持，帮助您避免因Key倾斜引发的内存不足、性能下降等问题。 |
| 通过redis-cli的**bigkeys**和**hotkeys**参数查找大Key和热Key  | 优点：方便、快速、安全。缺点：分析结果不可定制化，准确性与时效性差。 | redis-cli提供了**bigkeys**与**hotkeys**参数能够以遍历的方式分析Redis实例中的所有Key，并返回Key的整体统计信息与每个数据类型中Top1的大Key。以**bigkeys**为例，其仅能分析并输入六种数据类型（STRING、LIST、HASH、SET、ZSET、STREAM），命令示例为`redis-cli -h r-***************.redis.rds.aliyuncs.com -a <password> --bigkeys`。**说明**若您只需要分析STRING类型的大key或是找出成员数量超过10个的HASH Key，则**bigkeys**参数无法直接实现该类需求。 |
| 通过Redis内置命令对目标Key进行分析                           | 优点：方便、对线上服务影响小。缺点：返回的Key序列化长度并不等同于它在内存空间中的真实长度，因此不够准确，仅可作为参考。 | 对不同数据类型的目标Key，分别通过如下风险较低的命令进行分析，来判断目标Key是否符合大Key判定标准。STRING类型：执行**STRLEN**命令，返回对应Key的value的字节数。LIST类型：执行**LLEN**命令，返回对应Key的列表长度。HASH类型：执行**HLEN**命令，返回对应Key的成员数量。SET类型：执行**SCARD**命令，返回对应Key的成员数量。ZSET类型：执行**ZCARD**命令，返回对应Key的成员数量。STREAM类型：执行**XLEN**命令，返回对应Key的成员数量。**说明****DEBUG OBJECT**与**MEMORY USAGE**命令在执行时需占用较多资源，且时间复杂度为O(N)，有阻塞Redis实例的风险，不建议使用。 |
| 通过业务层定位热Key                                          | 优点：可准确并及时地定位热Key。缺点：业务代码复杂度的增加，同时可能会降低一些性能。 | 通过在业务层增加相应的代码对Redis的访问进行记录并异步汇总分析。 |
| 通过redis-rdb-tools工具以定制化方式找出大Key                 | 优点：支持定制化分析，对线上服务无影响。缺点：时效性差，RDB文件较大时耗时较长。 | [Redis-rdb-tools](https://github.com/sripathikrishnan/redis-rdb-tools)是通过Python编写，支持定制化分析Redis RDB快照文件的开源工具。您可以根据您的精细化需求，全面地分析Redis实例中所有Key的内存占用情况，同时也支持灵活地分析查询。 |
| 通过**MONITOR**命令找出热Key                                 | 优点：方便、安全。缺点：会占用CPU、内存、网络资源，时效性与准确性较差。 | Redis的**MONITOR**命令能够忠实地打印Redis中的所有请求，包括时间信息、Client信息、命令以及Key信息。在发生紧急情况时，可以通过短暂执行**MONITOR**命令并将返回信息输入至文件，在关闭**MONITOR**命令后，对文件中请求进行归类分析，找出这段时间中的热Key。**说明**由于**MONITOR**命令对Redis实例性能消耗较大，非特殊情况不推荐使用**MONITOR**命令。 |

## 集群





### 1.主从复制模式

**AP模型**

Redis 主从复制是一种用于提高数据冗余和可用性的机制。它允许多个 Redis 服务器之间建立主从关系，其中一个 Redis 服务器充当主服务器（Master），而其他服务器则充当从服务器（Slave）。

主从服务器之间的命令复制是**异步**进行的。无法实现强一致性保证

主从复制共有三种模式：**全量复制、基于长连接的命令传播、增量复制**。

主从复制的主要目标包括：

1. **数据冗余：** 主从复制可将主服务器上的数据完全复制到一个或多个从服务器，以确保数据的冗余存储。
2. **高可用性：** 当主服务器发生故障时，从服务器可以升级为主服务器，确保系统的连续性和高可用性。
3. **负载均衡：** 从服务器可以分担主服务器的读取负载，以提高性能。主从服务器之间采用的是**「读写分离」**的方式。

![2b7231b6aabb9a9a2e2390ab3a280b2d-20230309232920063](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021628109.png)



#### 设置

##### 命令行设置

```text
# 服务器 B 执行这条命令
replicaof <服务器 A 的 IP 地址> <服务器 A 的 Redis 端口号>
```

接着，服务器 B 就会变成服务器 A 的「从服务器」，然后与主服务器进行第一次同步。

##### 使用配置文件设置

步骤 1：准备主服务器的配置文件

首先，您需要准备主服务器的 Redis 配置文件。通常，Redis 的配置文件位于 `/etc/redis/redis.conf`（Linux 系统上）或 Redis 安装目录中。确保主服务器的配置文件正确配置，并具有以下设置：

```Plaintext
# 设置 Redis 为主服务器
port 6379             # 主服务器的端口号
daemonize yes         # 后台运行 Redis
pidfile /var/run/redis/redis-server.pid  # 指定 PID 文件位置
```

步骤 2：准备从服务器的配置文件

然后，您需要准备从服务器的 Redis 配置文件。通常，从服务器的配置文件也位于 `/etc/redis/redis.conf`，但您可以使用不同的配置文件名。在从服务器的配置文件中，确保具有以下设置：

```Plaintext
# 设置 Redis 为从服务器
port 6380             # 从服务器的端口号，可以与主服务器不同
daemonize yes         # 后台运行 Redis
pidfile /var/run/redis/redis-server-slave.pid  # 指定 PID 文件位置
slaveof <master_ip> <master_port>  # 指定主服务器的 IP 和端口
```





#### 第一次同步/全量复制

主从服务器间的第一次同步的过程可分为三个阶段：

- 第一阶段是建立链接、协商同步；
- 第二阶段是主服务器同步数据给从服务器；
- 第三阶段是主服务器发送新写操作命令给从服务器。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021631741.png" alt="ea4f7e86baf2435af3999e5cd38b6a26" style="zoom:50%;" />

*第一阶段：建立链接、协商同步*

执行了 replicaof 命令后，从服务器就会给主服务器发送 `psync` 命令，表示要进行数据同步。

psync 命令包含两个参数，分别是**主服务器的 runID** 和**复制进度 offset**。

- runID，每个 Redis 服务器在启动时都会自动生产一个随机的 ID 来唯一标识自己。当从服务器和主服务器第一次同步时，因为不知道主服务器的 run ID，所以将其设置为 "?"。
- offset，表示复制的进度，第一次同步时，其值为 -1。

主服务器收到 psync 命令后，会用 `FULLRESYNC` 作为响应命令返回给对方。

并且这个响应命令会带上两个参数：主服务器的 runID 和主服务器目前的复制进度 offset。从服务器收到响应后，会记录这两个值。

FULLRESYNC 响应命令的意图是采用**全量复制**的方式，也就是主服务器会把所有的数据都同步给从服务器。

所以，第一阶段的工作时为了全量复制做准备。



*第二阶段：主服务器同步数据给从服务器*

接着，主服务器会执行 bgsave 命令来生成 RDB 文件，然后把文件发送给从服务器。

从服务器收到 RDB 文件后，会先清空当前的数据，然后载入 RDB 文件。

这里有一点要注意，主服务器生成 RDB 这个过程是不会阻塞主线程的，因为 bgsave 命令是产生了一个子进程来做生成 RDB 文件的工作，是异步工作的，这样 Redis 依然可以正常处理命令。

但是，这期间的写操作命令并没有记录到刚刚生成的 RDB 文件中，这时主从服务器间的数据就不一致了。

那么为了保证主从服务器的数据一致性，**主服务器在下面这三个时间间隙中将收到的写操作命令，写入到 replication buffer 缓冲区里**：

- 主服务器生成 RDB 文件期间；
- 主服务器发送 RDB 文件给从服务器期间；
- 「从服务器」加载 RDB 文件期间；



*第三阶段：主服务器发送新写操作命令给从服务器*

在主服务器生成的 RDB 文件发送完，从服务器收到 RDB 文件后，丢弃所有旧数据，将 RDB 数据载入到内存。完成 RDB 的载入后，会回复一个确认消息给主服务器。

接着，主服务器将 replication buffer 缓冲区里所记录的写操作命令发送给从服务器，从服务器执行来自主服务器 replication buffer 缓冲区里发来的命令，这时主从服务器的数据就一致了。

至此，主从服务器的第一次同步的工作就完成了





#### 基于长连接的命令传播

主从服务器在完成第一次同步后，双方之间就会维护一个 TCP 连接。而且这个连接是长连接的，目的是避免频繁的 TCP 连接和断开带来的性能开销。

上面的这个过程被称为**基于长连接的命令传播**，通过这种方式来保证第一次同步后的主从服务器的数据一致性

在主服务器进行命令传播时，不仅会将写命令发送给从服务器，还会将写命令写入到 repl_backlog_buffer 缓冲区里，因此 这个缓冲区里会保存着最近传播的写命令。



#### 增量复制

从 Redis 2.8 开始，网络断开又恢复后，从主从服务器会采用**增量复制**的方式继续同步，也就是只会把网络断开期间主服务器接收到的写操作命令，同步给从服务器。

网络断开后，当从服务器重新连上主服务器时，从服务器会通过 psync 命令将自己的复制偏移量 slave_repl_offset 发送给主服务器，主服务器根据自己的 master_repl_offset 和 slave_repl_offset 之间的差距，然后来决定对从服务器执行哪种同步操作：

- 如果判断出从服务器要读取的数据还在 repl_backlog_buffer 缓冲区里，那么主服务器将采用**增量同步**的方式；
- 相反，如果判断出从服务器要读取的数据已经不存在 repl_backlog_buffer 缓冲区里，那么主服务器将采用**全量同步**的方式。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021649869.png" alt="e081b470870daeb763062bb873a4477e" style="zoom:50%;" />

主要有三个步骤：

- 从服务器在恢复网络后，会发送 psync 命令给主服务器，此时的 psync 命令里的 offset 参数不是 -1；
- 主服务器收到该命令后，然后用 CONTINUE 响应命令告诉从服务器接下来采用增量复制的方式同步数据；
- 然后主服务将主从服务器断线期间，所执行的写命令发送给从服务器，然后从服务器执行这些命令。

那么关键的问题来了，**主服务器怎么知道要将哪些增量数据发送给从服务器呢？**

答案藏在这两个东西里：

- **repl_backlog_buffer**，是一个「**环形**」缓冲区，用于主从服务器断连后，从中找到差异的数据；
- **replication offset**，标记上面那个缓冲区的同步进度，主从服务器都有各自的偏移量，主服务器使用 master_repl_offset 来记录自己「*写*」到的位置，从服务器使用 slave_repl_offset 来记录自己「*读*」到的位置。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021651109.png" alt="2db4831516b9a8b79f833cf0593c1f12" style="zoom:50%;" />

repl_backlog_buffer 缓行缓冲区的默认大小是 1M，并且由于它是一个环形缓冲区，所以当缓冲区写满后，主服务器继续写入的话，就会覆盖之前的数据。

因此，**为了避免在网络恢复时，主服务器频繁地使用全量同步的方式，我们应该调整下 repl_backlog_buffer 缓冲区大小，尽可能的大一些**，修改配置文件里下面这个参数项的值就可以。

```shell
repl-backlog-size 1mb
```

repl_backlog_buffer 最小的大小可以根据这面这个公式估算。

![5e9e65a4a59b3688fa37cadbd87bb5ac](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021652034.png)

- second 为从服务器断线后重新连接上主服务器所需的平均 时间(以秒计算)。
- write_size_per_second 则是主服务器平均每秒产生的写命令数据量大小。



#### 从库的过期键

当 Redis 运行在主从模式下时，**从库不会进行过期扫描，从库对过期的处理是被动的**。也就是即使从库中的 key 过期了，如果有客户端访问从库时，依然可以得到 key 对应的值，像未过期的键值对一样返回。

从库的过期键处理依靠主服务器控制，**主库在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从库**，从库通过执行这条 del 指令来删除过期的 key



#### 分摊主服务器的压力

主服务器是可以有多个从服务器的，如果从服务器数量非常多，而且都与主服务器进行全量同步的话，就会带来两个问题：

- 由于是通过 bgsave 命令来生成 RDB 文件的，那么主服务器就会忙于使用 fork() 创建子进程，如果主服务器的内存数据非大，在执行 fork() 函数时是会阻塞主线程的，从而使得 Redis 无法正常处理请求；
- 传输 RDB 文件会占用主服务器的网络带宽，会对主服务器响应命令请求产生影响。

从服务器可以有自己的从服务器，我们可以把拥有从服务器的从服务器当作经理角色，它不仅可以接收主服务器的同步数据，自己也可以同时作为主服务器的形式将数据同步给从服务器，

**主服务器生成 RDB 和传输 RDB 的压力可以分摊到充当经理角色的从服务器**。

```text
replicaof <目标服务器的IP> 6379
```

此时如果目标服务器本身也是「从服务器」，那么该目标服务器就会成为「经理」的角色

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021643883.png" alt="4d850bfe8d712d3d67ff13e59b919452" style="zoom:50%;" />

#### 心态检测机制

Redis 判断节点是否正常工作，基本都是通过互相的 ping-pong 心态检测机制，如果有一半以上的节点去 ping 一个节点的时候没有 pong 回应，集群就会认为这个节点挂掉了，会断开与这个节点的连接。

Redis 主从节点发送的心态间隔是不一样的，而且作用也有一点区别：

- Redis 主节点默认每隔 10 秒对从节点发送 ping 命令，判断从节点的存活性和连接状态，可通过参数repl-ping-slave-period控制发送频率。
- Redis 从节点每隔 1 秒发送 replconf ack{offset} 命令，给主节点上报自身当前的复制偏移量，目的是为了：
  - 实时监测主从节点网络状态；
  - 上报自身复制偏移量， 检查复制数据是否丢失， 如果从节点数据丢失， 再从主节点的复制缓冲区中拉取丢失数据



#### replication buffer和repl backlog buffer

- 出现的阶段不一样：
  - repl backlog buffer 是在**增量复制**阶段出现，**一个主节点只分配一个 repl backlog buffer**；
  - replication buffer 是在全量复制阶段和增量复制阶段都会出现，**主节点会给每个新连接的从节点，分配一个 replication buffer**；
- 这两个 Buffer 都有大小限制的，当缓冲区满了之后，发生的事情不一样：
  - 当 repl backlog buffer 满了，因为是环形结构，会直接**覆盖起始位置数据**;
  - 当 replication buffer 满了，会导致连接断开，删除缓存，从节点重新连接，**重新开始全量复制**。

#### 主从数据不一致

> 为什么会出现主从数据不一致？

主从数据不一致，就是指客户端从从节点中读取到的值和主节点中的最新值并不一致。

**因为主从节点间的命令复制是异步进行的**，所以无法实现强一致性保证（主从数据时时刻刻保持一致）。

具体来说，在主从节点命令传播阶段，主节点收到新的写命令后，会发送给从节点。但是，主节点并不会等到从节点实际执行完命令后，再把结果返回给客户端，而是主节点自己在本地执行完命令后，就会向客户端返回结果了。如果从节点还没有执行主节点同步过来的命令，主从节点间的数据就不一致了。

> 如何如何应对主从数据不一致？

第一种方法，尽量保证主从节点间的网络连接状况良好，避免主从节点在不同的机房。

第二种方法，可以开发一个外部程序来监控主从节点间的复制进度。具体做法：

- Redis 的 INFO replication 命令可以查看主节点接收写命令的进度信息（master_repl_offset）和从节点复制写命令的进度信息（slave_repl_offset），所以，我们就可以开发一个监控程序，先用 INFO replication 命令查到主、从节点的进度，然后，我们用 master_repl_offset 减去 slave_repl_offset，这样就能得到从节点和主节点间的复制进度差值了。
- 如果某个从节点的进度差值大于我们预设的阈值，我们可以让客户端不再和这个从节点连接进行数据读取，这样就可以减少读到不一致数据的情况。不过，为了避免出现客户端和所有从节点都不能连接的情况，我们需要把复制进度差值的阈值设置得大一些。



#### 主从切换如何减少数据丢失？

主从切换过程中，产生数据丢失的情况有两种：

- 异步复制同步丢失
- 集群产生脑裂数据丢失

我们不可能保证数据完全不丢失，只能做到使得尽量少的数据丢失。



##### 异步复制同步丢失

对于 Redis 主节点与从节点之间的数据复制，是异步复制的，当客户端发送写请求给主节点的时候，客户端会返回 ok，接着主节点将写请求异步同步给各个从节点，但是如果此时主节点还没来得及同步给从节点时发生了断电，那么主节点内存中的数据会丢失。

> 减少异步复制的数据丢失的方案

Redis 配置里有一个参数 min-slaves-max-lag，表示一旦所有的从节点数据复制和同步的延迟都超过了 min-slaves-max-lag 定义的值，那么**主节点就会拒绝接收任何请求**。

假设将 min-slaves-max-lag 配置为 10s 后，根据目前 master->slave 的复制速度，如果数据同步完成所需要时间超过10s，就会认为 master 未来宕机后损失的数据会很多，master 就拒绝写入新请求。这样就能将 master 和 slave 数据差控制在10s内，即使 master 宕机也只是这未复制的 10s 数据。

那么对于客户端，当客户端发现 master 不可写后，：

1. 采取降级措施，将数据暂时写入本地缓存和磁盘中，在一段时间（等 master 恢复正常）后重新写入 master 来保证数据不丢失，
2. 将数据写入 kafka 消息队列，等 master 恢复正常，再隔一段时间去消费 kafka 中的数据，让将数据重新写入 master 。



##### 集群产生脑裂数据丢失

由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了。

> 减少脑裂的数据丢的方案

当主节点发现「从节点下线的数量太多」，或者「网络延迟太大」的时候，那么主节点会禁止写操作，直接把错误返回给客户端。

在 Redis 的配置文件中有两个参数我们可以设置：

- min-slaves-to-write x，主节点必须要有**至少 x 个从节点连接**，如果小于这个数，主节点会禁止写数据。
- min-slaves-max-lag x，主从数据复制和同步的延迟**不能超过 x 秒**，如果主从同步的延迟超过 x 秒，主节点会禁止写数据。

我们可以把 min-slaves-to-write 和 min-slaves-max-lag 这两个配置项搭配起来使用，分别给它们设置一定的阈值，假设为 N 和 T。

这两个配置项组合后的要求是，**主节点连接的从节点中至少有 N 个从节点，「并且」主节点进行数据复制时的 ACK 消息延迟不能超过 T 秒**，否则，主节点就不会再接收客户端的写请求了。



#### 主从如何做到故障自动切换？

主节点挂了 ，从节点是无法自动升级为主节点的，这个过程需要人工处理，在此期间 Redis 无法对外提供写操作。

此时，Redis **哨兵机制**就登场了，哨兵在发现主节点出现故障时，由哨兵自动完成故障发现和故障转移，并通知给应用方，从而实现高可用性。







### 2.哨兵模式

#### 介绍

通常是用**三个哨兵**来保证高可用性，通过选举出一个领导者来监控其他节点，领导者挂了重新选举

Redis Sentinel 是 Redis 的高可用性解决方案，用于监控和管理 Redis 主从复制集群。它的主要任务包括：

- **监控：** 哨兵会定期检查主服务器和从服务器的状态，以确保它们正常运行。
- **故障检测：** 如果主服务器出现故障，哨兵会自动检测并触发一次故障切换。它会选择一个从服务器升级为新的主服务器，以确保系统的高可用性。
- **通知：** 哨兵可以向管理员发送警报，以便及时采取措施来修复问题。

哨兵节点通过 Redis 的**发布者/订阅者**机制，哨兵之间可以相互感知，相互连接，然后组成哨兵集群，同时哨兵又通过 INFO 命令，在主节点里获得了所有从节点连接信息，于是就能和从节点建立连接，并进行监控了。

#### 哨兵集群原理

在主从集群中，主节点上有一个名为`__sentinel__:hello`的频道，哨兵 A 把自己的 IP 地址和端口的信息发布到`__sentinel__:hello` 频道上，哨兵 B 和 C 订阅了该频道。那么此时，哨兵 B 和 C 就可以从这个频道直接获取哨兵 A 的 IP 地址和端口号

> 哨兵集群会对「从节点」的运行状态进行监控，那哨兵集群如何知道「从节点」的信息？

主节点知道所有「从节点」的信息，所以哨兵会每 10 秒一次的频率向主节点发送 INFO 命令来获取所有「从节点」的信息。

如下图所示，哨兵 B 给主节点发送 INFO 命令，主节点接受到这个命令后，就会把从节点列表返回给哨兵。接着，哨兵就可以根据从节点列表中的连接信息，和每个从节点建立连接，并在这个连接上持续地对从节点进行监控。哨兵 A 和 C 可以通过相同的方法和从节点建立连接。



#### 流程

*1、第一轮投票：判断主节点下线*

哨兵会每隔 1 秒给所有主从节点发送 PING 命令，当主从节点收到 PING 命令后，会发送一个响应命令给哨兵，这样就可以判断它们是否在正常运行。「规定的时间」是配置项 `down-after-milliseconds` 参数设定的，单位是毫秒。

当哨兵集群中的某个哨兵判定主节点下线（**主观下线**）后，就会向其他哨兵发起命令，其他哨兵收到这个命令后，就会根据自身和主节点的网络状况，做出赞成投票或者拒绝投票的响应。

因为有可能「主节点」其实并没有故障，可能只是因为主节点的系统压力比较大或者网络发送了拥塞，导致主节点没有在规定时间内响应哨兵的 PING 命令

当这个哨兵的赞同票数达到哨兵配置文件中的 quorum 配置项设定的值后，这时主节点就会被该哨兵标记为「**客观下线**」。

quorum 的值一般设置为哨兵个数的二分之一加 1

*2、第二轮投票：选出哨兵 leader*

某个哨兵判定主节点客观下线后，该哨兵就会发起投票，告诉其他哨兵，它想成为 leader，想成为 leader 的哨兵节点，要满足两个条件：

- 第一，拿到半数以上的赞成票；
- 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。

*3、由哨兵 leader 进行主从故障转移*

选举出了哨兵 leader 后，就可以进行主从故障转移的过程了。该操作包含以下四个步骤：

- 第一步：在已下线主节点（旧主节点）属下的所有「从节点」里面，挑选出一个从节点，并将其转换为主节点，选择的规则：

  - 过滤掉已经离线的从节点；

  - 过滤掉历史网络连接状态不好的从节点；

    怎么判断从节点之前的网络连接状态不好呢？

    Redis 有个叫 down-after-milliseconds * 10 配置项，其 down-after-milliseconds 是主从节点断连的最大连接超时时间。如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从节点的网络状况不好，不适合作为新主节点。

  - 将剩下的从节点，进行三轮考察：**优先级、复制进度、ID 号**。在每一轮考察过程中，如果找到了一个胜出的从节点，就将其作为新主节点。 slave-priority 配置项，可以给从节点设置优先级。

  - 选举出从节点后，哨兵 leader 向被选中的从节点发送 `SLAVEOF no one` 命令，让这个从节点解除从节点的身份，将其变为新主节点。在发送 `SLAVEOF no one` 命令之后，哨兵 leader 会以每秒一次的频率向被升级的从节点发送 `INFO` 命令（没进行故障转移之前，`INFO` 命令的频率是每十秒一次），并观察命令回复中的角色信息，当被升级节点的角色信息从原来的 slave 变为 master 时，哨兵 leader 就知道被选中的从节点已经顺利升级为主节点了

- 第二步：让已下线主节点属下的所有「从节点」修改复制目标，修改为复制「新主节点」；

  - 哨兵 leader 向所有从节点发送 `SLAVEOF`

- 第三步：将新主节点的 IP 地址和信息，通过「发布者/订阅者机制」通知给客户端；

  - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021851416.webp" alt="_E5_93_A8_E5_85_B5_E9_A2_91_E9_81_93" style="zoom: 25%;" />

- 第四步：继续监视旧主节点，当这个旧主节点重新上线时，将它设置为新主节点的从节点；

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409021850094.webp" alt="_E9_80_89_E4_B8_BB_E8_BF_87_E7_A8_8B" style="zoom:50%;" />

#### 配置

以下是如何设置 Redis Sentinel 模式的步骤：

步骤 1：准备 Redis 主从复制集群

首先，您需要创建一个 Redis 主从复制集群。这个集群由一个 Redis 主服务器和多个 Redis 从服务器组成。确保主从服务器正常运行，并已正确配置。

步骤 2：配置哨兵

接下来，您需要为 Redis Sentinel 配置文件。在配置文件中，您可以指定哨兵监控的主服务器和从服务器，以及其他参数。以下是一个示例的哨兵配置文件：

```Plaintext
# 哨兵的 ID
sentinel my-sentinel

# 监控的主服务器名称和地址
sentinel monitor my-master <master_ip> <master_port> <quorum>

# 哨兵之间的通信密码
sentinel auth-pass my-sentinel <password>

# 哨兵监控的从服务器
sentinel down-after-milliseconds my-master 10000
sentinel failover-timeout my-master 60000
```

在上述配置中，您需要替换以下内容：

- `<master_ip>`：主服务器的 IP 地址。
- `<master_port>`：主服务器的端口号。
- `<quorum>`：用于决定故障切换的投票数。
- `<password>`：哨兵之间的通信密码。

步骤 3：启动哨兵

使用以下命令启动 Redis Sentinel：

```Bash
redis-sentinel /path/to/sentinel.conf
```

其中，`/path/to/sentinel.conf` 是您的哨兵配置文件的路径。

步骤 4：监控和故障切换

哨兵将定期检查主服务器和从服务器的状态。如果主服务器出现故障，哨兵会协调一次故障切换，选择一个从服务器升级为新的主服务器，并重新配置其他从服务器以连接到新的主服务器。这确保了高可用性和数据的不丢失。

您可以使用以下命令来查看哨兵的状态和监控情况：

- `SENTINEL master my-master`：查看主服务器的状态和监控情况。
- `SENTINEL slaves my-master`：查看从服务器的状态。
- `SENTINEL get-master-addr-by-name my-master`：查看当前主服务器的地址和端口。

Redis Sentinel 是一个强大的工具，可确保 Redis 集群的高可用性。通过设置多个哨兵来监控主从复制集群，您可以实现自动故障切换，确保系统连续性。



### 3.切片集群模式

当 **Redis 缓存数据量大到一台服务器无法缓存时**，就需要使用 **Redis 切片集群**（Redis Cluster ）方案，它将数据分布在不同的服务器上，以此来降低系统对单主节点的依赖，从而提高 Redis 服务的读写性能。

Redis Cluster 方案采用**哈希槽（Hash Slot）**，来处理数据和节点之间的映射关系。在 Redis Cluster 方案中，**一个切片集群共有 16384 个哈希槽**，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中，具体执行过程分为两大步：

- 根据键值对的 key，按照 [CRC16 算法 (opens new window)](https://en.wikipedia.org/wiki/Cyclic_redundancy_check)计算一个 16 bit 的值。
- 再用 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。

接下来的问题就是，这些哈希槽怎么被映射到具体的 Redis 节点上的呢？有两种方案：

- **平均分配：** 在使用 cluster create 命令创建 Redis 集群时，Redis 会自动把所有哈希槽平均分布到集群节点上。比如集群中有 9 个节点，则每个节点上槽的个数为 16384/9 个。
- **手动分配：** 可以使用 cluster meet 命令手动建立节点间的连接，组成集群，再使用 cluster addslots 命令，指定每个节点上的哈希槽个数。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408182259125.png" alt="redis_E5_88_87_E7_89_87_E9_9B_86_E7_BE_A4_E6_98_A0_E5_B0_84_E5_88_86_E5_B8_83_E5_85_B3_E7_B3_BB" style="zoom:50%;" />

上图中的切片集群一共有 2 个节点，假设有 4 个哈希槽（Slot 0～Slot 3）时，我们就可以通过命令手动分配哈希槽，比如节点 1 保存哈希槽 0 和 1，节点 2 保存哈希槽 2 和 3。

```c
redis-cli -h 192.168.1.10 –p 6379 cluster addslots 0,1
redis-cli -h 192.168.1.11 –p 6379 cluster addslots 2,3
```

然后在集群运行的过程中，key1 和 key2 计算完 CRC16 值后，对哈希槽总个数 4 进行取模，再根据各自的模数结果，就可以被映射到哈希槽 1（对应节点1） 和 哈希槽 2（对应节点2）。

需要注意的是，在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。



#### CRC16 算法原理

CRC16（Cyclic Redundancy Check 16-bit）是一种循环冗余校验算法，常用于检测传输中的数据错误。CRC 算法使用二进制除法，并返回一个固定长度的校验值（在 Redis 中是 16-bit 的结果）。在 Redis Cluster 中，CRC16 主要用于计算键的哈希值。

**算法过程**：

1. **多项式**: CRC 使用的核心是一个生成多项式（polynomial），这个多项式在 CRC 算法中用于对输入的数据进行位运算。常见的 CRC16 多项式有：

   **CRC-16-CCITT**: 生成多项式 `0x1021`（x16+x12+x5+1x^{16} + x^{12} + x^5 + 1x16+x12+x5+1）

   **CRC-16-IBM**（也称为 CRC-16-ANSI）: 生成多项式 `0x8005`（x16+x15+x2+1x^{16} + x^{15} + x^2 + 1x16+x15+x2+1）

2. **输入数据处理**:

    输入数据通常被视为一个二进制串，这些数据会按照位与生成多项式进行**异或（XOR**）操作，并执行一系列**移位**运算。

   CRC16 的输出是一个 16 位的校验码，作为数据完整性的校验值。接收方在接收到数据时会重新计算 CRC 校验码，如果重新计算的结果与发送方附加的校验码一致，则说明数据没有错误。

**为什么使用 CRC16**：

1. **快速计算**：CRC16 是一种轻量级的哈希算法，计算复杂度低、效率非常高，适合 Redis 需要快速哈希分片的需求。
2. **分布均匀**：CRC16 算法能较好地将键分布到哈希槽中，保证数据分布均匀，避免热点问题。
3. **校验功能**：CRC 本质上是一种校验算法，它在数据传输中的作用非常广泛，能保证数据的一致性。在 Redis 中使用 CRC16，可以保证哈希值计算的正确性和可靠性。
4. **低碰撞率**：CRC16 能较好地保证哈希值的唯一性，降低哈希碰撞的风险，进而减少数据冲突。





### 脑裂

当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端。

在 Redis 的配置文件中有两个参数我们可以设置：

- min-slaves-to-write x，主节点必须要有至少 x 个从节点连接，如果小于这个数，主节点会禁止写数据。
- min-slaves-max-lag x，主从数据复制和同步的延迟不能超过 x 秒，如果超过，主节点会禁止写数据。

这两个配置项组合后的要求是，主库连接的从库中至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则，主库就不会再接收客户端的写请求了。



### 分布式锁

#### 介绍

分布式锁是用于分布式环境下并发控制的一种机制，用于控制某个资源在同一时刻只能被一个应用所使用。

Redis 本身可以被多个客户端共享访问，正好就是一个共享存储系统，可以用来保存分布式锁，而且 Redis 的读写性能高，可以应对高并发的锁操作场景。

Redis 的 SET 命令有个 NX 参数可以实现「key不存在才插入」，所以可以用它来实现分布式锁：

- 如果 key 不存在，则显示插入成功，可以用来表示加锁成功；
- 如果 key 存在，则会显示插入失败，可以用来表示加锁失败。

基于 Redis 节点实现分布式锁时，对于加锁操作，我们需要满足三个条件。

- 加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁；
- 锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间；
- 锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端；

满足这三个条件的分布式命令如下：

```c
SET lock_key unique_value NX PX 10000 
```

- lock_key 就是 key 键；
- unique_value 是客户端生成的唯一的标识，区分来自不同客户端的锁操作；
- NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作；
- PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。

而解锁的过程就是将 lock_key 键删除（del lock_key），但不能乱删，要保证执行操作的客户端就是加锁的客户端。所以，解锁的时候，我们要先判断锁的 unique_value 是否为加锁客户端，是的话，才将 lock_key 键删除。

可以看到，解锁是有两个操作，这时就需要 Lua 脚本来保证解锁的原子性，因为 Redis 在执行 Lua 脚本时，可以以原子性的方式执行，保证了锁释放操作的原子性。

```c
// 释放锁时，先比较 unique_value 是否相等，避免锁的误释放
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

这样一来，就通过使用 SET 命令和 Lua 脚本在 Redis 单节点上完成了分布式锁的加锁和解锁。

> 基于 Redis 实现分布式锁有什么优缺点？

基于 Redis 实现分布式锁的**优点**：

1. 性能高效（这是选择缓存实现分布式锁最核心的出发点）。
2. 实现方便。很多研发工程师选择使用 Redis 来实现分布式锁，很大成分上是因为 Redis 提供了 setnx 方法，实现分布式锁很方便。
3. 避免单点故障（因为 Redis 是跨集群部署的，自然就避免了单点故障）。

基于 Redis 实现分布式锁的**缺点**：

- 超时时间不好设置

  。如果锁的超时时间设置过长，会影响性能，如果设置的超时时间过短会保护不到共享资源。比如在有些场景中，一个线程 A 获取到了锁之后，由于业务代码执行时间可能比较长，导致超过了锁的超时时间，自动失效，注意 A 线程没执行完，后续线程 B 又意外的持有了锁，意味着可以操作共享资源，那么两个线程之间的共享资源就没办法进行保护了。

  - **那么如何合理设置超时时间呢？** 我们可以基于续约的方式设置超时时间：先给锁设置一个超时时间，然后启动一个守护线程，让守护线程在一段时间后，重新设置这个锁的超时时间。实现方式就是：写一个守护线程，然后去判断锁的情况，当锁快失效的时候，再次进行续约加锁，当主线程执行完成后，销毁续约锁即可，不过这种方式实现起来相对复杂。

- **Redis 主从复制模式中的数据是异步复制的，这样导致分布式锁的不可靠性**。如果在 Redis 主节点获取到锁后，在没有同步到其他节点时，Redis 主节点宕机了，此时新的 Redis 主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。



#### Redlock（红锁）

> Redis 如何解决集群情况下分布式锁的可靠性？

为了保证集群环境下分布式锁的可靠性，Redis 官方已经设计了一个分布式锁算法 Redlock（红锁）。

它是基于**多个 Redis 节点**的分布式锁，即使有节点发生了故障，锁变量仍然是存在的，客户端还是可以完成锁操作。官方推荐是至少部署 5 个 Redis 节点，而且都是主节点，它们之间没有任何关系，都是一个个孤立的节点。

Redlock 算法的基本思路，**是让客户端和多个独立的 Redis 节点依次请求申请加锁，如果客户端能够和半数以上的节点成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败**。

这样一来，即使有某个 Redis 节点发生故障，因为锁的数据在其他节点上也有保存，所以客户端仍然可以正常地进行锁操作，锁的数据也不会丢失。

Redlock 算法加锁三个过程：

- 第一步是，客户端获取当前时间（t1）。
- 第二步是，客户端按顺序依次向 N 个 Redis 节点执行加锁操作：
  - 加锁操作使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识。
  - 如果某个 Redis 节点发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给「加锁操作」设置一个超时时间（不是对「锁」设置超时时间，而是对「加锁操作」设置超时时间），加锁操作的超时时间需要远远地小于锁的过期时间，一般也就是设置为几十毫秒。
- 第三步是，一旦客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁，就再次获取当前时间（t2），然后计算计算整个加锁过程的总耗时（t2-t1）。如果 t2-t1 < 锁的过期时间，此时，认为客户端加锁成功，否则认为加锁失败。

可以看到，加锁成功要同时满足两个条件（*简述：如果有超过半数的 Redis 节点成功的获取到了锁，并且总耗时没有超过锁的有效时间，那么就是加锁成功*）：

- 条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁；
- 条件二：客户端从大多数节点获取锁的总耗时（t2-t1）小于锁设置的过期时间。

加锁成功后，客户端需要重新计算这把锁的有效时间，计算的结果是「锁最初设置的过期时间」减去「客户端从大多数节点获取锁的总耗时（t2-t1）」。如果计算的结果已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。

加锁失败后，客户端向**所有 Redis 节点发起释放锁的操作**，释放锁的操作和在单节点上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。





## linux

Redis 6.0 版本支持的 I/O 多线程特性，默认情况下 I/O 多线程只针对发送响应数据（write client socket），并不会以多线程的方式处理读请求（read client socket）。要想开启多线程处理客户端读请求，就需要把 Redis.conf 配置文件中的 io-threads-do-reads 配置项设为 yes。

```c
//读请求也使用io多线程
io-threads-do-reads yes 
```

同时， Redis.conf 配置文件中提供了 IO 多线程个数的配置项。

```c
// io-threads N，表示启用 N-1 个 I/O 多线程（主线程也算一个 I/O 线程）
io-threads 4 
```

关于线程数的设置，官方的建议是如果为 4 核的 CPU，建议线程数设置为 2 或 3，如果为 8 核 CPU 建议线程数设置为 6，线程数一定要小于机器核数，线程数并不是越大越好。



# Redis

微服务多台机器读写 分布式缓存

库 "github.com/ redis/go-redis/v9"

在 CentOS 上下载和安装 Redis 非常简单。你可以按照以下步骤在 CentOS 上安装 Redis：

1. **更新包管理器：** 打开终端，并使用以下命令更新包管理器：

```Bash
sudo yum update
```

1. **安装 Redis：** 在终端中运行以下命令来安装 Redis：

```Bash
sudo yum install redis
```

1. **启动 Redis 服务器：** 安装完成后，你可以使用以下命令来启动 Redis 服务器：

```Bash
sudo systemctl start redis
```

1. **设置 Redis 开机自启：** 如果你希望 Redis 在系统启动时自动启动，可以使用以下命令启用 Redis 服务自启动：

```Bash
sudo systemctl enable redis
```

1. **检查 Redis 状态：** 你可以使用以下命令检查 Redis 服务器的状态：

```Bash
sudo systemctl status redis
```

1. 如果一切正常，你应该会看到 Redis 服务器正在运行。

如果你需要进一步自定义 Redis 的配置，你可以编辑 `/etc/redis.conf` 配置文件。在这个文件中，你可以设置监听地址、端口、密码以及其他各种参数。

## 示例

```Go
// redis-cli 连接到redis，查看redis状态
//redis-server启动 Redis 服务器  但是请注意，如果关闭终端，Redis 服务器也会随之停止（redis-server --bind 0.0.0.0 --port 6380）  
//执行代码前，先确保启动了Redis服务
// linux:sudo service redis-server start/status

GET/SET/DEL/INCR/SETNX
HSET/HGET/HINCRBY
LPUSH/RPOP/LRANGE
ZADD/ZRANGEBYSCORE/ZREVRANGE/ZINCRBY/ZSCORE

// 使用 Count 方法计算记录数量
    var count int64
    db.Model(&User{}).Where("name = ?", "John").Count(&count)



func string(ctx context.Context, client *redis.Client) {
        key := "name"
        value := "大脸猫"
        err := client.Set(ctx, key, value, 1*time.Second).Err() //1秒后失效。0表示永不失效
    err := common.Redis.Set(fmt.Sprintf("%d:%d", videoId, userId), 1, 500*time.Millisecond)//返回err不一样
        checkError(err)

        client.Expire(ctx, key, 3*time.Second) //通过Expire设置3秒后失效  两种设置过期的时间
        time.Sleep(2 * time.Second)

        v2, err := client.Get(ctx, key).Result()
        checkError(err)
        fmt.Println(v2)

        client.Del(ctx, key)
}

func list(ctx context.Context, client *redis.Client) {
        key := "ids"
        values := []interface{}{1, "中", 3, 4}
        err := client.RPush(ctx, key, values...).Err() //向List右侧插入（LPush左）。如果List不存在会先创建
        checkError(err)

        v2, err := client.LRange(ctx, key, 0, -1).Result() //截取 双闭区间 -1表示倒数第一个 -2表示倒数第二个。
        checkError(err)
        fmt.Println(v2)

        client.Del(ctx, key)
}

func hashtable(ctx context.Context, client *redis.Client) {
        //hash 适合结构体存储,比JSON序列化简单一点 
    //key  field1 value1  field2 value2  ...  
    //linux和Windows的命令行参数解析方式不同
    //nux直接用HSet  Windows要用HMSet  最好都用HMSet
        err := client.HSet(ctx, "学生1", "Name", "张三", "Age", 18, "Height", 173.5).Err()
        checkError(err)
        err = client.HSet(ctx, "学生2", "Name", "李四", "Age", 20, "Height", 180.0).Err()
        checkError(err)

        age, err := client.HGet(ctx, "学生2", "Age").Result()
        checkError(err)
        fmt.Println(age)


//1. 使用 `.Val()` 方法遍历哈希表的字段和值：
hashMap := client.HGetAll(ctx, "学生1").Val()
for field, value := range hashMap {
    fmt.Println(field, value)
}

//2. 使用 `.Result()` 方法遍历哈希表的字段和值：
result, err := client.HGetAll(ctx, "学生1").Result()
if err != nil {
    // 处理错误
    return err
}
for field, value := range result {
    fmt.Println(field, value)
}

        client.Del(ctx, "学生1")
        client.Del(ctx, "学生2")
}

func main() {
        client := redis.NewClient(&redis.Options{
                Addr:     "127.0.0.1:6379",
                Password: "", //没有密码
                DB:       0,  //redis默认会创建0-15号DB，这里使用默认的DB
        })
        ctx := context.TODO()
        string(ctx, client)
        list(ctx, client)
        hashtable(ctx, client)
}

func checkError(err error) {
        if err != nil {
                if err == redis.Nil {
                        fmt.Println("key不存在")
                } else {
                        fmt.Println(err)
                        os.Exit(1)
                }
        }
}
```

## 数据结构

### 1.string

### 2.list

### 3.hash

在Go语言中使用`go-redis`库来操作Redis的哈希（Hash）数据结构非常方便。以下是一个使用`go-redis`库来操作Redis哈希的示例代码。在这个示例中，我们将演示如何设置和获取用户信息的哈希表。

首先，确保你已经安装了`go-redis`库。你可以使用以下命令来安装该库：

```Bash
go get github.com/go-redis/redis/v8
```

接下来，我们来演示一个基本的使用`go-redis`库操作Redis哈希的示例：

```Go
var ctx = context.Background()

func main() {
    // 创建Redis客户端
    client := redis.NewClient(&redis.Options{
        Addr:     "localhost:6379", // Redis服务器地址
        Password: "",               // 密码
        DB:       0,                // 使用默认数据库
    })

    // 设置用户信息到哈希表

    // 将结构体转换为 map[string]interface{}
    //1. map[string]interface{}
    //2.一一对应，但不能设置过期时间
    //set-->Err()  get-->Result()
    err := client.HSet(ctx, "user:1", map[string]interface{}{
        "username": "Alice",
        "age":      28,
        "city":     "New York",
    }).Err()
    
      /*不能直接存结构体，转化一下
func structToMap(obj interface{}) (map[string]interface{}, error) {
    data, err := json.Marshal(obj)
    if err != nil {
       return nil, err
    }

    var result map[string]interface{}
    err = json.Unmarshal(data, &result)
    if err != nil {
       return nil, err
    }

    return result, nil
}
countMap, err := structToMap(userCount)
if err != nil {
    return err
}
err = r.data.rdb.HSet(ctx, key, countMap).Err()
if err != nil {
    return err
}   */

    if err != nil {
        fmt.Println("Error setting user information:", err)
        return
    }

    // 获取用户信息中的特定字段
    username, err := client.HGet(ctx, "user:1", "username").Result()
    if err != nil {
        fmt.Println("Error getting username:", err)
        return
    }
    fmt.Println("Username:", username)


    // 获取整个用户信息的哈希表
    user, err := client.HGetAll(ctx, "user:1").Result()
    if err != nil {
        fmt.Println("Error getting user information:", err)
        return
    }

    fmt.Println("User Information:")
    for key, value := range user {
        fmt.Printf("%s: %s\n", key, value)
    }
}
```

### 4.Zset

```Go
package main

import (
        "context"
        "fmt"
        "github.com/go-redis/redis/v8"
)

var ctx = context.Background()

func main() {
        rdb := redis.NewClient(&redis.Options{
                Addr:     "localhost:6379",
                Password: "",
                DB:       0,
        })

        _, err := rdb.Ping(ctx).Result()
        if err != nil {
                panic(err)
        }

        fmt.Println("Connected to Redis")

        key := "myzset"
        addElementToZset(rdb, key, 1, "user1")
        addElementToZset(rdb, key, 2, "user2")

        elements, _ := getElementsFromZset(rdb, key, 0, -1)
        for _, elem := range elements {
                fmt.Printf("Member: %s, Score: %f\n", elem.Member, elem.Score)
        }

        count, _ := getZsetCount(rdb, key)
        fmt.Printf("Zset count: %d\n", count)

        removeElementFromZset(rdb, key, "user1")
        elements, _ = getElementsFromZset(rdb, key, 0, -1)
        for _, elem := range elements {
                fmt.Printf("Member: %s, Score: %f\n", elem.Member, elem.Score)
        }

        count, _ = getZsetCount(rdb, key)
        fmt.Printf("Zset count: %d\n", count)
}

func addElementToZset(rdb *redis.Client, key string, score float64, member string) error {
        err := rdb.ZAdd(ctx, key, &redis.Z{
                Score:  score,
                Member: member,
        }).Err()
        return err
}

func getElementsFromZset(rdb *redis.Client, key string, start, stop int64) ([]redis.Z, error) {
        values, err := rdb.ZRangeWithScores(ctx, key, start, stop).Result()
        return values, err
}

func removeElementFromZset(rdb *redis.Client, key, member string) error {
        err := rdb.ZRem(ctx, key, member).Err()
        return err
}

func getZsetCount(rdb *redis.Client, key string) (int64, error) {
        count, err := rdb.ZCard(ctx, key).Result()
        return count, err
}
```

### 5.set

```go
多个成员一次添加时，SAdd 期望接收到变长参数（...interface{}），而不是一个切片。
var userIds []int64
//更新缓存
if len(userIds) != 0 {
	// 将 []int64 转换为 []interface{}
	userIdsInterface := make([]interface{}, len(userIds))
	for i, v := range userIds {
		userIdsInterface[i] = v
	}

	// 使用解包操作将 userIdsInterface 传递给 SAdd
	err := r.data.rdb.SAdd(context.Background(), "follow::"+strconv.Itoa(int(userId)), userIdsInterface...).Err()
	if err != nil {
		r.log.Errorf("SAdd err: %v", err)
		return nil, err
	}

	return userIds, nil
}
```



```Go
package main

import (
        "context"
        "fmt"
        "github.com/go-redis/redis/v8"
)

var ctx = context.Background()

// 初始化 Redis 客户端
func initRedisClient() *redis.Client {
        rdb := redis.NewClient(&redis.Options{
                Addr:     "localhost:6379",
                Password: "",
                DB:       0,
        })
        return rdb
}

// 添加成员到 Set
func addMembers(rdb *redis.Client, key string, members ...interface{}) error {
        err := rdb.SAdd(ctx, key, members...).Err()
        return err
}

// 获取 Set 的所有成员
func getMembers(rdb *redis.Client, key string) ([]string, error) {
        members, err := rdb.SMembers(ctx, key).Result()
        if err != nil {
                return nil, err
        }
        return members, nil
}

// 检查成员是否存在于 Set 中
func isMember(rdb *redis.Client, key string, member interface{}) (bool, error) {
        exists, err := rdb.SIsMember(ctx, key, member).Result()
        if err != nil {
                return false, err
        }
        return exists, nil
}

// 删除成员
func removeMembers(rdb *redis.Client, key string, members ...interface{}) error {
        err := rdb.SRem(ctx, key, members...).Err()
        return err
}

// 获取 Set 的成员数量
func getMemberCount(rdb *redis.Client, key string) (int64, error) {
        count, err := rdb.SCard(ctx, key).Result()
        if err != nil {
                return 0, err
        }
        return count, nil
}

func main() {
        rdb := initRedisClient()
        key := "myset"

        // 添加成员
        err := addMembers(rdb, key, "member1", "member2", "member3")
        if err != nil {
                fmt.Println("Error adding members:", err)
                return
        }
        fmt.Println("Members added to set.")

        // 获取所有成员
        members, err := getMembers(rdb, key)
        if err != nil {
                fmt.Println("Error getting members:", err)
                return
        }
        fmt.Println("Members:", members)

        // 检查成员是否存在
        isMember, err := isMember(rdb, key, "member1")
        if err != nil {
                fmt.Println("Error checking member:", err)
                return
        }
        fmt.Println("Is member1 in set?", isMember)

        // 删除成员
        err = removeMembers(rdb, key, "member1")
        if err != nil {
                fmt.Println("Error removing member:", err)
                return
        }
        fmt.Println("Member 'member1' removed from set.")

        // 获取成员数量
        count, err := getMemberCount(rdb, key)
        if err != nil {
                fmt.Println("Error getting member count:", err)
                return
        }
        fmt.Println("Member count:", count)
}
```

## 使用场景

1.连续签到 Key过期

2.消息通知 list做消息队列

3.用户计数 HASH 点赞量关注统计

4.排行榜

5.限流

6.分布式锁

- 掘金计数，使用到
- 排行榜ZSET
- 使用SETNX实现分布式锁

## 函数

### RedisClient.ExpireAt

用于设置 Redis 中的键的过期时间。具体来说，它允许你设置一个键在指定的时间点过期，一旦过了这个时间，Redis 会自动删除这个键。

以下是 `RedisClient.ExpireAt` 方法的基本用法和签名：

```Go
func (c *Client) ExpireAt(ctx context.Context, key string, tm time.Time) *StatusCmd
```

在这里：

- `c` 是 Redis 客户端实例。
- `ctx` 是 `context.Context`，用于处理超时和取消操作。
- `key` 是要设置过期时间的 Redis 键的名称。
- `tm` 是一个 `time.Time`，表示键应该在这个时间点之后过期。

示例代码中的用法类似于这样：

```Go
expAt := time.Now().Add(time.Hour * 24) // 设置为当前时间后的24小时
err := RedisClient.ExpireAt(ctx, key, expAt).Err()
if err != nil {
    panic(err)
}
```

这会将 `key` 设置为在当前时间后的24小时过期。如果在过期时间之后尝试访问这个键，它将不再存在。

### RedisClient.Incr

是 Redis 客户端库中的一个方法，用于对存储在 Redis 中的整数值执行自增操作。它会将指定键的整数值增加1，并返回增加后的新值。

如果指定的键不存在，它会自动创建一个键并将其初始值设置为 0，然后再执行自增操作。

以下是 `RedisClient.Incr` 方法的基本用法和签名：

```Go
func (c *Client) Incr(ctx context.Context, key string) *IntCmd
```

在这里：

- `c` 是 Redis 客户端实例。
- `ctx` 是 `context.Context`，用于处理超时和取消操作。
- `key` 是要执行自增操作的 Redis 键的名称。

示例代码中的用法类似于这样：

```Go
newCount, err := RedisClient.Incr(ctx, "user:123:count").Result()
if err != nil {
    panic(err)
}
fmt.Printf("New count: %d\n", newCount)
```

在这个示例中，`Incr` 方法对名为 "user:123:count" 的键执行自增操作，并返回自增后的新值。你可以将这个方法用于计数器、统计等场景，非常适用于实现类似用户签到天数、点赞数等功能。

注意，在使用 `Incr` 方法时，要确保连接到 Redis 服务器，处理可能的错误，并根据你的业务需求适当地使用返回的新值。

### 管道

RedisClient.Pipeline()是 Redis 客户端库中的一个方法，用于创建一个 Redis 操作的批处理管道。批处理管道允许你在一次网络往返中发送多个命令，从而提高了操作的效率，特别是在需要执行多个操作时。

批处理管道通过将多个操作排队并一次性发送给 Redis 服务器来减少网络延迟。这在需要执行多个 Redis 命令并从结果中获取数据时非常有用，因为它可以减少每个操作的往返时间。

以下是 `RedisClient.Pipeline()` 方法的基本用法和签名：

```Go
func (c *Client) Pipeline() *Pipeline
```

在这里：

- `c` 是 Redis 客户端实例。

示例代码中的用法类似于这样：

```Go
pipe := RedisClient.Pipeline()

// 在管道中添加多个操作
pipe.Incr(ctx, "counter")
pipe.Get(ctx, "some_key")
pipe.ZAdd(ctx, "sorted_set", &redis.Z{Score: 1, Member: "one"})

// 执行管道中的操作并获取结果
results, err := pipe.Exec(ctx)
if err != nil {
    panic(err)
}

// 处理结果
for _, result := range results {
    // 处理 result
}

// 记得关闭管道
pipe.Close()
```

在这个示例中，你首先创建了一个管道 `pipe`，然后在管道中添加了多个操作（`Incr`、`Get` 和 `ZAdd`）。最后，通过调用 `pipe.Exec` 方法执行管道中的操作，并获取每个操作的结果。

注意，在使用管道时，要确保你了解每个操作的执行顺序、结果的顺序以及如何处理错误。在处理完操作后，你应该使用 `pipe.Close()` 方法来关闭管道。

使用管道可以显著提高 Redis 操作的性能，特别是在需要批量执行操作时。然而，在使用管道时，也需要注意合理地处理错误和管理结果。

Redis 实际上同时支持事务和管道这两种概念，但它们之间是不同的机制，用途也不同。让我为您澄清一下两者的区别：

1. **Redis 事务：** Redis 事务是一种用于执行多个命令的机制，它提供了事务的 ACID 特性，即要么全部成功执行，要么全部不执行。Redis 事务由 `MULTI`、`EXEC`、`WATCH` 和 `DISCARD` 命令组成。`MULTI` 标记事务开始，然后可以添加多个命令到事务队列中，最后通过 `EXEC` 命令来执行事务。`WATCH` 用于监视一个或多个键，如果被监视的键发生变化，事务将被取消，而 `DISCARD` 用于取消事务。
2. **Redis 管道：** Redis 管道（pipeline）是一种将多个命令一次性发送到服务器并一次性接收响应的机制，它主要用于减少网络往返次数，从而提高性能。在管道中，可以一次性发送多个命令，然后在一次性接收多个响应。每个命令的响应是独立的，不受其他命令的影响，而且没有事务的 ACID 特性。

总结起来，Redis 既支持事务也支持管道。事务用于保证多个命令的原子性和 ACID 特性，适用于需要事务性保证的场景。而管道主要用于减少网络开销和提高性能，适用于需要批量执行多个命令的场景。根据您的应用需求，可以选择适合的机制。

### SISMEMBER

是 Redis 的一个集合操作命令，用于判断某个成员是否存在于集合中。在 Go 语言中，通常会使用 Redis 客户端库来与 Redis 服务器交互。根据你提到的 redisClient.SIsMember，我可以猜测它是使用 Redis 客户端库提供的方法来执行 `SISMEMBER` 命令。

在常见的 Go Redis 客户端库（例如 "github.com/go-redis/redis"）中，`SISMEMBER` 命令的对应方法可能会是 `SIsMember` 或者类似的函数名。

以下是一个使用 Go Redis 客户端库的示例代码，演示了如何使用 `SISMEMBER` 命令来判断某个成员是否存在于 Redis 集合中：

```Go
package main

import (
    "fmt"
    "github.com/go-redis/redis/v8"
    "context"
)

func main() {
    // 创建 Redis 客户端连接
    rdb := redis.NewClient(&redis.Options{
        Addr:     "localhost:6379", // Redis 服务器地址
        Password: "",               // 密码
        DB:       0,                // 使用默认的数据库
    })

    // 创建一个上下文
    ctx := context.Background()

    // 集合名
    setKey := "myset"
    
    // 要判断的成员
    member := "value1"

    // 使用 SISMEMBER 命令判断成员是否在集合中
    result, err := rdb.SIsMember(ctx, setKey, member).Result()
    if err != nil {
        fmt.Println("Error:", err)
        return
    }

    // 打印结果
    if result {
        fmt.Printf("成员 %s 存在于集合 %s 中\n", member, setKey)
    } else {
        fmt.Printf("成员 %s 不存在于集合 %s 中\n", member, setKey)
    }
}
```

请注意，示例代码中使用了 "github.com/go-redis/redis/v8" 这个库。你需要根据你实际使用的 Redis 客户端库来调用相应的方法。当然，不同的 Redis 客户端库可能会有略微不同的 API 设计，但基本思想是相通的：使用合适的方法来执行 `SISMEMBER` 命令，判断某个成员是否存在于指定的集合中。

## 错误处理

没有查询到 或者 数据为空/默认值，两种情况都要注意

```Go
     
redsiData, err := common.RedisA.Get(c, fmt.Sprintf("isLike:%d:%d", videoId, userId)).Result()
        //查询为空：err == redis.Nil     数据为空：len(redsiData) == 0
  		//string 为 ""   ZCard/count 为 0
        if errors.Is(err, redis.Nil) || len(redsiData) == 0 {
            return
        } else if err != nil {
                return 
    }
    return
```



在Redis中查询数据时，如果查询的键不存在，Redis通常会返回特殊的`nil`值，这用来表示找不到相应的键。而在查询集合、哈希表、列表等数据结构的元素时，如果特定元素不存在，也可能会返回`nil`。但是，当查询集合的大小、哈希表的字段或列表的长度时，即使对应的键不存在或集合为空，Redis通常会返回`0`作为默认值，而不是`nil`。

区别总结：

- **`nil`**: 通常表示键不存在，或者在一个集合、哈希表、列表中查询的特定元素不存在。
- **`0`**: 通常表示集合、哈希表、列表等数据结构为空，或者是查询的某个指标（如大小、长度）为零。

示例：

1. **键不存在时的查询**：
   - `GET mykey`: 如果`mykey`不存在，Redis将返回`nil`。

2. **查询集合大小**：
   - `ZCARD myzset`: 如果`myzset`不存在或是一个空的有序集合，Redis将返回`0`。

3. **查询哈希表字段**：
   - `HGET myhash field`: 如果`field`在`myhash`中不存在，Redis将返回`nil`。
   - `HLEN myhash`: 如果`myhash`不存在或是一个空的哈希表，Redis将返回`0`。

4. **查询列表长度**：
   - `LLEN mylist`: 如果`mylist`不存在或是一个空列表，Redis将返回`0`。

通过上述区别，你可以根据返回值判断是键不存在，还是数据结构为空。这在编写Redis客户端代码时非常重要，因为它可以帮助你区分不同的情况并做出适当的响应。例如，如果`GET`命令返回`nil`，你可能需要处理键不存在的情况；而如果`ZCARD`返回`0`，则表示有序集合为空，可能需要填充数据。



## 后台服务

要将Redis服务器切换为后台服务，您可以按照以下步骤进行操作：

1. 打开命令提示符：按下Windows键 + R，输入"cmd"并按下Enter键，将打开命令提示符窗口。
2. 导航到Redis安装目录：在命令提示符中，使用`cd`命令导航到Redis的安装目录。例如：

```Plaintext
mathematicaCopy code
cd D:\Environment\Redis-x64-3.0.504
```

1. 这将使命令提示符处于Redis安装目录下。
2. 启动Redis服务器作为后台服务：执行以下命令启动Redis服务器作为后台服务：

```Plaintext
cssCopy code
redis-server --service-install redis.windows.conf --service-name RedisService
```

1. 这将使用redis.windows.conf配置文件并将服务名称设置为"RedisService"来安装Redis服务器作为后台服务。
2. 启动Redis服务：执行以下命令来启动Redis服务：

```Plaintext
cssCopy code
redis-server --service-start --service-name RedisService
```

1. 这将启动Redis服务并将其设置为在后台运行。

现在，Redis服务器已被设置为后台服务，并且会在系统启动时自动启动。您可以通过执行`redis-cli`命令连接到Redis服务器，或通过在服务列表中启动/停止RedisService服务来管理Redis服务器。

1. 打开命令行终端。
2. 运行 `redis-cli` 命令进入 Redis 命令行界面。
3. 在 Redis 命令行界面中，输入 `shutdown` 命令，并按下回车键。
4. Redis 会执行关闭操作并退出后台程序。

请注意，这种方法要求你已经安装并正确配置了 Redis，并且能够访问 Redis 服务器。如果你无法使用命令行终端或无法访问 Redis 服务器，可以考虑使用其他方法来停止 Redis 后台程序，例如通过操作系统的任务管理器或使用 Redis 的管理工具。

另外，如果你是通过在后台运行 Redis 的方式启动的，可以使用以下命令来停止 Redis 后台程序：

# 缓存和数据库的一致性

强一致性，加锁，得不偿失

最终一致性

500ms——》代表主从同步的时间

## 读

先从redis读，没有去mysql去取，读取到的数据写入redis+过期时间，否则，从redis返回 

## 写

一般来说缓存都是直接删除或插入，不要进行更新

删除一个数据，相比更新一个数据更加轻量级，出问题的概率更小。

 从另外一个角度，不是所有的缓存数据都是频繁访问的，更新后的缓存可能会长时间不被访问，所以说，从计算资源和整体性能的考虑，更新的时候删除缓存，等到下次查询命中再填充缓存，是一个更好的方案。



### 先操作缓存：

#### 一、延迟双删：

1.先删除缓存。 2.写数据库。 3.休眠500毫秒，再删除缓存。

**优点：**

- 简单易实现。
- 对数据库的写操作有较好的保障，确保数据最终一致性。

**缺点：**

- 延迟双删中的500毫秒的休眠时间需要精心调节，太短可能导致脏数据读取，太长则影响系统性能。
- 适用于**读多写少**的场景，且对读数据的一致性要求不高。

**休眠时间的确定：**

- 休眠时间应该大于线程二从数据库读取数据写入缓存的时间。
- 需要通过实际测试和监控，评估业务逻辑的平均耗时，选择一个相对安全的值。





#### 二、定时任务

500ms——》代表主从同步的时间

1. 先写Redis（带过期时间）。
2. 定时任务异步存MySQL，类似于标记脏数据。
3. MySQL  删除/回写  Redis（通过阿里巴巴的Canal）。

**优点：**

- 写操作性能较好，因为Redis写入速度快。
- MySQL与Redis的数据同步通过定时任务进行，可以减轻数据库压力。

**缺点：**

- 存在一定的延迟，数据最终一致性依赖于定时任务的执行频率。
- 实现较为复杂，需要处理定时任务和数据同步逻辑。

**适用场景：**

- **写多读少**的场景。
- 对数据实时一致性要求不高，可以容忍一定的延迟。





### 先操作数据库

#### 三、flinkcdc

1. 更新mysql 

   ​	如果要保证实时性，直接 -----》删除redis / 更新（redis+过期时间） 

3. 然后 canal/flink cdc 监听 mysql，删除/回写对应redis     

flinkcdc 监听  mysql到kafka， 消费kafka删除对应redis

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202407111547304.png" alt="202407090219327" style="zoom: 67%;" />

**优点：**

- 实时性好，能够快速同步数据。
- 通过Flink CDC可以处理复杂的数据同步和流处理逻辑。

**缺点：**

- 实现复杂度高，需要熟悉Flink CDC和Canal的使用。
- 依赖外部工具，增加了系统的复杂度和维护成本。

**适用场景：**

- 对数据实时一致性要求高的场景。
- 需要处理复杂数据流的场景。

删除缓存失败，重试机制 ： 直接将失败删除发送到mq，接收方从mq接收后重试删除操作 （但这样对业务代码侵入过大）

 优化是使用canal（可以理解为一个从节点）读取binlog异步删除，当Canal监听到binlog变化时，会通知Canal的客户端延迟删除。



#### 四、消息队列

使用消息队列异步重试缓存的方案，可以有以下几个步骤：

- 当用户信息发生变化时，先更新数据库，并返回成功结果给前端。
- 尝试去删除缓存，成功则结束操作，失败则将要删除或更新缓存的操作生成一个消息（比如包含key和操作类型），并发送到消息队列中（比如使用Kafka或RabbitMQ）
- 另外有一个消费者线程从消息队列中订阅并获取这些消息，并根据消息内容删除或更新Redis中的对应信息。
- 如果删除或更新缓存成功，则把这个消息从消息队列中移除（丢弃），以免重复操作。
- 如果删除或更新缓存失败，则执行失败策略，比如设置一个延迟时间或者一个重试次数限制
- 如果重试超过一定次数仍然失败，则向业务层发送报错信息，并记录日志。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409022058840.png" alt="a4440f0d572612e0832b903e4a62bd2b" style="zoom:67%;" />



# Flink CDC

![image-20240925210544089](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409252105393.png)

**FlinkCDC 不仅支持增量同步，还支持全量/全量+增量的同步，同时 FlinkCDC 还支持故障恢复（基于检查点机制实现），能够快速恢复数据同步的进度**

Flink CDC（Change Data Capture）是 Apache Flink 社区中的一个扩展，用于实时捕获和处理数据库中的数据变更。Flink CDC 可以将数据库的变更事件（例如插入、更新、删除操作）流式地传递到 Flink 作业中，使得 Flink 可以在这些变更发生时立即对数据进行处理和分析。

### Flink CDC 的关键功能

1. **实时数据捕获**：Flink CDC 可以实时捕获数据库表中的数据变更，并将这些变更流式地传递到 Flink 的数据流中。
2. **支持多种数据库**：Flink CDC 支持多种主流数据库，如 MySQL、PostgreSQL、Oracle 等。
3. **无缝集成**：Flink CDC 可以无缝集成到 Flink 作业中，使得开发人员能够轻松地构建实时数据处理管道。
4. **高可用性和容错性**：利用 Flink 强大的分布式计算能力，Flink CDC 提供了高可用性和容错性，确保数据处理的可靠性。

### 工作原理

Flink CDC 利用数据库的变更数据捕获（CDC）技术来捕获数据的变更。其基本工作流程如下：

1. **监听数据库变更**：Flink CDC 通过连接数据库的二进制日志（如 MySQL 的 binlog）或其他变更日志，监听数据库表中的数据变更。
2. **解析变更事件**：捕获到的数据变更事件会被解析为相应的事件流，如插入事件、更新事件和删除事件。
3. **数据流处理**：这些事件流会作为 Flink 数据流中的源（Source）输入，开发者可以在 Flink 中应用各种算子（Operators）来处理这些变更数据，例如过滤、聚合、联接等操作。
4. **输出到目标存储**：处理后的数据可以实时输出到各种存储系统或下游系统中，如 Kafka、ElasticSearch、HDFS 等。

### 典型应用场景

1. **实时数据同步**：将数据库中的变更实时同步到数据仓库或搜索引擎中，确保数据的一致性。
2. **实时分析**：通过捕获数据库变更事件，可以在数据变更时立即触发实时分析或告警。
3. **增量ETL**：构建增量ETL管道，将数据库中的增量数据实时提取、转换，并加载到目标系统中。

### 相关组件

- **Flink SQL**：Flink CDC 可以结合 Flink SQL 使用，用户可以通过 SQL 查询来处理和分析实时变更数据。
- **Debezium**：Flink CDC 基于 Debezium 进行开发，Debezium 是一个开源的 CDC 平台，支持多种数据库的变更捕获。



Flink CDC（Change Data Capture）是一种捕获数据库变更的技术，它可以实时监控数据库的变化（增删改），并将这些变化流式地传递到数据处理系统中。Flink CDC 基于 Apache Flink 实现，并依赖 Debezium 这样的 CDC 引擎来捕获数据库的变更事件。它的底层原理涵盖了如何捕获数据变更、如何将变更数据转换为流、以及如何与 Flink 进行无缝集成。以下是 Flink CDC 的底层原理的详细介绍。

### 1. **数据变更捕获 (Change Data Capture)**

Flink CDC 的核心任务是从数据库中捕获数据变更。它基于 **binlog** 或 **WAL**（Write-Ahead Logging）等数据库日志机制，通过解析这些日志文件来跟踪数据库的每一次变更事件。

- **数据库日志机制**：大多数数据库都采用了日志机制来记录数据变更操作。以 MySQL 为例，所有的增删改操作都会记录在 binlog（Binary Log）中。这些日志不仅仅用于备份和恢复，也可以用来实现变更数据捕获。其他数据库如 PostgreSQL 使用 WAL 日志机制。
  
- **Debezium 作为底层引擎**：Flink CDC 底层使用了 **Debezium**，这是一个开源的 CDC 引擎，能够从多个数据库中捕获数据变更。Debezium 通过连接到数据库并监听日志文件来捕获变更事件，之后将这些事件发送给 Flink 进行流处理。

### 2. **日志解析与事件生成**
Flink CDC 通过 Debezium 从数据库中读取日志，并将这些日志解析为结构化的事件数据。具体步骤如下：

- **日志读取**：Debezium 通过数据库提供的接口（如 MySQL 的 binlog 或 PostgreSQL 的 WAL）读取数据变更日志。
  
- **解析日志**：Debezium 会解析这些日志条目，将其转化为可读的变更事件，包括操作类型（插入、更新、删除）、表名、字段名、旧值和新值等关键信息。

- **生成变更事件**：解析完成后，Debezium 会生成一系列变更事件（event），每个事件代表数据库中一次具体的操作。例如，插入操作会生成一个"INSERT"事件，更新操作会生成一个"UPDATE"事件。这些事件会被封装为 Kafka 消息或通过其他中间件传递给 Flink。

### 3. **Flink 与 CDC 事件的集成**
在 Flink 中，Flink CDC 连接器负责从 Debezium 捕获的事件流中提取数据，并将其转换为 Flink 的数据流（DataStream），以便 Flink 可以对其进行进一步的处理。

- **Flink Source**：Flink CDC 提供了专门的 `Flink CDC Source`，该源连接器可以直接与 Debezium 集成，监听从数据库生成的 CDC 事件。Flink CDC Source 会将数据库的变更事件转换为 Flink DataStream。
  
- **增量数据处理**：Flink 可以将捕获到的数据变更事件作为增量数据流进行处理。通过 Flink 的流式处理框架，用户可以实时处理这些增量数据，实现数据清洗、转换、聚合等复杂任务。

### 4. **Flink 状态一致性保证**
在处理流数据时，数据一致性是非常重要的。Flink CDC 保证数据的准确性和一致性，特别是在分布式和容错场景下。

- **状态管理与一致性**：Flink 通过分布式状态管理系统来维护状态，并通过**两阶段提交协议（2PC, Two-phase Commit Protocol）**确保数据流处理的一致性。即使在处理过程中发生故障，Flink 也能够恢复数据处理流程，并且保证不会丢失或者重复处理数据。

- **Checkpoints 和恢复**：Flink 定期对流式作业进行 checkpoint（检查点）操作，记录数据处理的中间状态。当 Flink 作业失败时，可以通过恢复到上一个成功的 checkpoint 来恢复数据流处理。同时，Flink CDC 也会跟踪数据库日志的位置（offset），以确保在作业恢复后能够从正确的日志位置继续处理变更数据。

### 5. **低延迟和高吞吐量**
Flink CDC 的设计目标是提供低延迟和高吞吐量的数据处理能力。Flink 本身是一个流处理引擎，能够处理海量的数据流，而 CDC 技术使其能够实时获取数据库的增量变更，从而实现低延迟的流处理。

- **低延迟处理**：通过实时监听数据库变更日志，Flink CDC 能够将数据变化迅速地捕获并送入 Flink 流中，整个流程具有非常低的延迟，能够做到准实时的数据同步。
  
- **高吞吐量处理**：由于 Flink 天生的分布式架构，Flink CDC 可以处理大规模的变更数据流，同时确保系统在高并发和大数据量的情况下仍然保持稳定。

### 6. **Schema Evolution（模式演进）**
数据库表的模式可能会随着业务的发展不断演进，而 Flink CDC 需要能够处理这种模式的变化。

- **模式变化捕获**：Flink CDC 可以监控数据库表的结构变化，例如列的增加、删除、修改等。当发生模式变化时，Flink CDC 会捕获这些变化并将其反映在 Flink 数据流中。
  
- **动态解析与兼容性**：当数据库表结构发生变化时，Flink CDC 会动态解析新的表结构，并保证历史数据和新数据能够被一致处理，确保在整个数据流处理中的兼容性。

### 7. **分布式容错与扩展性**
Flink CDC 的分布式架构使其能够在容错性和扩展性方面表现出色。

- **分布式处理**：Flink CDC 可以在分布式环境下运行，将数据库的变更数据分发到多个 Flink 节点中进行处理，能够应对高负载的应用场景。
  
- **容错机制**：Flink CDC 具有容错能力，能够在节点故障时自动恢复数据处理流程。通过 Flink 的状态恢复和 checkpoint 机制，Flink CDC 可以在故障发生后保证数据处理的准确性和一致性。

- 

### 8. **Flink CDC 的应用场景**
Flink CDC 可以应用于多种场景，常见的包括：

- **实时数据同步**：可以将源数据库的变化实时同步到其他数据存储系统（如 Elasticsearch、Hadoop 等），保持数据的一致性。
  
- **实时分析与监控**：捕获数据变更后，Flink 可以立即处理这些增量数据，实时更新统计分析结果，常用于实时监控、流式 BI 等场景。
  
- **数据湖建设**：将数据库的变更数据直接同步到数据湖中，结合 Flink 实现统一的流批处理架构，构建实时数据仓库。

### 总结
Flink CDC 的底层原理依赖于数据库日志的捕获和解析，通过 Debezium 等工具获取数据库的变更日志，并将这些变更转化为 Flink 流处理框架中的事件数据。借助 Flink 的流式处理能力，Flink CDC 实现了数据的实时增量同步、低延迟处理和强一致性保证。它在处理大规模数据变更时，能够有效应对复杂的流式数据处理场景，并确保系统的高可用性和容错性。

## 命令

```SQL
mysql开启binlog
--log-bin=mysql-bin
--binlog-format=ROW
--server-id=1

./bin/start-cluster.sh
./bin/stop-cluster.sh

export FLINK_HOME=/home/lanmengyou/.code/go_code/TikTok/flink/flink-1.18.0


bin/flink-cdc.sh job/user-mysql-to-kafka.yaml
bin/flink-cdc.sh job/video-mysql-to-kafka.yaml
bin/flink-cdc.sh job/favorite-mysql-to-kafka.yaml
bin/flink-cdc.sh job/comment-mysql-to-kafka.yaml
bin/flink-cdc.sh job/relation-mysql-to-kafka.yaml
bin/flink-cdc.sh job/message-mysql-to-kafka.yaml

docker exec -it kafka  kafka-console-consumer.sh --topic flink-user2 --bootstrap-server localhost:9092 --from-beginning
```

# ES





索引

切片

文档



Elasticsearch（通常简称 ES）是一个开源的分布式全文搜索和数据分析引擎，构建在 Apache Lucene 的基础上。它不仅支持对结构化和非结构化数据进行存储和索引，而且提供了 RESTful API，使得数据的存储、检索和实时分析变得十分便捷。下面是对 ES 的一些核心介绍：

- **核心功能**：
   ES 能够快速、近实时地存储和搜索海量数据。它通过建立倒排索引，实现对文本数据的高效全文检索，同时也支持结构化查询、地理位置查询以及复杂的聚合分析，广泛应用于日志分析、网站搜索、商业智能等场景。
   citeturn1search0
- **分布式架构**：
   一个 ES 集群由一个或多个节点（Node）组成，每个节点可以存储数据并参与索引和查询操作。数据在索引时会被分为多个分片（Shard），每个分片又可以有多个副本（Replica），这不仅提高了数据存储容量，也确保了高可用性和故障转移能力。
   citeturn1search2
- **近实时性**：
   数据写入 ES 后，不是立即可被查询，而是经过短暂的延迟（通常约1秒）后，数据才刷新到可搜索状态。这种设计既保证了写入效率，也使得系统可以几乎实时地响应查询请求。
   citeturn1search9
- **使用简单**：
   ES 使用 JSON 作为数据存储格式，通过简单的 RESTful API 进行操作，无论是数据的增删改查还是复杂的搜索和聚合，都可以通过 HTTP 请求轻松实现，这使得 ES 非常适合与各种编程语言和应用程序集成。
   citeturn1search8

总之，Elasticsearch 既是一个高性能的搜索引擎，也是一个功能丰富的数据分析平台，非常适合用于需要快速、灵活检索和实时分析海量数据的场景。

## 优势

Elasticsearch 的快速性能主要得益于以下几个方面：

1. **倒排索引**：它使用倒排索引结构，使得文本搜索非常高效。
2. **分布式架构**：可以将数据分散到多个节点上，支持水平扩展。
3. **内存使用**：通过将索引/部分数据存储在内存中，并利用缓存技术，提高了查询速度。
4. **并发处理**：支持高并发请求，可以同时处理多个搜索请求。
5. **优化的查询语言**：Elasticsearch 提供了 DSL 查询语言，使得查询更高效。

## 倒排索引

![v2-b601cbe28ef7c822b393451cf2347e9c](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409252219808.webp)

倒排索引（Inverted Index）是搜索引擎核心的数据结构，其基本思想是将文档中出现的每个词条映射到包含该词条的文档列表上，而不是像传统数据库那样记录每个文档包含哪些词条。通过这种方式，当用户输入查询关键词时，系统可以迅速定位到所有包含该关键词的文档，从而实现高效的全文搜索。

例如，假设有以下几篇文档：

- Doc1："苹果手机非常好用"
- Doc2："手机质量不错"
- Doc3："苹果公司的手机设计独特"

构建倒排索引后，会生成如下映射：

- “苹果”：Doc1, Doc3
- “手机”：Doc1, Doc2, Doc3
- “非常好用”：Doc1
- “质量不错”：Doc2
- “公司的”：Doc3
- “设计独特”：Doc3

此外，倒排索引不仅记录文档ID，还可能包含词频（TF）和词在文档中的位置等信息，这有助于进行相关性打分和精确搜索（比如短语搜索）【citeturn1search0】【citeturn1search7】。

总的来说，倒排索引使得搜索引擎能够在海量文档中快速找出与查询词匹配的文档，极大提高了搜索效率，这也是 Elasticsearch 能够实现近实时全文检索的关键所在。

