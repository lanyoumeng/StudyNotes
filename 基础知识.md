基础知识



# 计算机网络



1.网络：节点和边构成 和大小和形状无关 的布

2.计算机网络： 联网的计算机所构成的系统

- 主机节点（方形）、数据交换节点（圆）----》 交换机、路由器等
- 链路 接入网链路（连接方）、主干链路（连接圆）
- 协议



## 网络模型



OSI 网络模型：

  七层和四层负载均衡，是用 OSI 网络模型来描述的，七层对应的是应用层，四层对应的是传输层

- 应用层，负责给应用程序提供统一的接口；
- 表示层，负责把数据转换成兼容另一个系统能识别的格式；比如字符编码与转换，以及数据加密。
- 会话层，负责建立、管理和终止表示层实体之间的通信会话；
- 传输层，负责端到端的数据传输；
- 网络层，负责数据的路由、转发、分片；
- 数据链路层，负责数据的封帧和差错检测，以及 MAC 寻址；
- 物理层，负责在物理网络中传输数据帧；



TCP/IP 网络模型，Linux 系统正是按照这套网络模型来实现网络协议栈的。

- 应用层，负责向用户提供一组应用程序，比如 HTTP、DNS、FTP 等;
- 传输层，负责端到端的通信，比如 TCP、UDP 等；
- 网络层，负责网络包的封装、分片、路由、转发，比如 IP、ICMP 等；
- 网络接口层，负责网络包在物理网络中的传输，比如网络包的封帧、 MAC 寻址、差错检测，以及通过网卡传输网络帧等



## Linux通信

### 本机通信

对于同一台设备上的进程间通信，有很多种方式，比如有管道、消息队列、共享内存、信号等方式



### Linux 网络协议栈



<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408091215470.png" alt="_E5_8D_8F_E8_AE_AE_E6_A0_88" style="zoom: 50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408091219456.png" alt="_E6_94_B6_E5_8F_91_E6_B5_81_E7_A8_8B" style="zoom: 33%;" />

#### 接收和处理网络包

1. 网卡接收网络包

- **硬件部分**：
  - 当网卡接收到一个网络包时，它通过**DMA（Direct Memory Access）**技术，将这个网络包直接写入到内存中的**Ring Buffer（环形缓冲区）**中。DMA允许网卡直接将数据传输到内存，而不需要CPU的介入，这大大提高了效率。
  - 一旦网络包写入到Ring Buffer中，网卡会触发一个硬件中断，通知CPU有新的数据包到达。

2. 中断处理

- **硬件中断处理**：
  - 当CPU接收到来自网卡的中断时，操作系统内核会根据中断向量表调用相应的**中断处理函数**。
  - 在中断处理函数中，首先会**屏蔽后续的同类中断**，以防止CPU被频繁中断影响系统性能。
  - 然后，中断处理函数会启动一个**软中断**，用于进一步处理数据包。
  - 处理完成后，恢复之前屏蔽的中断，并退出中断处理函数。

3. 软中断处理

- **NAPI机制**：
  - Linux内核采用**NAPI（New API）**机制来平衡中断和轮询之间的性能。NAPI的核心思想是，当网络流量低时，使用中断处理数据；当流量高时，转而使用轮询，以减少中断频率，提高系统效率。
  - **ksoftirqd线程**专门用于处理软中断。当软中断触发后，ksoftirqd线程会开始执行软中断处理函数。

- **数据包提取**：
  - ksoftirqd线程从Ring Buffer中读取网络包，并将其封装为**sk_buff**结构，表示一个网络包的数据。
  - sk_buff结构包含了网络包的所有必要信息，供后续的网络协议栈处理。

4. 网络协议栈处理

- **网络接口层**：
  - 网络包首先被传递到网络接口层（如以太网层）。在这一层，操作系统会检查包的合法性（如校验和检测）。如果包不合法，则会被丢弃。
  - 如果合法，接口层会提取网络包的帧头信息，识别上层协议类型（如IPv4、IPv6），并去掉帧头，将包传递给网络层。

- **网络层**：
  - 在网络层，操作系统会处理IP层的数据包。这里将IP头去除，并判断下一步的处理动作。
  - 如果数据包是发送给本地的，网络层会将其传递给上层协议（传输层）；如果是转发的包，则会通过路由机制进行转发。

- **传输层**：
  - 传输层处理传输协议（如TCP、UDP）。操作系统根据包头的四元组（源IP、源端口、目的IP、目的端口）找到对应的Socket，并将数据放入对应Socket的接收缓冲区。
  - 在TCP的情况下，还会进行重组和顺序处理，以确保数据的完整性和正确性。

5. 用户空间处理

- **应用程序读取数据**：
  - 最后，应用程序通过系统调用（如`recv`、`read`）从Socket的接收缓冲区中读取数据。
  - 这一步通常涉及将数据从内核空间拷贝到用户空间，之后应用程序便可以处理接收到的数据。

总结

这个流程展示了Linux系统中网络包从网卡接收到最终被用户空间的应用程序处理的完整过程。通过DMA、硬件中断、NAPI、软中断等机制的结合，Linux系统能够高效地处理网络数据，尤其在高流量场景下，能有效降低中断带来的性能开销。



#### 发送网络包

1. 应用层发起网络包发送请求

- **系统调用**：
  - 应用程序通过Socket API（如`send()`、`write()`）发起发送数据的请求。这是一个系统调用，导致程序从用户态陷入内核态。

2. 数据拷贝到内核缓冲区

- **sk_buff的创建**：
  - 内核为待发送的数据分配一个`sk_buff`结构体，这是一个内核中的数据结构，用于描述网络数据包。`sk_buff`不仅包含实际的数据，还包含包的元数据和控制信息。
  - 应用程序的数据被拷贝到`sk_buff`的缓冲区中，并将其加入到Socket的发送缓冲区。

3. 传输层处理

- **TCP层**：
  - 如果使用TCP协议，内核首先对数据进行TCP封装，添加TCP头。
  - 由于TCP支持可靠传输（丢包重传），`sk_buff`通常会被克隆，克隆的副本将被用于后续的网络层处理，而**原始的`sk_buff`会保留在传输层中，直到收到对方的ACK确认。**

4. 网络层处理

- **IP层**：
  - 数据包被传递到网络层，在这里添加IP头，并选择路由（即确定数据包的下一跳）。
  - 如果数据包的大小超过MTU（最大传输单元），IP层会对数据包进行分片，每个片段都将被封装为一个单独的`sk_buff`。

5. 网络接口层处理

- **数据链路层**：
  - 数据包进一步被传递到网络接口层，添加数据链路层的帧头和帧尾（如以太网头部和尾部）。
  - 如果目标IP地址是通过ARP（地址解析协议）获取的，会在此处添加MAC地址。

6. 触发软中断和发送数据包

- **软中断和网卡驱动**：
  - 数据包处理完成后，内核会触发软中断，通知网卡驱动程序有数据包需要发送。
  - 网卡驱动从发送队列中读取`sk_buff`，并将其映射到网卡可以访问的DMA内存区域中。网卡将此数据包放入其发送队列中。

7. 数据包发送和内存清理

- **数据发送**：
  - 网卡通过物理链路将数据包发送出去。当发送完成后，网卡会触发一个硬中断，通知CPU数据包已成功发送。

- **内存清理**：
  - 硬中断处理程序负责清理发送过程中使用的内存，包括释放`sk_buff`结构体和清理Ring Buffer内存。
  - 如果使用TCP协议，当传输层收到对方的ACK确认后，保存在传输层中的原始`sk_buff`才会被释放。

8. 内存拷贝操作的次数

在整个发送过程，涉及三次主要的内存拷贝操作：

1. **用户态到内核态的拷贝**：
   - 当应用程序调用发送系统调用时，数据从用户态缓冲区拷贝到内核态的`sk_buff`缓冲区。

2. **传输层的`sk_buff`克隆**：
   - 使用TCP协议时，为了实现可靠传输，内核会克隆`sk_buff`，原始`sk_buff`保留在传输层中，克隆的副本将被传递到网络层进行处理。

3. **IP层的分片处理**：
   - 当数据包大小超过MTU时，IP层会对数据包进行分片，并为每个片段申请新的`sk_buff`，从而进行额外的内存拷贝。

总结

Linux系统中发送网络包的过程是一个自上而下的过程，涉及用户态和内核态的数据传递，以及各层协议的逐步封装处理。通过合理的内存管理和中断机制，Linux能够高效地处理并发送网络包，尤其在高负载的网络环境中。



#### sk_buff 

可以表示各个层的数据包，在应用层数据包叫 data，在 TCP 层我们称为 segment，在 IP 层我们叫 packet，在数据链路层称为 frame。

你可能会好奇，为什么全部数据包只用一个结构体来描述呢？协议栈采用的是分层结构，上层向下层传递数据时需要增加包头，下层向上层数据时又需要去掉包头，如果每一层都用一个结构体，那在层之间传递数据的时候，就要发生多次拷贝，这将大大降低 CPU 效率。

于是，为了在层级之间传递数据时，不发生拷贝，只用 sk_buff 一个结构体来描述所有的网络包，那它是如何做到的呢？是通过调整 sk_buff 中 `data` 的指针，比如：

- 当接收报文时，从网卡驱动开始，通过协议栈层层往上传送数据报，通过增加 skb->data 的值，来逐步剥离协议首部。
- 当要发送报文时，创建 sk_buff 结构体，数据缓存区的头部预留足够的空间，用来填充各层首部，在经过各下层协议时，通过减少 skb->data 的值来增加协议首部。

![sk_buff](D:\DownLoad\sk_buff.jpg)



### 检验数据完整性的方法

确保数据完整性是关键，特别是在存储和传输数据时。有几种方法可以用来检验数据的完整性：

1. 校验和（Checksums）

校验和是通过对数据块使用特定算法生成的固定长度值。当数据传输完成后，接收方可以使用相同的算法重新计算校验和，并将其与发送方提供的校验和进行比较，以验证数据是否完整。

2. 哈希函数

哈希函数将数据块转换为固定长度的唯一哈希值。常见的哈希函数有MD5、SHA-1、SHA-256等。发送方和接收方都可以使用相同的哈希函数对数据进行哈希，并比较生成的哈希值以验证数据的一致性。

3. 数字签名

数字签名使用公钥加密和私钥解密来验证数据的完整性和身份认证。发送方使用私钥对数据进行签名，而接收方使用相应的公钥来验证签名的有效性。

4. 纠错码（Error-Correcting Codes）

纠错码是一种在数据中添加冗余信息以帮助恢复受损数据的方法。它们可以检测和修复数据中的错误或丢失部分。

5. 数据备份和镜像

定期创建数据备份并进行比对可以验证数据的完整性。通过比较备份和原始数据，可以确保数据没有发生变化或损坏。

6. 奇偶校验

对于位级别的数据传输，奇偶校验可以检测并纠正单个位的错误。它通过添加一个奇偶位来验证数据的正确性。





## 综合

![image-20240809072629305](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090726636.png)



![7](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090823770.png)

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408101053291.jpeg" alt="21" style="zoom: 33%;" />

------





## 7.应用层

应用层工作在操作系统中的用户态，传输层及以下则工作在内核态



### RPC

- 纯裸 TCP 是能收发数据，但它是个**无边界**的数据流，上层需要定义**消息格式**用于定义**消息边界**。于是就有了各种协议，HTTP 和各类 RPC 协议就是在 TCP 之上定义的应用层协议。
- **RPC 本质上不算是协议，而是一种调用方式**，而像 gRPC 和 Thrift 这样的具体实现，才是协议，它们是实现了 RPC 调用的协议。目的是希望程序员能像调用本地方法那样去调用远端的服务方法。同时 RPC 有很多种实现方式，**不一定非得基于 TCP 协议**。
- 从发展历史来说，**HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合**。很多软件同时支持多端，所以对外一般用 HTTP 协议，而内部集群的微服务之间则采用 RPC 协议进行通讯。
- RPC 其实比 HTTP 出现的要早，且比目前主流的 HTTP/1.1 **性能**要更好，所以大部分公司内部都还在使用 RPC。
- **HTTP/2.0** 在 **HTTP/1.1** 的基础上做了优化，性能可能比很多 RPC 协议都要好，但由于是这几年才出来的，所以也不太可能取代掉 RPC。

RPC（Remote Procedure Call，远程过程调用）是一种计算机通信协议，允许程序在不同的地址空间（通常是在不同的计算机或服务器上）之间互相调用方法或函数，就像在本地调用一样。

RPC的基本原理：

1. **调用方式**：
   - 在RPC中，客户端调用一个远程的函数或方法，实际上是在本地**调用一个“代理”函数**。这种代理函数会负责将调用请求打包，并通过网络发送到远程服务器。
   - 服务器接收到请求后，将其解包并调用对应的实际函数或方法，然后将结果返回给客户端。

2. **序列化和反序列化**：
   - 当客户端调用一个远程函数时，输入参数和调用请求会被序列化为一个字节流（或其他可传输的格式），并通过网络发送到服务器端。
   - 服务器接收到数据后，会将其反序列化为可执行的命令和数据，然后执行相应的操作。

3. **网络传输**：
   - RPC的请求和响应通常通过TCP/IP、HTTP等协议传输。传输的细节对开发者是透明的，RPC库或框架会处理这些细节。

4. **同步与异步**：
   - RPC调用可以是同步的（客户端等待服务器响应）或异步的（客户端不等待直接继续执行其他操作，服务器响应后再处理结果）。

RPC的常见使用场景：

- **分布式系统**：RPC是分布式系统中最常见的通信方式，允许不同服务或组件之间进行高效的交互。
- **微服务架构**：在微服务架构中，不同的微服务可能分布在不同的服务器或数据中心，通过RPC进行调用和数据交换。
- **跨语言调用**：一些RPC框架，如gRPC，支持跨编程语言的调用，使得不同语言编写的服务可以互相通信。

常见的RPC框架和协议：

1. **gRPC**：
   - 基于HTTP/2的高性能RPC框架，支持多种编程语言，并使用Protocol Buffers（Protobuf）进行数据序列化。
   - 适用于微服务架构，提供了强大的跨语言支持和流式传输功能。

2. **Apache Thrift**：
   - 由Facebook开发的跨语言RPC框架，支持多种编程语言和协议，可以在不同的网络环境中使用。
   
3. **XML-RPC**：
   - 基于XML的RPC协议，使用HTTP作为传输协议，适合跨平台和跨语言的应用，但相对较为简单，性能不如gRPC等现代框架。

4. **JSON-RPC**：
   - 类似于XML-RPC，但使用JSON作为数据格式，适合轻量级的应用场景。

RPC的优缺点：

**优点**：

- 透明性：开发者可以像调用本地函数一样调用远程服务。
- **提高开发效率**
   通过标准化的接口定义和自动生成客户端代理代码，RPC简化了分布式应用的开发流程，开发者无需手动处理网络传输、序列化和反序列化问题。
- **跨语言与跨平台支持**
   许多RPC框架支持多种编程语言，这使得不同技术栈和平台之间的服务可以互相调用，有助于构建异构系统。
- **高性能通信**
   通过采用二进制协议或高效序列化技术（如Protocol Buffers），RPC能够减少数据传输量，提高通信效率，适用于对性能有较高要求的场景。

**缺点**：

- **调试与异常处理困难**
   当远程调用失败时，由于网络环境和多节点协作等因素，错误的原因往往不易排查；此外，超时、重试等异常处理机制需要额外设计，增加了开发复杂性。
- 延迟：由于网络传输的存在，RPC调用比本地调用要慢。
- 异构性：在不同的操作系统、硬件和网络环境中可能会遇到兼容性问题。
- **性能折衷**
   虽然RPC在设计上注重高性能，但远程调用毕竟不如本地调用快速，序列化和反序列化过程也会引入额外开销，这在高频调用场景下尤为明显。





### WebSocket

WebSocket 是一种网络通信协议，提供了全双工通信通道，允许客户端和服务器之间进行实时双向数据交换。与传统的 HTTP 请求/响应模型不同，WebSocket 在建立连接后可以持续保持打开状态，使得客户端和服务器之间能够以更低的延迟进行高效的数据传输。

WebSocket 的关键特性：

1. **全双工通信**：
   - 服务端推送
   - WebSocket 允许服务器和客户端随时向对方发送数据，不必等待对方发送请求。与传统的 HTTP 请求-响应模型相比，这种全双工通信模式减少了通信延迟和带宽消耗。
   
2. **长连接**：
   - 一旦 WebSocket 连接建立（通过 HTTP 升级握手），该连接将保持打开状态，直到客户端或服务器主动关闭连接。这减少了频繁建立和关闭连接的开销。

3. **轻量级的头部信息**：
   - WebSocket 数据帧的头部信息非常小，相较于 HTTP 的请求/响应头，WebSocket 减少了不必要的数据开销，适合频繁的小数据包通信。

4. **低延迟**：
   - 因为 WebSocket 在连接建立后是持久连接，因此可以实现实时性更强的应用场景，如在线游戏、实时聊天、股票行情推送等。

5. **基于 TCP 的协议**：
   - WebSocket 是一个基于 TCP 的协议，确保了数据传输的可靠性。它通过 HTTP/HTTPS 建立初始连接，然后升级到 WebSocket 协议。

WebSocket 的工作原理：

1. **连接建立**：
   - WebSocket 连接从客户端发起，首先通过 HTTP/HTTPS 协议向服务器发送一个握手请求，并在头部包含 `Upgrade: websocket` 字段，表示要升级到 WebSocket 协议。
   - 服务器接收到这个请求后，如果同意升级，会返回一个带有 `101 Switching Protocols` 状态码的响应，确认升级到 WebSocket 协议。
   - 一旦握手成功，HTTP 连接会升级为 WebSocket 连接，双方可以通过这个持久的连接进行数据交换。

2. **数据传输**：
   - 数据通过 WebSocket 帧（frame）进行传输。一个 WebSocket 帧包括了帧头、有效载荷等部分。帧头信息非常轻量，通常只有 2 到 14 个字节，极大地减少了数据传输的开销。
   - WebSocket 的数据可以是文本、二进制数据等。传输过程中，双方都可以随时发送数据，不需要等对方的响应。

3. **连接关闭**：
   - WebSocket 连接可以由客户端或服务器随时关闭。关闭时会发送一个关闭控制帧（close frame），对方收到后应返回一个关闭帧，然后双方关闭连接。

WebSocket 的典型使用场景：

1. **实时聊天应用**：
   - WebSocket 非常适合实现实时聊天系统，如在线客服、聊天室等，确保消息即时传达。

2. **在线游戏**：
   - WebSocket 的低延迟特性使其适合用于需要实时通信的在线游戏，保证玩家之间的互动流畅。

3. **股票和金融数据推送**：
   - 股票行情、外汇交易等金融数据需要实时更新，WebSocket 可以在变化时立即推送最新的数据给客户端。

4. **实时通知和更新**：
   - WebSocket 可以用于实时通知系统，如推送社交媒体更新、新闻推送、在线编辑器中的协作更新等。

5. **物联网（IoT）**：
   - WebSocket 也可以用于物联网设备与服务器之间的实时通信，确保设备状态的及时更新和控制。

WebSocket 与其他协议的比较：

- **与 HTTP/HTTPS**：
  - HTTP 是无状态的请求-响应协议，每次通信需要建立和关闭连接，而 WebSocket 通过长连接实现持久通信，减少了频繁握手的开销。

- **与 AJAX/Comet**：
  - AJAX 和 Comet 可以实现类似实时更新的效果，但它们依赖于轮询或长轮询技术，存在较大的延迟和带宽浪费。WebSocket 则直接通过持久连接实现实时通信，效率更高。

- **与 Server-Sent Events (SSE)**：
  - SSE 允许服务器向客户端推送数据，但 SSE 是单向的（服务器向客户端），而 WebSocket 是双向的，功能更强大。

WebSocket 的优缺点：

**优点**：

- 实现真正的实时双向通信，适合实时性要求高的应用。
- 长连接减少了网络开销，提高了通信效率。
- 支持二进制和文本数据传输，灵活性高。

**缺点**：

- 对低延迟、高性能的服务器需求较高，适用于实时通信的场景，而非所有应用都需要这种实时性。
- 由于连接长期保持，需要注意管理连接的生命周期，以防止资源泄漏或 DoS 攻击。

WebSocket 的实现：

1. **客户端代码示例**（JavaScript）：
   ```javascript
   // 创建 WebSocket 连接
   const socket = new WebSocket('ws://example.com/socket');
   
   // 连接打开时执行
   socket.onopen = function(event) {
       console.log('WebSocket is open now.');
       socket.send('Hello Server!');  // 发送数据到服务器
   };
   
   // 接收到消息时执行
   socket.onmessage = function(event) {
       console.log('Message from server:', event.data);
   };
   
   // 连接关闭时执行
   socket.onclose = function(event) {
       console.log('WebSocket is closed now.');
   };
   
   // 连接发生错误时执行
   socket.onerror = function(error) {
       console.log('WebSocket Error:', error);
   };
   ```

2. **服务器端代码示例**（Node.js 使用 `ws` 模块）：
   ```javascript
   const WebSocket = require('ws');
   
   // 创建 WebSocket 服务器
   const wss = new WebSocket.Server({ port: 8080 });
   
   wss.on('connection', function connection(ws) {
       // 接收到客户端消息时执行
       ws.on('message', function incoming(message) {
           console.log('Received:', message);
           ws.send('Hello Client!');  // 发送数据到客户端
       });
   
       // 发送欢迎消息
       ws.send('Welcome to the WebSocket server!');
   });
   ```

WebSocket 是一种强大的通信协议，非常适合需要实时数据传输的应用。如果你有具体的问题或场景需要探讨，我可以进一步为你提供帮助。





### wsl2

**WSL2** **使用** **NAT** **网络**：

- WSL2 默认使用 NAT (Network Address Translation) 来连接到外部网络。这使得 WSL2 内部和 Windows 主机之间的网络是隔离的。
- 因此，WSL2 会有自己的 IP 地址和网络配置（例如：`172.26.71.21`），而 Windows 主机上查看到的 IP 地址是主机本身的网络接口（例如：`192.168.0.121` 和 `172.21.16.1`）。

**虚拟网络****适配器**：

- WSL2 会在 Windows 主机上创建一个虚拟网络适配器（例如：`vEthernet (WSL)`），其 IP 地址（`172.26.64.1`）用于 Windows 主机和 WSL2 之间的通信。
- Windows 主机看到的 `172.21.16.1` 可能是 Docker 或其他虚拟网络环境创建的虚拟网络接口。





### HTTP

默认80端口

超文本传输协议(**H**yperText **T**ransfer **P**rotocol), 传输单位则是消息或报文（message）

HTTP 是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」。



#### HTTP 请求处理流程

HTTP 请求处理流程如下图所示（网图）：

![202407090214361](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202407111544357.png)

一次完整的 HTTP 请求处理流程如上图所示（图片出自[《HTTP 权威指南》](https://link.juejin.cn/?target=https%3A%2F%2Fbook.douban.com%2Fsubject%2F10746113%2F)，推荐想全面理解 HTTP 的读者阅读此书），具体分为以下 4 步：

1. 建立连接

客户端发送 HTTP 请求后，服务器会根据域名进行域名解析，就是将网站名称转变成 IP 地址：`localhost` -> `127.0.0.1`，Linux hosts 文件、DNS 域名解析等可以实现这种功能。之后通过发起 TCP 的三次握手建立连接。TCP 三次连接请参考 [TCP 三次握手详解及释放连接过程](https://link.juejin.cn/?target=https%3A%2F%2Fblog.csdn.net%2Foney139%2Farticle%2Fdetails%2F8103223)，建立连接之后就可以发送 HTTP 请求了。

1. 接收请求

HTTP 服务器软件进程，这里指的是 API 服务器，在接收到请求之后，首先根据 HTTP 请求行的信息来解析到 HTTP 方法和路径。在上图所示的报文中，方法是 `GET`，路径是 `/index.html`，之后根据 API 服务器注册的路由信息（大概可以理解为：HTTP 方法 + 路径和具体处理函数的映射）找到具体的处理函数。

1. 处理请求

在接收到请求之后，API 通常会解析 HTTP 请求报文获取请求头和消息体，然后根据这些信息进行相应的业务处理，HTTP 框架一般都有自带的解析函数，只需要输入 HTTP 请求报文，就可以解析到需要的请求头和消息体。通常情况下，业务逻辑处理可以分为两种：包含对数据库的操作和不包含对数据库的操作。大型系统中通常两种都会有：

- **包含对数据库的操作：** 需要访问数据库（增删改查），然后获取指定的数据，对数据处理后构建指定的响应结构体，返回响应包。数据库通常用的是 MySQL，因为免费，功能和性能也都能满足企业级应用的要求；
- **不包含对数据库的操作：** 进行业务逻辑处理后，构建指定的响应结构体，返回响应包。

1. 记录事务处理过程

在业务逻辑处理过程中，需要记录一些关键信息，方便后期 Debug 用。在 Go 中有各种各样的日志包可以用来记录这些信息。

HTTP 请求和响应格式介绍

一个 HTTP 请求报文由请求行（request line）、请求头部（header）、空行和请求数据四部分组成，下图是请求报文的一般格式。

![202407090215751](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202407111544729.png)

- 第一行必须是一个请求行（request line），用来说明请求类型、要访问的资源以及所使用的 HTTP 版本；
- 紧接着是一个头部（header）小节，用来说明服务器要使用的附加信息；
- 之后是一个空行；
- 再后面可以添加任意的其他数据（称之为主体：body）。

> HTTP 响应格式跟请求格式类似，也是由 4 个部分组成：状态行、消息报头、空行和响应数据。



#### URI

URI（统一资源标识符）

URI是一个通用的概念，用于标识互联网上的资源。它可以标识任何类型的资源，如文档、图像、服务、电子邮件地址等。URI的结构通常分为以下几个部分：

1. **方案（Scheme）**：标识资源访问协议或方法，如`http`、`https`、`ftp`、`mailto`等。
2. **权威部分（Authority）**：可选部分，通常包含资源所在的主机名或IP地址，可能还包含端口号。
3. **路径（Path）**：资源在服务器上的具体位置或路径。
4. **查询（Query）**：可选部分，提供额外的参数，通常以键值对的形式表示。
5. **片段（Fragment）**：可选部分，指向资源中的某个部分或位置，如HTML文档中的特定锚点。

例子：

- `http://example.com/path/to/resource?query=example#fragment`

- `mailto:someone@example.com`

  

URL（统一资源定位符）

**URL是URI的一个子集**，通常用来明确定位互联网上的资源，并指示如何访问这些资源。
简单地说，URL不仅标识了资源的位置，还包含了访问该资源的具体方法（如使用HTTP、FTP等协议）。

URL的例子：

- `https://www.example.com/index.html`
- `ftp://ftp.example.com/file.txt`

URI 与 URL 的关系

- **URL是URI的一种特殊形式**。所有的URL都是URI，但并非所有的URI都是URL。
  URI可以只是一个名称或标识符，并不一定涉及访问资源的具体方法，而URL则必须包含资源的访问方法。
  
- **URI的广泛性**：URI可以用来标识任何资源，包括那些无法通过网络直接访问的资源（如URN，统一资源名称），而URL是特定用于网络资源的。

例子对比

- **URI**: `mailto:someone@example.com`（这个URI标识了一个电子邮件地址，但不是URL，因为它没有提供访问资源的方法。）
- **URL**: `https://www.example.com/index.html`（这个URL不仅标识了资源，还提供了访问方法，即通过HTTPS协议访问。）

总结来说，URL是URI的一个具体应用，专注于通过特定的协议定位和访问资源，而URI是一个更为通用的标识符概念。



#### DNS

- 根 DNS 服务器（.）
- 顶级域 DNS 服务器（.com）
- 权威 DNS 服务器（server.com）

> 域名解析的工作流程

1. 客户端首先会发出一个 DNS 请求，问 www.server.com 的 IP 是啥，并发给本地 DNS 服务器（也就是客户端的 TCP/IP 设置中填写的 DNS 服务器地址）。
2. 本地域名服务器收到客户端的请求后，如果缓存里的表格能找到 www.server.com，则它直接返回 IP 地址。如果没有，本地 DNS 会去问它的根域名服务器：“老大， 能告诉我 www.server.com 的 IP 地址吗？” 根域名服务器是最高层次的，它不直接用于域名解析，但能指明一条道路。
3. 根 DNS 收到来自本地 DNS 的请求后，发现后置是 .com，说：“www.server.com 这个域名归 .com 区域管理”，我给你 .com 顶级域名服务器地址给你，你去问问它吧。”
4. 本地 DNS 收到顶级域名服务器的地址后，发起请求问“老二， 你能告诉我 www.server.com 的 IP 地址吗？”
5. 顶级域名服务器说：“我给你负责 www.server.com 区域的权威 DNS 服务器的地址，你去问它应该能问到”。
6. 本地 DNS 于是转向问权威 DNS 服务器：“老三，www.server.com对应的IP是啥呀？” server.com 的权威 DNS 服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。
7. 权威 DNS 服务器查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS。
8. 本地 DNS 再将 IP 地址返回客户端，客户端和目标建立连接。

![image-20240809081746137](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090817422.png)



#### 状态码

（Status Code）

用于表示 HTTP 请求的处理结果和响应状态。每个状态码由三位数字组成，分别表示不同的状态类别。常见的 HTTP 响应码如下：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408091849740.png" alt="6-_E4_BA_94_E5_A4_A7_E7_B1_BBHTTP_E7_8A_B6_E6_80_81_E7_A0_81" style="zoom:50%;" />

1xx（信息性状态码）：表示请求已接收，继续处理。

- 100 Continue
- 101 Switching Protocols
- 102 Processing

2xx（成功状态码）：表示请求已成功被服务器接收、理解和处理。

- 200 OK 如果是非 `HEAD` 请求，服务器返回的响应头都会有 body 数据。
- 201 Created
- 202 Accepted
- 204 No Content 响应头没有 body 数据
- 206 Partial Content ,应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态

3xx（重定向状态码）：表示需要客户端进一步操作才能完成请求。

- 301 Moved Permanently ，表示**永久重定向**，说明请求的资源已经不存在了，需改用新的 URL 再次访问。
- 302 Found ，表示**临时重定向**，说明请求的资源还在，但暂时需要用另一个 URL 来访问。
- 304 Not Modified ，不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称**缓存重定向**，也就是告诉客户端可以继续使用缓存资源，用于缓存控制。
- 307 Temporary Redirect
- 308 Permanent Redirect

4xx（客户端错误状态码）：表示客户端发生错误，请求包含语法错误或无法完成请求。

- **400 Bad Request** ，表示客户端请求的报文有错误，但只是个笼统的错误。
- **401 Unauthorized**
- 403 Forbidden ，表示服务器禁止访问资源，并不是客户端的请求出错。
- **404 Not Found** ，表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。
- 405 Method Not Allowed
- 429 Too Many Requests

5xx（服务器错误状态码）：表示服务器在处理请求时发生了错误。

- **500 Internal Server Error** ，服务器发生了什么错误，我们并不知道。
- 501  Not Implemented  表示客户端请求的功能还不支持
- **502 Bad Gateway** ，通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。
- 503 Service Unavailable ，表示服务器当前很忙，暂时无法响应客户端，类似“网络服务正忙，请稍后重试”的意思
- **504 Gateway Timeout**
- 507 Insufficient Storage

这些状态码是标准的 HTTP 响应状态码，用于向客户端传达请求的处理结果和服务器状态。客户端在收到响应后，可以根据不同的状态码来做相应的处理，比如重试、跳转或者展示相应的错误信息。



#### 常见字段

在HTTP（Hypertext Transfer Protocol）协议中，HTTP请求和响应报文由多种字段组成，这些字段提供了关于请求或响应的重要信息。以下是HTTP报文中常见的字段及其作用：

##### 1. **请求字段（Request Fields）**
这些字段出现在HTTP请求报文中，通常由客户端发送给服务器。

- **`Host`**：指定请求的目标主机和端口号。例如，`Host: www.example.com`。
- **`User-Agent`**：包含发出请求的客户端软件信息，如浏览器类型和版本。例如，`User-Agent: Mozilla/5.0`。
- **`Accept`**：指定客户端可以处理的内容类型。例如，`Accept: text/html` 表示客户端希望接收HTML文档。
- **`Accept-Language`**：指定客户端希望接收的语言种类。例如，`Accept-Language: en-US` 表示客户端希望接收美国英语的内容。
- **`Accept-Encoding`**：指定客户端支持的内容编码方式，如压缩类型。例如，`Accept-Encoding: gzip, deflate`。
- **`Content-Type`**：指明请求主体的媒体类型。例如，`Content-Type: application/json` 表示请求体是JSON格式的数据。
- **`Content-Length`**：表示请求主体的长度，以字节为单位。例如，`Content-Length: 348`。
- **`Authorization`**：提供身份验证信息。例如，`Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==`。
- **`Cookie`**：包含从服务器接收并存储在客户端的cookie信息。例如，`Cookie: sessionId=abc123`。
- **`Referer`**：指明客户端是从哪个URL链接跳转而来的。例如，`Referer: http://www.example.com/previous-page`。
- **`Connection`**：控制连接的类型。例如，`Connection: keep-alive` 表示保持连接以便复用。

##### 2. **响应字段（Response Fields）**
这些字段出现在HTTP响应报文中，由服务器返回给客户端。

- **`Status-Code`**：表示请求的结果状态码。例如，`200 OK` 表示请求成功，`404 Not Found` 表示资源未找到。
- **`Server`**：包含处理请求的服务器软件信息。例如，`Server: Apache/2.4.41 (Ubuntu)`。
- **`Date`**：指明响应生成的日期和时间。例如，`Date: Mon, 15 Aug 2024 12:34:56 GMT`。
- **`Content-Type`**：指明响应主体的媒体类型。例如，`Content-Type: text/html; charset=UTF-8` 表示响应体是HTML文档，使用UTF-8编码。
- **`Content-Length`**：表示响应主体的长度，以字节为单位。例如，`Content-Length: 1042`。
- **`Set-Cookie`**：用于设置客户端的cookie信息。例如，`Set-Cookie: sessionId=xyz456; Path=/; Expires=Wed, 16-Aug-2024 12:34:56 GMT`。
- **`Last-Modified`**：指明资源的最后修改时间。例如，`Last-Modified: Wed, 21 Oct 2023 07:28:00 GMT`。
- **`ETag`**：提供资源的唯一标识符，用于缓存控制。例如，`ETag: "686897696a7c876b7e"`。
- **`Cache-Control`**：指示缓存机制的指令。例如，`Cache-Control: no-cache` 表示不要缓存响应内容。
- **`Location`**：在重定向响应中使用，指示客户端应访问的新URL。例如，`Location: http://www.example.com/new-page`。
- **`Connection`**：指示连接的管理。例如，`Connection: close` 表示响应后关闭连接。

##### 3. **通用字段（General Fields）**
这些字段既可以在请求报文中出现，也可以在响应报文中出现，提供通用的信息。

- **`Cache-Control`**：定义缓存机制的指令。
- **`Connection`**：控制网络连接的管理。
- **`Date`**：指示报文生成的日期和时间。
- **`Transfer-Encoding`**：表示内容传输时采用的编码方式（例如`chunked`）。
- **`Via`**：显示消息经过的中间节点（如代理服务器）的信息。

##### 4. **实体字段（Entity Fields）**
这些字段描述了HTTP报文的实体部分，即请求或响应的主体内容。

- **`Content-Encoding`**：指示实体主体采用的编码方式。
- **`Content-Language`**：指示实体主体的语言。
- **`Content-Location`**：指明实体的实际位置URL。
- **`Content-MD5`**：提供实体的MD5校验值，用于数据完整性校验。
- **`Content-Range`**：表示实体在资源中的范围，通常用于分段下载。
- **`Expires`**：指示实体数据的过期时间。
- **`Last-Modified`**：指示实体数据的最后修改时间。

这些HTTP字段在请求和响应过程中起着重要作用，帮助客户端和服务器之间有效传递信息和控制数据的传输行为。



#### 方法

HTTP（Hypertext Transfer Protocol）常见的方法用于定义客户端与服务器之间的请求类型。以下是HTTP中常见的几种方法及其用途：

1. **GET**

- **用途**：从服务器请求指定资源。请求使用GET方法应该只提取数据，而不应对数据进行任何修改。
- **特点**：GET请求的参数包含在URL中，不适合传输敏感信息。安全且幂等
- **示例**：`GET /index.html HTTP/1.1`

2. **POST**

- **用途**：向服务器提交数据，通常是表单数据，用于创建资源。
- **特点**：POST请求将数据包含在请求体中，相对于GET更安全一些。不是幂等的
- **示例**：`POST /submit-form HTTP/1.1`

3. **PUT**

- **用途**：向服务器上传文件，或者更新服务器上的资源。
- **特点**：PUT请求的数据通常包含在请求体中，且PUT请求是幂等的（多次执行相同操作的结果一致）。
- **示例**：`PUT /user/123 HTTP/1.1`

PUT 和 POST 的区别主要在以下几个方面：

1. 语义不同：PUT 请求通常用于更新或替换服务器上的资源，而 POST 请求通常用于创建新的资源或提交数据到服务器进行处理。
2. 客户端发送的数据不同：PUT 请求需要客户端发送完整的资源内容，而 POST 请求可以只发送部分资源内容。
3. 响应不同：PUT 请求成功后通常返回 200 OK 状态码，而 POST 请求成功后通常返回 201 Created 状态码，并返回表示新资源的 URI。
4. 幂等性不同：PUT 请求具有幂等性，即执行多次 PUT 请求的结果应该相同，而 POST 请求不具有幂等性。
5. 安全性不同：PUT 请求是安全的，即对服务器上的资源的任何操作都不会导致状态的改变。而 POST 请求不是安全的，它可能会导致状态的改变。
6. 可缓存性不同：PUT 请求是可缓存的，客户端可以使用缓存的响应来更新资源。而 POST 请求不是可缓存的，客户端不能使用缓存的响应来更新资源。
7. 使用场景不同：PUT 请求适用于更新完整的资源，比如更新用户信息、文章内容等。而 POST 请求适用于提交数据到服务器进行处理，比如创建新的文章、提交表单等。



4. **DELETE**

- **用途**：请求服务器删除指定的资源。
- **特点**：DELETE请求是幂等的。
- **示例**：`DELETE /user/123 HTTP/1.1`

5. **HEAD**

- **用途**：与GET方法类似，但服务器在响应中只返回头部信息，不返回实际的资源内容。
- **特点**：用于检查资源的状态或元数据（如资源是否存在、资源的修改时间等）。
- **示例**：`HEAD /index.html HTTP/1.1`

6. **OPTIONS**

- **用途**：用于查询服务器支持的HTTP方法，或者检查某个资源支持的HTTP方法。
- **特点**：服务器会在响应中返回Allow头部字段，指明支持的方法。
- **示例**：`OPTIONS / HTTP/1.1`

7. **PATCH**

- **用途**：用于对资源进行部分修改。
- **特点**：PATCH请求通常包含要更新的数据，而不是整个资源。
- **示例**：`PATCH /user/123 HTTP/1.1`

8. **TRACE**

- **用途**：用于测试和诊断，请求服务器回显收到的请求，主要用于诊断或调试。
- **特点**：不常用，存在安全风险（如XSS攻击）。
- **示例**：`TRACE / HTTP/1.1`

9. **CONNECT**

- **用途**：用于建立到服务器的隧道，通常用于HTTPS协议，代理服务器使用。
- **特点**：建立一个隧道连接，实现端到端的通信。
- **示例**：`CONNECT www.example.com:443 HTTP/1.1`

10. **LINK 和 UNLINK**

- **用途**：用于创建或删除资源之间的链接关系。
- **特点**：不常用，更多是为了语义上的清晰。
- **示例**：
  - `LINK /user/123 HTTP/1.1`
  - `UNLINK /user/123 HTTP/1.1`

#### 缓存技术

##### 强制缓存

强缓存指的是只要浏览器判断缓存没有过期，则直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边。

强缓存是利用下面这两个 HTTP 响应头部（Response Header）字段实现的，它们都用来表示资源在客户端缓存的有效期：

- `Cache-Control`， 是一个相对时间；
- `Expires`，是一个绝对时间；

如果 HTTP 响应头部同时有 Cache-Control 和 Expires 字段的话，**Cache-Control 的优先级高于 Expires** 。

Cache-control 选项更多一些，设置更加精细，所以建议使用 Cache-Control 来实现强缓存。具体的实现流程如下：

- 当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 Cache-Control，Cache-Control 中设置了过期时间大小；
- 浏览器再次请求访问服务器中的该资源时，会先**通过请求资源的时间与 Cache-Control 中设置的过期时间大小，来计算出该资源是否过期**，如果没有，则使用该缓存，否则重新请求服务器；
- 服务器再次收到请求后，会再次更新 Response 头部的 Cache-Control。



##### 协商缓存

通过服务端告知客户端是否可以使用缓存的方式被称为协商缓存。

上图就是一个协商缓存的过程，所以**协商缓存就是与服务端协商之后，通过协商结果来判断是否使用本地缓存**。

协商缓存可以基于两种头部来实现。

第一种：请求头部中的 `If-Modified-Since` 字段与响应头部中的 `Last-Modified` 字段实现，这两个字段的意思是：

- 响应头部中的 `Last-Modified`：标示这个响应资源的最后修改时间；
- 请求头部中的 `If-Modified-Since`：当资源过期了，发现响应头中具有 Last-Modified 声明，则再次发起请求的时候带上 Last-Modified 的时间，服务器收到请求后发现有 If-Modified-Since 则与被请求资源的最后修改时间进行对比（Last-Modified），如果最后修改时间较新（大），说明资源又被改过，则返回最新资源，HTTP 200 OK；如果最后修改时间较旧（小），说明资源无新修改，响应 HTTP 304 走缓存。

第二种：请求头部中的 `If-None-Match` 字段与响应头部中的 `ETag` 字段，这两个字段的意思是：

- 响应头部中 `Etag`：唯一标识响应资源；
- 请求头部中的 `If-None-Match`：当资源过期时，浏览器发现响应头里有 Etag，则再次向服务器发起请求时，会将请求头 If-None-Match 值设置为 Etag 的值。服务器收到请求后进行比对，如果资源没有变化返回 304，如果资源变化了返回 200。

第一种实现方式是基于时间实现的，第二种实现方式是基于一个唯一标识实现的，相对来说后者可以更加准确地判断文件内容是否被修改，避免由于时间篡改导致的不可靠问题。

如果在第一次请求资源的时候，服务端返回的 HTTP 响应头部同时有 Etag 和 Last-Modified 字段，那么客户端再下一次请求的时候，如果带上了 ETag 和 Last-Modified 字段信息给服务端，**这时 Etag 的优先级更高**，也就是服务端先会判断 Etag 是否变化了，如果 Etag 有变化就不用在判断 Last-Modified 了，如果 Etag 没有变化，然后再看 Last-Modified。

**为什么 ETag 的优先级更高？**这是因为 ETag 主要能解决 Last-Modified 几个比较难以解决的问题：

1. 在没有修改文件内容情况下文件的最后修改时间可能也会改变，这会导致客户端认为这文件被改动了，从而重新请求；
2. 可能有些文件是在秒级以内修改的，`If-Modified-Since` 能检查到的粒度是秒级的，使用 Etag就能够保证这种需求下客户端在 1 秒内能刷新多次；
3. 有些服务器不能精确获取文件的最后修改时间。

注意，**协商缓存这两个字段都需要配合强制缓存中 Cache-Control 字段来使用，只有在未能命中强制缓存的时候，才能发起带有协商缓存字段的请求**。

![_E7_BC_93_E5_AD_98etag](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408110908603.png)

![http_E7_BC_93_E5_AD_98 (1)](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408110909281.png)



### http1.1

默认端口号是 80

#### HTTP/1.1 的特性

1. **持久连接（Persistent Connections）**:
   - HTTP/1.1默认启用了持久连接，也称为长连接，即在同一个TCP连接上可以进行多个HTTP请求和响应，减少了连接的建立和关闭开销。
   - 通过`Connection: keep-alive`头部字段，可以显式控制连接的保持和关闭。

2. **管道化请求（Pipelining）**:
   - HTTP/1.1支持管道化请求，允许在发送前一个请求的响应之前，客户端继续发送后续请求，从而提高网络利用率和减少延迟。不过，由于早期浏览器和服务器对该特性的支持不佳，实际使用较少。

3. **分块传输编码（Chunked Transfer Encoding）**:
   - HTTP/1.1引入了分块传输编码，允许在发送响应时逐步传输数据，而无需事先知道内容的总大小。这对于流媒体传输和动态生成内容非常有用。

4. **额外的缓存控制**:
   - 引入了更多的缓存控制机制，通过`Cache-Control`头部字段来精细化地控制缓存行为，比如`max-age`、`no-cache`、`no-store`等指令。

5. **内容协商（Content Negotiation）**:
   - 支持基于客户端的能力（如语言、编码、媒体类型等）进行内容协商，服务器可以根据客户端请求头中的`Accept`字段，返回不同的资源版本。

6. **增强的状态管理**:
   - 通过Cookie机制实现了更为复杂的状态管理。服务器可以通过Set-Cookie头部字段向客户端发送状态信息，客户端在后续请求中通过Cookie头部字段回传这些状态信息。

#### HTTP/1.1 的优点

1. **简单性**:
   - HTTP/1.1的报文格式非常简单，易于理解和实现。头部字段使用纯文本的key-value对形式，便于调试和扩展。

2. **灵活性和可扩展性**:
   - HTTP/1.1在请求方法、URI、状态码和头字段等方面提供了很大的灵活性，允许自定义和扩展。这使得HTTP/1.1能够适应不同的应用场景和需求。
   - **下层可以随意变化**，比如：
     - HTTPS 就是在 HTTP 与 TCP 层之间增加了 SSL/TLS 安全传输层；
   
3. **广泛的应用和跨平台性**:
   - HTTP/1.1已经被广泛应用于各种Web应用程序、API服务和移动应用。它的跨平台性使其能够在各种设备和操作系统上运行，成为了互联网的基础协议。

4. **持久连接提升了性能**:
   - 持久连接减少了频繁建立和关闭TCP连接的开销，提高了网络资源的利用率和响应速度。

#### HTTP/1.1 的缺点

1. **无状态**:
   - HTTP是无状态协议，每个请求都是独立的，服务器不会自动记忆请求之间的状态。虽然无状态性简化了协议设计和实现，但在处理需要保持状态的操作时，如用户登录、购物车管理等，变得复杂且不高效。为了解决这个问题，通常需要引入Cookie、Session等机制。

2. **明文传输**:
   - HTTP/1.1使用明文传输，这意味着所有的数据在网络上传输时是未加密的，容易被窃听和篡改。对于敏感信息（如登录凭据、个人数据等），这种缺陷可能导致严重的安全问题。

3. **安全性问题**:
   - HTTP/1.1缺乏内置的安全机制，不能验证通信双方的身份，也无法保证数据的完整性。攻击者可以进行中间人攻击、伪装攻击和数据篡改。HTTPS（通过在HTTP和TCP之间引入SSL/TLS层）是解决这些问题的常用方案。

4. **性能瓶颈**:
   - 尽管HTTP/1.1引入了持久连接和管道化请求，但它仍然受限于**TCP的头部阻塞**问题（Head-of-Line Blocking）。一个连接上的请求和响应是顺序处理的，一个请求的延迟会影响后续所有请求的处理。HTTP/2和HTTP/3在这方面进行了改进。

#### 优化

##### 一、缓存



##### 二、减少 HTTP 请求次数

###### 1.减少重定向请求次数

​	将原本由客户端处理的重定向请求，交给代理服务器处理，这样可以减少重定向请求的次数；

###### 2.*合并请求*；

​	将多个小资源合并成一个大资源再传输，能够减少 HTTP 请求次数以及 头部的重复传输，再来减少 TCP 连接数量，进而省去 TCP 握手和慢启动的网络消耗；

###### 3.*延迟发送请求*

​	按需访问资源，只访问当前用户看得到/用得到的资源，当客户往下滑动，再访问接下来的资源，以此达到延迟请求，也就减少了同一时间的 HTTP 请求次数。

##### 三、压缩响应资源

​	通过压缩响应资源，降低传输资源的大小，从而提高传输效率，所以应当选择更优秀的压缩算法。

- *无损压缩*；

  ```text
  Accept-Encoding: gzip, deflate, br
  ```

- *有损压缩*；

  通过 HTTP 请求头部中的 `Accept` 字段里的「 q 质量因子」，告诉服务器期望的资源质量。

  ```text
  Accept: audio/*; q=0.2, audio/basic
  ```

  图片压缩，视频常见的编码格式有 H264、H265 等，音频常见的编码格式有 AAC、AC3。





### https

默认端口号是 443



#### 安全

HTTPS 协议本身到目前为止还是没有任何漏洞的，即使你成功进行中间人攻击，本质上是利用了客户端的漏洞（**用户点击继续访问或者被恶意导入伪造的根证书**），并不是 HTTPS 不够安全。



HTTP 由于是明文传输，所以安全上存在以下三个风险：

- **窃听风险**，比如通信链路上可以获取通信内容，用户号容易没。
- **篡改风险**，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。
- **冒充风险**，比如冒充淘宝网站，用户钱容易没。

HTTP**S** 在 HTTP 与 TCP 层之间加入了 `SSL/TLS` 协议，可以很好的解决了上述的风险：

- **信息加密**：交互信息无法被窃取，但你的号会因为「自身忘记」账号而没。
- **校验机制**：无法篡改通信内容，篡改了就不能正常显示，但百度「竞价排名」依然可以搜索垃圾广告。
- **身份证书**：证明淘宝是真的淘宝网，但你的钱还是会因为「剁手」而没。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408111010590.png" alt="image-20240811101034314" style="zoom:50%;" />

1. **混合加密**的方式实现信息的**机密性**，解决了窃听的风险。

2. **摘要算法+数字签名**的方式来实现**完整性**，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。

3. 将服务器公钥放入到**数字证书**中，解决了冒充的风险。

   

##### 1.*混合加密*

HTTPS 采用的是**对称加密**和**非对称加密**结合的「混合加密」方式：

- 在通信建立前采用**非对称加密**的方式交换「会话秘钥」，后续就不再使用非对称加密。
- 在通信过程中全部使用**对称加密**的「会话秘钥」的方式加密明文数据。

采用「混合加密」的方式的原因：

- **对称加密**只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。
- **非对称加密**使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。



##### *2. 摘要算法 + 数字签名*

为了保证传输的内容不被篡改，在计算机里会**用摘要算法（哈希函数）来计算出内容的哈希值**，也就是内容的「指纹」，这个**哈希值是唯一的，且无法通过哈希值推导出内容**。

流程的不同，意味着目的也不相同：

- **公钥加密，私钥解密**。这个目的是为了**保证内容传输的安全**，因为被公钥加密的内容，其他人是无法解密的，只有持有私钥的人，才能解密出实际的内容；
- **私钥加密，公钥解密**。这个目的是为了**保证消息不会被冒充**，因为私钥是不可泄露的，如果公钥能正常解密出私钥加密的内容，就能证明这个消息是来源于持有私钥身份的人发送的。

一般我们不会用非对称加密来加密实际的传输内容，因为非对称加密的计算比较耗费性能的。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408111014528.png" alt="_E6_95_B0_E5_AD_97_E7_AD_BE_E5_90_8D" style="zoom: 67%;" />

##### *3. 数字证书*

**身份验证的环节**

![22-_E6_95_B0_E5_AD_97_E8_AF_81_E4_B9_A6_E5_B7_A5_E4_BD_9C_E6_B5_81_E7_A8_8B](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408111016298.jpeg)





#### 建立连接

SSL/TLS 协议基本流程：

- 客户端向服务器索要并验证服务器的公钥。
- 双方协商生产「会话秘钥」。
- 双方采用「会话秘钥」进行加密通信。



##### RSA 四次握手

基于 RSA 算法的 HTTPS 存在**「前向安全」**的问题：如果服务端的私钥泄漏了，过去被第三方截获的所有 TLS 通讯密文都会被破解。



TLS 协议建立的详细流程：

###### *1. ClientHello*

首先，由客户端向服务器发起加密通信请求，也就是 `ClientHello` 请求。

在这一步，客户端主要向服务器发送以下信息：

（1）客户端支持的 TLS 协议版本，如 TLS 1.2 版本。

（2）客户端生产的随机数（`Client Random`），后面用于生成「会话秘钥」条件之一。

（3）客户端支持的密码套件列表，如 RSA 加密算法。

###### *2. SeverHello*

服务器收到客户端请求后，向客户端发出响应，也就是 `SeverHello`。服务器回应的内容有如下内容：

（1）确认 TLS 协议版本，如果浏览器不支持，则关闭加密通信。

（2）服务器生产的随机数（`Server Random`），也是后面用于生产「会话秘钥」条件之一。

（3）确认的密码套件列表，如 RSA 加密算法。

​	密码套件基本的形式是「**密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法**」 

​	Cipher Suite: TLS_RSA_WITH_AES_128_GCM_SHA256

（4）**服务器的数字证书**。

###### *3.客户端回应*

客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。

如果证书没有问题，客户端会**从数字证书中取出服务器的公钥**，然后使用它加密报文，向服务器发送如下信息：

（1）一个随机数（`pre-master key`）。该随机数会被服务器公钥加密。

（2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。

（3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。

上面第一项的随机数是整个握手阶段的第三个随机数，会发给服务端，所以这个随机数客户端和服务端都是一样的。

服务器和客户端有了这**三个随机数（Client Random、Server Random、pre-master key）**，接着就用双方协商的加密算法，**各自生成本次通信的「会话秘钥」**。

「Server Certificate」和「Server Hello Done」



###### 4. 服务器的最后回应

服务器收到客户端的第三个随机数（`pre-master key`）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。

然后，向客户端发送最后的信息：

（1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。

（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。

至此，整个 TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408111019343.jpeg" alt="23-HTTPS_E5_B7_A5_E4_BD_9C_E6_B5_81_E7_A8_8B" style="zoom:50%;" />





##### ECDHE 算法

**每次会话生成新的密钥**

**密钥对生成**：

- 客户端生成一个临时的椭圆曲线密钥对（私钥和公钥）。
- 服务器也生成一个临时的椭圆曲线密钥对。

**公钥交换**：

- 客户端将其公钥发送给服务器。
- 服务器也将其公钥发送给客户端。

**共享密钥计算**：

- 客户端使用自己的私钥和服务器的公钥计算共享密钥。
- 服务器使用自己的私钥和客户端的公钥计算相同的共享密钥。

**对称密钥生成**：

- 共享密钥可以用作生成对称密钥，供后续通信加密使用。



- 双方事先确定好使用哪种椭圆曲线，和曲线上的基点 G，这两个参数都是公开的；
- 双方各自随机生成一个随机数作为**私钥d**，并**与基点 G相乘得到公钥Q**（Q = dG），此时小红的公私钥为 Q1 和 d1，小明的公私钥为 Q2 和 d2；
- 双方交换各自的公钥，最后小红计算点（x1，y1） = d1Q2，小明计算点（x2，y2） = d2Q1，由于椭圆曲线上是可以满足乘法交换和结合律，所以 d1Q2 = d1d2G = d2d1G = d2Q1 ，因此**双方的 x 坐标是一样的，所以它是共享密钥，也就是会话密钥**。

这个过程中，双方的私钥都是随机、临时生成的，都是不公开的，即使根据公开的信息（椭圆曲线、公钥、基点 G）也是很难计算出椭圆曲线上的离散对数（私钥）

1. **握手过程的效率**：**使用了 ECDHE，在 TLS 第四次握手前，客户端就已经发送了加密的 HTTP 数据**，而对于 RSA 握手过程，必须要完成 TLS 四次握手，才能传输应用数据。所以，**ECDHE 相比 RSA 握手过程省去了一个消息往返的时间**

2. **前向保密性**：ECDHE支持前向保密，这意味着即使将来某一方的长期密钥泄露，过去的会话密钥也不会受到威胁。RSA不支持前向保密。

   ECDHE使用的密钥对（私钥和公钥）是临时生成的，也被称为**会话密钥对**。每次会话都使用不同的密钥对进行密钥交换。

   由于这些密钥是临时的、会话专用的，即使长期密钥（如证书中的私钥）被泄露，攻击者也无法解密先前的通信，因为每次会话的密钥都是独立的、动态生成的。



##### TLS1.3

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408112301446.png" alt="4cad213f5125432693e0e2a512c2d1a1-20230309231022316" style="zoom: 67%;" />

 TLS 1.3 把客户端的 **Hello 和公钥交换**这两个消息合并成了一个消息，于是这样就减少到只需 **1 RTT** 就能完成 TLS 握手。



客户端在 Client Hello 消息里带上了支持的椭圆曲线，以及这些椭圆曲线对应的公钥。

服务端收到后，选定一个椭圆曲线等参数，然后返回消息时，带上服务端这边的公钥。经过这 1 个 RTT，双方手上已经有生成会话密钥的材料了，于是客户端计算出会话密钥，就可以进行应用数据的加密传输了。

而且，TLS1.3 对密码套件进行“减肥”了， 对于密钥交换算法，**废除了**不支持前向安全性的 **RSA 和 DH 算法**，**只支持 ECDHE 算法。**

对于对称加密和签名算法，只支持目前最安全的几个密码套件，比如 openssl 中仅支持下面 5 种密码套件：

- TLS_AES_256_GCM_SHA384
- TLS_CHACHA20_POLY1305_SHA256
- TLS_AES_128_GCM_SHA256
- TLS_AES_128_CCM_8_SHA256
- TLS_AES_128_CCM_SHA256



<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408130910654.png" alt="tls1.2and1.3" style="zoom:50%;" />

###### 重连 

Session ID 和 Session Ticket 方式都需要在 1 RTT 才能恢复会话。

对于重连 TLS1.3 只需要 **0 RTT**，

在重连时，客户端会把 Ticket 和 HTTP 请求一同发送给服务端，这种方式叫 **Pre-shared Key**。同样的，Pre-shared Key 也有重放攻击的危险





#### 客户端校验数字证书的流程

签发和验证流程：

CA 签发证书的过程：

- 首先 CA 会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包，然后对这些信息进行 Hash 计算，得到一个 Hash 值；
- 然后 CA 会使用自己的私钥将该 Hash 值加密，生成 Certificate Signature，也就是 CA 对证书做了签名；
- 最后将 Certificate Signature 添加在文件证书上，形成数字证书；

客户端校验服务端的数字证书的过程：

- 首先客户端会使用同样的 Hash 算法获取该证书的 Hash 值 H1；
- 通常浏览器和操作系统中集成了 CA 的公钥信息，浏览器收到证书后可以使用 CA 的公钥解密 Certificate Signature 内容，得到一个 Hash 值 H2 ；
- 最后比较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408111025969.png" alt="_E8_AF_81_E4_B9_A6_E7_9A_84_E6_A0_A1_E9_AA_8C" style="zoom:50%;" />



**证书信任链**

![baidu_E8_AF_81_E4_B9_A6](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408111026803.png)

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408111026415.png" alt="_E7_94_A8_E6_88_B7_E4_BF_A1_E4_BB_BB" style="zoom:50%;" />

最后一个问题，为什么需要证书链这么麻烦的流程？Root CA 为什么不直接颁发证书，而是要搞那么多中间层级呢？

**这是为了确保根证书的绝对安全性，将根证书隔离地越严格越好，不然根证书如果失守了，那么整个信任链都会有问题。**



##### 证书验证优化

**OCSP Stapling**，其原理是：服务器向 CA 周期性地查询证书状态，获得一个带有时间戳和签名的响应结果并缓存它。

当有客户端发起连接请求时，服务器会把这个「响应结果」在 TLS 握手过程中发给客户端。由于有签名的存在，服务器无法篡改，因此客户端就能得知证书是否已被吊销了，这样客户端就不需要再去查询。

![opscp-stapling](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408130916765.png)



#### 应用数据完整性

TLS 在实现上分为**握手协议**和**记录协议**两层：

- TLS 握手协议就是我们前面说的 TLS 四次握手的过程，负责协商加密算法和生成对称密钥，后续用此密钥来保护应用程序数据（即 HTTP 数据）；
- TLS 记录协议负责保护应用程序数据并验证其完整性和来源，所以对 HTTP 数据加密是使用记录协议；

TLS 记录协议主要负责消息（HTTP 数据）的压缩，加密及数据的认证，过程如下图：

具体过程如下：

- 首先，消息被分割成多个较短的片段,然后分别对每个片段进行压缩。
- 接下来，经过压缩的片段会被**加上消息认证码（MAC 值，这个是通过哈希算法生成的），这是为了保证完整性，并进行数据的认证**。通过附加消息认证码的 MAC 值，可以识别出篡改。与此同时，为了防止重放攻击，在计算消息认证码时，还加上了片段的编码。
- 再接下来，经过压缩的片段再加上消息认证码会一起通过对称密码进行加密。
- 最后，上述经过加密的数据再加上由数据类型、版本号、压缩后的长度组成的报头就是最终的报文数据。

记录协议完成后，最终的报文数据将传递到传输控制协议 (TCP) 层进行传输。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408111028705.png" alt="_E8_AE_B0_E5_BD_95_E5_8D_8F_E8_AE_AE" style="zoom:50%;" />



#### 为什么抓包工具能截取 HTTPS 数据？

中间人要拿到私钥只能通过如下方式：

1. 去网站服务端拿到私钥；
2. 去CA处拿域名签发私钥；
3. 自己签发证书，切要被浏览器信任；

抓包工具只能使用第三种方式取得中间人的身份。

使用抓包工具进行 HTTPS 抓包的时候，需要在客户端安装 **Fiddler 的根证书**，这里实际上起认证中心（CA）的作用。

抓包工具能够抓包的关键是客户端会往系统受信任的根证书列表中导入抓包工具生成的证书，而这个证书会被浏览器信任，也就是抓包工具给自己创建了一个认证中心 CA，客户端拿着中间人签发的证书去中间人自己的 CA 去认证，当然认为这个证书是有效的。



如何避免被中间人抓取数据？

通过 **HTTPS 双向认证**



####  TLS1.2 握手会话复用

##### Session ID

Session ID 的工作原理是，**客户端和服务器首次 TLS 握手连接后，双方会在内存缓存会话密钥，并用唯一的 Session ID 来标识**，Session ID 和会话密钥相当于 key-value 的关系。

当客户端再次连接时，hello 消息里会带上 Session ID，服务器收到后就会从内存找，如果找到就直接用该会话密钥恢复会话状态，跳过其余的过程，只用一个消息往返就可以建立安全通信。当然为了安全性，内存中的会话密钥会定期失效。

两个缺点：

- 服务器必须保持每一个客户端的会话密钥，随着客户端的增多，**服务器的内存压力也会越大**。
- 现在网站服务一般是由多台服务器通过负载均衡提供服务的，**客户端再次连接不一定会命中上次访问过的服务器**，于是还要走完整的 TLS 握手过程；



##### Session Ticket

为了解决 Session ID 的问题，就出现了 Session Ticket，**服务器不再缓存每个客户端的会话密钥，而是把缓存的工作交给了客户端**，类似于 HTTP 的 Cookie。

客户端与服务器首次建立连接时，服务器会加密「会话密钥」作为 Ticket 发给客户端，交给客户端缓存该 Ticket。

客户端再次连接服务器时，客户端会发送 Ticket，服务器解密后就可以获取上一次的会话密钥，然后验证有效期，如果没问题，就可以恢复会话了，开始加密通信。

对于集群服务器的话，**要确保每台服务器加密 「会话密钥」的密钥是一致的**，这样客户端携带 Ticket 访问任意一台服务器时，都能恢复会话。

Session ID 和 Session Ticket **都不具备前向安全性**，因为一旦加密「会话密钥」的密钥被破解或者服务器泄漏「会话密钥」，前面劫持的通信密文都会被破解。



同时应对**重放攻击**也很困难，这里简单介绍下重放攻击工作的原理。

假设 Alice 想向 Bob 证明自己的身份。 Bob 要求 Alice 的密码作为身份证明，爱丽丝应尽全力提供（可能是在经过如哈希函数的转换之后）。与此同时，Eve 窃听了对话并保留了密码（或哈希）。

交换结束后，Eve（冒充 Alice ）连接到 Bob。当被要求提供身份证明时，Eve 发送从 Bob 接受的最后一个会话中读取的 Alice 的密码（或哈希），从而授予 Eve 访问权限。

重放攻击的危险之处在于，如果中间人截获了某个客户端的 Session ID 或 Session Ticket 以及 POST 报文，而一般 POST 请求会改变数据库的数据，中间人就可以利用此截获的报文，不断向服务器发送该报文，这样就会导致数据库的数据被中间人改变了，而客户是不知情的。

避免重放攻击的方式就是需要**对会话密钥设定一个合理的过期时间**。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408130921767.png" alt="_E9_87_8D_E6_94_BE_E6_94_BB_E5_87_BB" style="zoom:50%;" />







####  TLS 和 TCP 同时握手

需要下面这两个条件同时满足才可以：

- **客户端和服务端都开启了 TCP Fast Open 功能，且 TLS 版本是 1.3；**
- **客户端和服务端已经完成过一次通信；**

------



### http2

#### 性能改进

HTTP/1.1 相比 HTTP/1.0 性能上的改进：

- 使用长连接的方式改善了 HTTP/1.0 短连接造成的性能开销。
- 支持管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。

但 HTTP/1.1 还是有性能瓶颈：

- 请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 `Body` 的部分；
- 发送冗长的首部。每次互相发送相同的首部造成的浪费较多；
- 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞；
- 没有请求优先级控制；
- 请求只能从客户端开始，服务器只能被动响应。



##### *1. 头部压缩*

HTTP/2 会**压缩头**（Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你**消除重复的部分**。

这就是所谓的 `HPACK` 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就**提高速度**了。



HTTP/2 没使用常见的 gzip 压缩方式来压缩头部，而是开发了**HPACK** 算法，HPACK 算法主要包含三个组成部分：

- 静态字典；
- 动态字典；
- Huffman 编码（压缩算法）；

客户端和服务器两端都会建立和维护「**字典**」，用长度较小的索引号表示重复的字符串，再用 Huffman 编码压缩数据，**可达到 50%~90% 的高压缩率**



 静态表编码

HTTP/2 为高频出现在头部的字符串和字段建立了一张**静态表**，它是写入到 HTTP/2 框架里的，不会变化的，静态表里共有 `61` 组，如下图：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408131022929.png" alt="image-20240105142818571" style="zoom:50%;" />

动态表编码

使得动态表生效有一个前提：**必须同一个连接上，重复传输完全相同的 HTTP 头部**



自行构建**动态表**，它的 Index 从 `62` 起步，会在编码解码的时候随时更新。

比如，第一次发送时头部中的「`User-Agent` 」字段数据有上百个字节，经过 Huffman 编码发送出去后，客户端和服务器双方都会更新自己的动态表，添加一个新的 Index 号 62。**那么在下一次发送的时候，就不用重复发这个字段的数据了，只用发 1 个字节的 Index 号就好了，因为双方都可以根据自己的动态表获取到字段的数据**。





##### *2. 二进制格式*

HTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了**二进制格式**，头信息和数据体都是二进制，并且统称为帧（frame）：**头信息帧（Headers Frame）和数据帧（Data Frame）**。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408111033463.png" alt="_E4_BA_8C_E8_BF_9B_E5_88_B6_E5_B8_A7" style="zoom:50%;" />

帧头（Frame Header）很小，只有 9 个字节，帧开头的前 3 个字节表示帧数据（Frame Playload）的**长度**。

帧长度后面的一个字节是表示**帧的类型**，HTTP/2 总共定义了 10 种类型的帧，一般分为**数据帧**和**控制帧**两类，如下表格：

帧类型后面的一个字节是**标志位**，可以保存 8 个标志位，用于携带简单的控制信息，比如：

- **END_HEADERS** 表示头数据结束标志，相当于 HTTP/1 里头后的空行（“\r\n”）；
- **END_Stream** 表示单方向数据发送结束，后续不会再有数据帧。
- **PRIORITY** 表示流的优先级；

帧头的最后 4 个字节是**流标识符**（Stream ID），但最高位被保留不用，只有 31 位可以使用，因此流标识符的最大值是 2^31，大约是 21 亿，它的作用是用来标识该 Frame 属于哪个 Stream，接收方可以根据这个信息从乱序的帧里找到相同 Stream ID 的帧，从而有序组装信息。

最后面就是**帧数据**了，它存放的是通过 **HPACK 算法**压缩过的 HTTP 头部和包体

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408131024198.png" alt="image-20240105143150947 (1)" style="zoom:50%;" />

![image-20240105143208962](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408131023872.png)



##### *3. 并发传输*

针对不同的 HTTP 请求用独一无二的 Stream ID 来区分，接收端可以通过 Stream ID 有序组装成 HTTP 消息，不同 Stream 的帧是可以乱序发送的，因此可以并发不同的 Stream ，也就是 HTTP/2 可以并行交错地发送请求和响应。

**多个 Stream 复用一条 TCP 连接，达到并发的效果**

**不同 Stream 的帧是可以乱序发送的（因此可以并发不同的 Stream ）**，因为每个帧的头部会携带 Stream ID 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息，而**同一 Stream 内部的帧必须是严格有序的**。

![image-20240105143224839](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408131025237.png)

##### *4、服务器推送*

HTTP/2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务端不再是被动地响应，可以**主动**向客户端发送消息。

客户端和服务器**双方都可以建立 Stream**， Stream ID 也是有区别的，**客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号**。



#### 缺陷

HTTP/2 通过 Stream 的并发能力，解决了 HTTP/1 队头阻塞的问题，但是 HTTP/2 在 TCP 这一层还是存在“队头阻塞”的问题

HTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP/2 队头阻塞问题。

所以，一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的**所有的 HTTP 请求都必须等待这个丢了的包被重传回来**。

HTTP/2 多个 Stream 共用同一个 TCP 滑动窗口，那么当发生数据丢失，滑动窗口是无法往前移动的，此时就会阻塞住所有的 HTTP 请求，这属于 TCP 层队头阻塞。



 QUIC 给每一个 Stream 都分配了一个独立的滑动窗口，这样使得一个连接上的多个 Stream 之间没有依赖关系，都是相互独立的，各自控制的滑动窗口。





#### 和 HTTPS关系

HTTP/2 和 HTTPS 虽然经常一起使用，但它们不是同一个东西。HTTP/2 是一种更高效的传输协议，而 HTTPS 是一种安全通信协议。HTTP/2 可以通过 HTTPS 来实现安全的传输，但两者的功能和目的并不相同。

- **协同工作**：HTTP/2 可以在 HTTPS 上运行，也可以在传统的 HTTP 上运行。然而，大多数浏览器只支持通过 HTTPS 使用 HTTP/2。这意味着 HTTP/2 的实际使用通常与 HTTPS 紧密结合。
- **独立性**：HTTP/2 提供了性能改进，而 HTTPS 提供了安全性。HTTP/2 和 HTTPS 是互补的，但它们解决的问题不同：**HTTP/2 主要关注传输效率和性能，而 HTTPS 关注通信的安全性。**





### http3/QUIC

![ab3283383013b707d1420b6b4cb8517c](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011048659.png)

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011048276.jpeg" alt="img" style="zoom:50%;" />

#### Packet Header

Packet Header 细分这两种：

- Long Packet Header 用于首次建立连接。
- Short Packet Header 用于日常传输数据。

QUIC 也是需要三次握手来建立连接的，主要目的是为了**协商连接 ID**。

协商出连接 ID 后，后续传输时，双方只需要固定住连接 ID，从而实现连接迁移功能。所以，你可以看到日常传输数据的 Short Packet Header 不需要在传输 Source Connection ID 字段了，只需要传输 Destination Connection ID。



Short Packet Header 中的 `Packet Number` 是每个报文独一无二的编号，它是**严格递增**的，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值。

Packet Number 单调递增的两个好处：

- 可以更加精确计算 RTT，没有 TCP 重传的歧义性问题；
- 可以支持乱序确认，因为丢包重传将当前窗口阻塞在原地，而 TCP 必须是顺序确认的，丢包时会导致窗口不滑动

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011049144.jpeg" alt="bcf3ccb6a15c4cdebe1cd0527fdd9a5e" style="zoom:50%;" />



#### QUIC Frame Header

一个 Packet 报文中可以存放多个 QUIC Frame。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011052030.png" alt="6a94d41ef3d14cb6b7846e73da6c3104" style="zoom:50%;" />

每一个 Frame 都有明确的类型，针对类型的不同，功能也不同，自然格式也不同。

我这里只举例 Stream 类型的 Frame 格式，Stream 可以认为就是一条 HTTP 请求，它长这样：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011054025.jpeg" alt="536298d2c54a43b699026bffe0f85010" style="zoom:50%;" />

- Stream ID 作用：多个并发传输的 HTTP 消息，通过不同的 Stream ID 加以区别，类似于 HTTP2 的 Stream ID；
- Offset 作用：类似于 TCP 协议中的 Seq 序号，**保证数据的顺序性和可靠性**；
- Length 作用：指明了 Frame 数据的长度。



既然重传数据包的 Packet N+M 与丢失数据包的 Packet N 编号并不一致，我们怎么确定这两个数据包的内容一样呢？

所以引入 Frame Header 这一层，**通过 Stream ID + Offset 字段信息实现数据的有序性**，通过比较两个数据包的 Stream ID 与 Stream Offset ，如果都是一致，就说明这两个数据包的内容一致。

QUIC 通过**单向递增**的 Packet Number，配合 Stream ID 与 Offset 字段信息，可以支持**乱序确认**而不影响数据包的正确组装，摆脱了TCP 必须按顺序确认应答 ACK 的限制，解决了 TCP 因某个数据包重传而阻塞后续所有待发送数据包的问题

#### 乱序确认

超时重新编号再重发

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011111383.png" alt="_E4_B9_B1_E5_BA_8F_E7_A1_AE_E8_AE_A45" style="zoom:50%;" />



发送窗口唯一限制就是最大绝对字节偏移量，该值是接收方基于当前已经提交的偏移量（连续已确认并向上层应用提交的数据包offset）和发送方协商得出



`接收窗口 = 最大窗口数 - 接收到的最大偏移数`。

#### 没有队头阻塞的 QUIC

HTTP/2 多个 Stream 共用同一个 TCP 滑动窗口，那么当发生数据丢失，滑动窗口是无法往前移动的，此时就会阻塞住所有的 HTTP 请求，这属于 TCP 层队头阻塞。

 QUIC **给每一个 Stream 都分配了一个独立的滑动窗口**，这样使得一个连接上的多个 Stream 之间没有依赖关系，都是相互独立的，各自控制的滑动窗口。



#### 流量控制

QUIC 实现流量控制的方式：

- 通过 window_update 帧告诉对端自己可以接收的字节数，这样发送方就不会发送超过这个数量的数据。
- 通过 BlockFrame 告诉对端由于流量控制被阻塞了，无法发送数据。

UDP 没有流量控制，因此 QUIC 实现了自己的流量控制机制，QUIC 的滑动窗口滑动的条件跟 TCP 有一点差别，但是同一个 Stream 的数据也是要保证顺序的，不然无法实现可靠传输，因此同一个 Stream 的数据包丢失了，也会造成窗口无法滑动。



接收端口不同：

- TCP 的接收窗口只有在前面所有的 Segment 都接收的情况下才会移动左边界，当在前面还有字节未接收但收到后面字节的情况下，窗口也不会移动。
- QUIC 的接收窗口的左边界滑动条件取决于接收到的最大偏移字节数。`接收窗口 = 最大窗口数 - 接收到的最大偏移数`。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011104292.png" alt="77e9a7cf70da4a1b981f61e78db2ad56" style="zoom:50%;" />

当图中的绿色部分数据超过最大接收窗口的一半后，最大接收窗口向右移动，接收窗口的右边界也向右扩展，同时给对端发送「窗口更新帧」，当发送方收到接收方的窗口更新帧后，发送窗口的右边界也会往右扩展，以此达到窗口滑动的效果。

绿色部分的数据是已收到的顺序的数据，**如果中途丢失了数据包，导致绿色部分的数据没有超过最大接收窗口的一半，那接收窗口就无法滑动了**，这个只影响同一个 Stream



QUIC 实现了两种级别的流量控制，分别为 Stream 和 Connection 两种级别：

- **Stream 级别的流量控制**：Stream 可以认为就是一条 HTTP 请求，每个 Stream 都有独立的滑动窗口，所以每个 Stream 都可以做流量控制，防止单个 Stream 消耗连接（Connection）的全部接收缓冲。

- **Connection 流量控制**：限制连接中所有 Stream 相加起来的总字节数，防止发送方超过连接的缓冲容量

  - ```text
    可用窗口 = Stream 1 可用窗口 + Stream 2 可用窗口 + Stream 3 可用窗口
    ```
  
  - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011113555.png" alt="839501cffa7146cbb8d992264594e61d" style="zoom:50%;" />



#### 对拥塞控制改进

QUIC 协议当前默认使用了 TCP 的 Cubic 拥塞控制算法（我们熟知的慢开始、拥塞避免、快重传、快恢复策略），同时也支持 CubicBytes、Reno、RenoBytes、BBR、PCC 等拥塞控制算法，相当于将 TCP 的拥塞控制算法照搬过来了。



**QUIC 可以随浏览器更新，QUIC 的拥塞控制算法就可以有较快的迭代速度**。

传统TCP需要内核支持，迭代较慢。但是因为 QUIC 处于**应用层**，所以就**可以针对不同的应用设置不同的拥塞控制算法**，这样灵活性就很高了。



#### 更快的连接建立和迁移

连接

对于 HTTP/1 和 HTTP/2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层

HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是**QUIC 内部包含了 TLS**，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 **TLS1.3**，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011129894.png" alt="4cad213f5125432693e0e2a512c2d1a1" style="zoom:67%;" />

迁移

QUIC 协议没有用四元组的方式来“绑定”连接，而是通过**连接 ID**来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了**连接迁移**的功能。





#### 优化

QUIC（Quick UDP Internet Connections）是一个基于**UDP**的传输协议，旨在提供比传统TCP更快、更高效的网络连接。

##### 1. 无队头阻塞
- **多路复用与独立流处理**：QUIC与HTTP/2一样，支持多路复用（Multiplexing），可以在同一连接上并行处理多个流（Stream）。但与HTTP/2不同的是，QUIC可以在一个流发生丢包时，仅阻塞该流，而不影响其他流。这种独立处理的机制消除了HTTP/2中由于TCP的顺序传输导致的队头阻塞（Head-of-Line Blocking）问题，使得数据传输更加高效。
##### 2. 更快的连接建立
- **合并握手流程**：在HTTP/1和HTTP/2中，TCP和TLS的握手是分层的，通常需要两个RTT（Round-Trip Time）才能完成。

  而在HTTP/3中，QUIC协议将**TLS集成到自身内部，并使用TLS/1.3协议**。这意味着QUIC可以在**1个RTT**内同时完成连接建立和密钥协商，从而大大加快了连接速度。

- **0-RTT重连**：QUIC支持在第二次连接时实现0-RTT，允许客户端在发送握手信息的同时传输应用数据。这样可以进一步减少延迟，提升用户体验。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408112301350.jpeg" alt="28-HTTP3_E4_BA_A4_E4_BA_92_E6_AC_A1_E6_95_B0" style="zoom:80%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408112301446.png" alt="4cad213f5125432693e0e2a512c2d1a1-20230309231022316" style="zoom: 67%;" />

##### 3. 连接迁移
- **通过连接ID实现无缝迁移**：传统的TCP连接依赖于四元组（源IP、源端口、目的IP、目的端口）来标识连接。一旦网络条件变化（如设备从4G切换到WiFi），IP地址变化就会导致连接断开，必须重新建立。
- 而QUIC使用连接ID（Connection ID）来标识连接，使得即使IP地址发生变化，连接也可以继续保持。这种特性使得QUIC特别适合于移动网络环境中，无需断开和重新建立连接，减少了延迟和卡顿感。

##### 4.HTTP/3 协议

HTTP/3 在头部压缩算法这一方面也做了升级，升级成了 **QPACK**

HTTP/3 中的 QPACK 也采用了静态表、动态表及 Huffman 编码。

HTTP/3 中的 QPACK 的静态表扩大到 91 项。



两个特殊的单向流用来**同步双方的动态表**，编码方收到解码方更新确认的通知后，才使用动态表编码 HTTP 头部

QUIC 会有两个特殊的单向流，所谓的单向流只有一端可以发送消息，双向则指两端都可以发送消息，传输 HTTP 消息时用的是双向流，这两个单向流的用法：

- 一个叫 QPACK Encoder Stream，用于将一个字典（Key-Value）传递给对方，比如面对不属于静态表的 HTTP 请求头部，客户端可以通过这个 Stream 发送字典；
- 一个叫 QPACK Decoder Stream，用于响应对方，告诉它刚发的字典已经更新到自己的本地动态表了，后续就可以使用这个字典来编码了。



<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408131123341.png" alt="image-20240105144457456 (1)" style="zoom:50%;" />



#### 额外注意事项

- **基于UDP的挑战**：QUIC在UDP之上构建，与传统的TCP不同。一些网络设备和防火墙可能不识别QUIC，并将其视为普通的UDP流量，甚至可能丢弃这些包。这是QUIC面临的一个挑战，也影响了HTTP/3的普及速度。





### FTP

（File Transfer Protocol，文件传输协议）是一种用于在网络上进行文件传输的标准协议。它通常用于在客户端和服务器之间传输文件，特别是在局域网或互联网上。FTP的主要功能包括上传文件、下载文件、创建目录、删除文件和目录、查看文件列表等。

#### FTP的基本概念

1. **客户端和服务器模型**：FTP基于客户端-服务器模型。FTP客户端是发起文件传输请求的一方，而FTP服务器则是提供文件传输服务的一方。

2. **传输模式**：FTP支持两种传输模式：
   - **主动模式**（Active Mode）：在主动模式下，客户端向服务器发送指令后，服务器会主动连接到客户端指定的端口来传输数据。
   - **被动模式**（Passive Mode）：在被动模式下，客户端连接到服务器指定的端口来进行数据传输，这种模式更适合在客户端处于防火墙或NAT后面的情况下使用。

3. **数据传输方式**：FTP支持两种主要的数据传输方式：
   - **ASCII模式**：适用于文本文件传输，在传输过程中，系统会对文件中的行尾标志进行转换。
   - **二进制模式**：适用于所有类型的文件传输，文件会逐字节传输，不会对数据做任何修改。

4. **认证机制**：FTP通常要求用户提供用户名和密码进行身份验证。也可以设置匿名FTP，允许用户以匿名身份（通常使用`anonymous`作为用户名）进行访问。

#### FTP的工作过程

1. **建立连接**：客户端通过指定FTP服务器的IP地址或域名，连接到服务器的21号端口（控制端口）。
   
2. **身份验证**：客户端提供用户名和密码进行身份验证。如果通过，客户端就能访问服务器上的文件和目录。

3. **文件传输**：客户端可以根据需要上传、下载文件，或者进行其他文件操作（如删除文件、创建目录等）。

4. **关闭连接**：完成文件传输后，客户端可以关闭与服务器的连接。

#### 安全性问题

传统的FTP协议在传输过程中不加密数据，这意味着传输的用户名、密码和文件内容都是明文传输，容易被中间人攻击拦截。为了提高安全性，可以使用FTPS（FTP Secure）或SFTP（SSH File Transfer Protocol）：
- **FTPS**：在FTP的基础上加入了SSL/TLS加密。
- **SFTP**：基于SSH协议，提供加密的文件传输功能，且与FTP不兼容。

FTP是一种广泛使用的文件传输协议，尽管在现代网络中它逐渐被更加安全的协议所取代，但在某些环境下，FTP仍然是一个常见的文件传输方式。



------

## 6.**表示层**

**（Presentation Layer）**：处理数据的表示形式，如数据加密、解密、压缩等。

- 表示层，负责把数据转换成兼容另一个系统能识别的格式；
- 会话层，负责建立、管理和终止表示层实体之间的通信会话；

------



## 5.**会话层**

**（Session Layer）**：管理通信会话，负责建立、维护和终止会话。

------



## 4.传输层

端口到端口



###  TCP 

#### 介绍

传输控制协议（*Transmission Control Protocol*），传输单位是段（segment）

**面向连接的、可靠的、基于字节流**

缺点：

- 升级 TCP 的工作很困难；
  -  TCP 协议是在内核中实现的，应用程序只能使用不能修改，如果要想升级 TCP 协议，那么只能升级内核。
- TCP 建立连接的延迟；
- TCP 存在队头阻塞问题；
- 网络迁移需要重新建立 TCP 连接；



#### 数据包发送流程

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011248746.png" alt="28e4d6b004530fbf75fe346d181baa81" style="zoom: 67%;" />

为了发送数据包，两端首先会通过**三次握手**，建立TCP连接。

一个数据包，从聊天框里发出，消息会从**聊天软件**所在的**用户空间**拷贝到**内核空间**的**发送缓冲区（send buffer）**，数据包就这样顺着**传输层、网络层，进入到数据链路层，在这里数据包会经过流控（qdisc），再通过RingBuffer发到物理层的网卡**。数据就这样顺着**网卡**发到了**纷繁复杂**的网络世界里。这里头数据会经过n多个**路由器和交换机**之间的跳转，最后到达**目的机器的网卡**处。

此时目的机器的网卡会通知**DMA**将数据包信息放到`RingBuffer`中，再触发一个**硬中断**给`CPU`，`CPU`触发**软中断**让`ksoftirqd`去`RingBuffer`收包，于是一个数据包就这样顺着**物理层，数据链路层，网络层，传输层**，最后从内核空间拷贝到用户空间里的**聊天软件**里。



#### 什么是 TCP 连接？

用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 Socket、序列号和窗口大小称为连接。

- **Socket**：由 IP 地址和端口号组成

- **序列号**：用来解决乱序问题等

- **窗口大小**：用来做流量控制

  

TCP 四元组可以唯一的确定一个连接，四元组包括如下：

- 源地址
- 源端口
- 目的地址
- 目的端口

源地址和目的地址的字段（32 位）是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机。
源端口和目的端口的字段（16 位）是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程。



##### 连接流程

1. 服务端和客户端初始化 `socket`，得到文件描述符；
2. 服务端调用 `bind`，将 socket 绑定在指定的 IP 地址和端口;
3. 服务端调用 `listen`，进行监听；
4. 服务端调用 `accept`，等待客户端连接；
5. 客户端调用 `connect`，向服务端的地址和端口发起连接请求；
6. 服务端 `accept` 返回用于传输的 `socket` 的文件描述符；
7. 客户端调用 `write` 写入数据；服务端调用 `read` 读取数据；
8. 客户端断开连接时，会调用 `close`，那么服务端 `read` 读取数据的时候，就会读取到了 `EOF`，待处理完数据后，服务端调用 `close`，表示连接关闭。

这里需要注意的是，服务端调用 `accept` 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。

所以，监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作**监听 socket**，一个叫作**已完成连接 socket**。

listen 时候参数 backlog 

```c
int listen (int socketfd, int backlog)
```

- 参数一 socketfd 为 socketfd 文件描述符

- 参数二 backlog，这参数在历史版本有一定的变化

- 现在通常认为 backlog 是 accept 队列。

  但是上限值是内核参数 somaxconn 的大小，也就说 **accpet 队列长度 = min(backlog, somaxconn)。**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408141133965.png" alt="format,png-20230309230545997" style="zoom:50%;" />



#### TCP 包头格式

**序**号：解决包乱序的问题。

**确认号**：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。**用来解决丢包的问题。**

**状态位**。例如 `SYN` 发起一个连接，`ACK` 回复，`RST` 重新连接，`FIN` 结束连接等。

TCP 是面向连接的，因而双方要维护连接的状态，这些带状态位的包的发送，会引起双方的状态变更。

传输层的数据包大小超过 **MSS（TCP 最大报文段长度）** ，就要将**数据包分块**，这样即使中途有一个分块丢失或损坏了，只需要重新发送这一个分块，而不用重新发送整个数据包。在 TCP 协议中，我们把每个分块称为一个 **TCP 段**（*TCP Segment*）。



<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090828694.png" alt="8" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408162051645.png" alt="TCP_20option_E5_AD_97_E6_AE_B5_20-_20TFO" style="zoom: 50%;" />

#### 三次握手

##### 目的

1. **保证双方都有发送和接收的能力**
2. **能防止历史连接的建立，**
3. **能减少双方不必要的资源开销，**
4. **能帮助双方同步初始化序列号**。序列号能够保证数据包不重复、不丢弃和按序传输。

不使用「两次握手」和「四次握手」的原因：

- 「两次握手」：无法防止历史连接的建立，会造成双方资源的浪费，也无法可靠的同步双方序列号；
- 「四次握手」：三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数



##### 流程

syn---*Synchronize Sequence Numbers*（同步序列号）

- 一开始，客户端和服务端都处于 `CLOSED` 状态。先是服务端主动监听某个端口，处于 `LISTEN` 状态。
- 然后客户端主动发起连接 `SYN`，之后处于 `SYN-SENT` 状态。
- 服务端收到发起的连接，返回 `SYN`，并且 `ACK` 客户端的 `SYN`，之后处于 `SYN-RCVD` 状态。
- 客户端收到服务端发送的 `SYN` 和 `ACK` 之后，发送对 `SYN` 确认的 `ACK`，之后处于 `ESTABLISHED` 状态，因为它一发一收成功了。
- 服务端收到 `ACK` 的 `ACK` 之后，处于 `ESTABLISHED` 状态，因为它也一发一收了。



SYN： 标志位被设置时，它表示请求方希望与目标建立一个新的 TCP 连接。

SEQ：每个 TCP 报文段都会包含一个序列号，用于表示该报文段中的第一个字节在整个 TCP 数据流中的位置。

ACK：（Acknowledgment Number）用于确认接收到的数据包，并告知发送方下一个期望的数据包序列号。值等于已成功接收的数据的最后一个字节序列号加 1。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090833386.png" alt="TCP_E4_B8_89_E6_AC_A1_E6_8F_A1_E6_89_8B.drawio" style="zoom: 50%;" />

**服务端如果只 bind 了 IP 地址和端口，而没有调用 listen 的话，然后客户端对服务端发起了连接建立，服务端会回 RST 报文。**



> 不使用 listen ，可以建立 TCP 连接吗？

答案，**是可以的，**

1. 客户端是可以自己连自己的形成连接（TCP自连接），
2. 两个客户端同时向对方发出请求建立连接（TCP同时打开）

> 那没有listen，为什么还能建立连接？

我们知道执行 listen 方法时，会创建半连接队列和全连接队列。

三次握手的过程中会在这两个队列中暂存连接信息。

所以形成连接，前提是你得有个地方存放着，方便握手的时候能根据 IP + 端口等信息找到对应的 socket。

内核有个**全局hash表**，可以用于存放`sock`连接的信息。这个全局`hash`表其实还细分为`ehash，bhash和listen_hash`等，但因为过于细节，大家理解成有一个**全局hash**就够了，

在TCP自连接的情况中，客户端在`connect`方法时，最后会将自己的连接信息放入到这个**全局hash表**中，然后将信息发出，消息在经过回环地址重新回到TCP传输层的时候，就会根据IP端口信息，再一次从这个**全局hash**中取出信息。于是握手包一来一回，最后成功建立连接。

TCP 同时打开的情况也类似，只不过从一个客户端变成了两个客户端而已。



TCP 同步打开

当 TCP 连接双方同时发起连接请求时，会发生一种特殊的情形，称为 **TCP 同步打开（TCP Simultaneous Open）**。

最终建立的是一个TCP连接，而不是两个，这点要特别注意。

此时我们没有将任何一端称为客户或[服务器](https://cloud.tencent.com/act/pro/promotion-cvm?from_column=20065&from=20065)，因为每一端既是客户又是服务器。



##### TCP Fast Open (快速重连)

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408162050725.png" alt="22" style="zoom:50%;" />

1. 客户端发送 SYN 报文，该报文包含「数据」（对于非 TFO 的普通 TCP 握手过程，SYN 报文中不包含「数据」）以及此前记录的 Cookie；
2. 支持 TCP Fast Open 的服务器会对收到 Cookie 进行校验：如果 Cookie 有效，服务器将在 SYN-ACK 报文中对 SYN 和「数据」进行确认，服务器随后将「数据」递送至相应的应用程序；如果 Cookie 无效，服务器将丢弃 SYN 报文中包含的「数据」，且其随后发出的 SYN-ACK 报文将只确认 SYN 的对应序列号；
3. 如果服务器接受了 SYN 报文中的「数据」，服务器可在握手完成之前发送「数据」，**这就减少了握手带来的 1 个 RTT 的时间消耗**；
4. 客户端将发送 ACK 确认服务器发回的 SYN 以及「数据」，但如果客户端在初始的 SYN 报文中发送的「数据」没有被确认，则客户端将重新发送「数据」；
5. 此后的 TCP 连接的数据传输过程和非 TFO 的正常情况一致。





<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408162052605.png" alt="23" style="zoom:50%;" />

tcp_fastopn 各个值的意义:

- 0 关闭
- 1 作为客户端使用 Fast Open 功能
- 2 作为服务端使用 Fast Open 功能
- 3 无论作为客户端还是服务器，都可以使用 Fast Open 功能

**TCP Fast Open 功能需要客户端和服务端同时支持，才有效果**



##### 连接队列

Linux 内核会维护两个队列

- **半连接队列（SYN队列）**，服务端收到**第一次握手**后，会将`sock`加入到这个队列中，队列内的`sock`都处于`SYN_RECV` 状态。
- **全连接队列（ACCEPT队列）**，在服务端收到**第三次握手**后，会将半连接队列的`sock`取出，放到全连接队列中。队列里的`sock`都处于 `ESTABLISHED`状态。这里面的连接，就**等着服务端执行accept()后被取出了**
  - `accept方法`只是为了从全连接队列中拿出一条连接，本身跟三次握手几乎**毫无关系**

虽然都叫**队列**，但其实出于**效率**考虑，**半连接队列（syn_table）是个哈希表**，**全连接队列（icsk_accept_queue）是个链表**

​	如果半连接队列还是个链表，那我们就需要依次遍历，才能拿到我们想要的那个连接，算法复杂度就是O(n)。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161706274.png" alt="3" style="zoom:50%;" />



###### TCP 全连接队列

最大值取决于 somaxconn 和 backlog 之间的最小值，也就是 **min(somaxconn, backlog)**

发生 TCP 全连接队溢出的时候，后续的请求就会被丢弃，这样就会出现服务端请求数量上不去的现象。



> Linux 有个参数可以指定当 TCP 全连接队列满了会使用什么策略来回应客户端。

实际上，丢弃连接只是 Linux 的默认行为，我们还可以选择向客户端发送 RST 复位报文，告诉客户端连接已经建立失败。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408162016970.png" alt="12" style="zoom: 33%;" />

tcp_abort_on_overflow 共有两个值分别是 0 和 1，其分别表示：

- 0 ：如果全连接队列满了，那么 server 扔掉 client 发过来的 ack ；
- 1 ：如果全连接队列满了，server 发送一个 `reset` 包给 client，表示废掉这个握手过程和这个连接；

如果要想知道客户端连接不上服务端，是不是服务端 TCP 全连接队列满的原因，那么可以把 tcp_abort_on_overflow 设置为 1，这时如果在客户端异常中可以看到很多 `connection reset by peer` 的错误，那么就可以证明是由于服务端 TCP 全连接队列溢出的问题。



###### TCP 半连接队列溢出

要想增大半连接队列，**不能只单纯增大 tcp_max_syn_backlog 的值，还需一同增大 somaxconn 和 backlog，也就是增大 accept 队列。**



开启 **syncookies** 功能就可以在不使用 SYN 半连接队列的情况下成功建立连接

syncookies 参数主要有以下三个值：

- 0 值，表示关闭该功能；
- 1 值，表示仅当 SYN 半连接队列放不下时，再启用它；
- 2 值，表示无条件开启功能；

`cookies`并不会有一个专门的队列保存，它是通过**通信双方的IP地址端口、时间戳、MSS**等信息进行**实时计算**的，保存在**TCP报头**的`seq`里。

坏处：

凡事皆有利弊，`cookies`方案虽然能防 **SYN Flood攻击**，但是也有一些问题。

1.因为服务端并不会保存连接信息，所以如果传输过程中数据包丢了，也不会重发第二次握手的信息。

2.编码解码`cookies`，比较**耗CPU**的，利用这一点，如果此时攻击者构造大量的**第三次握手包（ACK包）**，同时带上各种瞎编的`cookies`信息，服务端收到`ACK包`后**以为是正经cookies**，憨憨地跑去解码（**耗CPU**），最后发现不是正经数据包后才丢弃。

这种通过构造大量`ACK包`去消耗服务端资源的攻击，叫**ACK攻击**，受到攻击的服务器可能会因为**CPU资源耗尽**导致没能响应正经请求。





######  SYN 攻击

攻击者短时间伪造不同 IP 地址的 `SYN` 报文，服务端每接收到一个 `SYN` 报文，就进入`SYN_RCVD` 状态，但服务端发送出去的 `ACK + SYN` 报文，无法得到未知 IP 主机的 `ACK` 应答，久而久之就会**占满服务端的半连接队列**，使得服务端不能为正常用户服务。

避免 SYN 攻击方式，可以有以下四种方法：

- 调大 netdev_max_backlog；
- **增大 TCP 半连接队列；**
- **开启 tcp_syncookies；**
- **减少 SYN+ACK 重传次数**

自定义设计防御方案：

如果不使用现有方案，可以尝试以下自定义设计思路：

- **挑战-响应机制**：在收到 SYN 请求时，不立即返回 SYN-ACK，而是发送一个加密挑战消息（如验证码）。只有挑战通过的 IP 才继续建立连接。
- **多级验证**：分级建立连接，如第一次握手时通过外部代理或中间节点过滤，只有通过初级验证的 IP 才能与主服务器建立连接。



#####  SYN 报文被丢弃

两种场景：

- 开启 tcp_tw_recycle 参数，并且在 NAT 环境下，造成 SYN 报文被丢弃
- TCP 两个队列满了（半连接队列和全连接队列），造成 SYN 报文被丢弃



##### 已建立连接的TCP，收到SYN会发生什么？

TCP 连接是由「四元组」唯一确认的。

**1. 客户端的 SYN 报文里的端口号与历史连接不相同**

服务端会认为是新的连接要建立，于是就会通过三次握手来建立新的连接。



旧连接里处于 Established 状态的服务端最后会怎么样呢？

如果服务端发送了数据包给客户端，由于客户端的连接已经被关闭了，此时客户的内核就会回 RST 报文，服务端收到后就会释放连接。

如果服务端一直没有发送数据包给客户端，在超过一段时间后，TCP 保活机制就会启动，检测到客户端没有存活后，接着服务端就会释放掉该连接。

**2. 客户端的 SYN 报文里的端口号与历史连接相同**

处于 Established 状态的服务端，如果收到了客户端的 SYN 报文（注意此时的 SYN 报文其实是乱序的，因为 SYN 报文的初始化序列号其实是一个随机数），会回复一个携带了正确序列号和确认号的 ACK 报文，这个 ACK 被称之为 Challenge ACK。

接着，客户端收到这个 Challenge ACK，发现确认号（ack num）并不是自己期望收到的，于是就会回 RST 报文，服务端收到后，就会释放掉该连接







> 如何查看 TCP 半连接队列长度？

![21](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408162021236.png)

#### 四次挥手

关闭连接的方式通常有两种，分别是 **RST 报文关闭和 FIN 报文关闭**。

如果进程收到 RST 报文，就直接关闭连接了，不需要走四次挥手流程，是一个暴力关闭连接的方式。

安全关闭连接的方式必须通过四次挥手，它由进程调用 `close` 和 `shutdown` 函数发起 FIN 报文



##### 流程

- 客户端打算关闭连接，此时会发送一个 TCP 首部 `FIN` 标志位被置为 `1` 的报文，也即 `FIN` 报文，之后客户端进入 `FIN_WAIT_1` 状态。
- 服务端收到该报文后，就向客户端发送 `ACK` 应答报文，接着服务端进入 `CLOSE_WAIT` 状态。
  - 服务端接收到了 FIN 报文，TCP 协议栈会为 FIN 包插入一个文件结束符 `EOF`（文件的结束标志） 到接收缓冲区中，应用程序可以通过 `read` 调用来感知这个 FIN 包。这个 `EOF` 会被**放在已排队等候的其他已接收的数据之后**，这就意味着服务端需要处理这种异常情况，因为 EOF 表示在该连接上再无额外数据到达。此时，服务端进入 CLOSE_WAIT 状态；
- 客户端收到服务端的 `ACK` 应答报文后，之后进入 `FIN_WAIT_2` 状态。
- 等待服务端处理完数据后（调用 close 函数），也向客户端发送 `FIN` 报文，之后服务端进入 `LAST_ACK` 状态。
- 客户端收到服务端的 `FIN` 报文后，回一个 `ACK` 应答报文，之后进入 `TIME_WAIT` 状态
- 服务端收到了 `ACK` 应答报文后，就进入了 `CLOSE` 状态，至此服务端已经完成连接的关闭。
- 客户端在经过 `2MSL` 一段时间后，自动进入 `CLOSE` 状态，至此客户端也完成连接的关闭。

你可以看到，每个方向都需要**一个 FIN 和一个 ACK**，因此通常被称为**四次挥手**。

这里一点需要注意是：**主动关闭连接的，才有 TIME_WAIT 状态**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408140903172.png" alt="format,png-20230309230614791" style="zoom:50%;" />

##### 为什么挥手需要四次为什么挥手需要四次？

- 关闭连接时，客户端向服务端发送 `FIN` 时，仅仅表示客户端不再发送数据了但是还能接收数据。
- 服务端收到客户端的 `FIN` 报文时，先回一个 `ACK` 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 `FIN` 报文给客户端来表示同意现在关闭连接。

从上面过程可知，**服务端通常需要等待完成数据的发送和处理**，所以服务端的 `ACK` 和 `FIN` 一般都会分开发送，因此是需要四次挥手。





###### 问题

被问到既然打开 net.ipv4.tcp_tw_reuse 参数可以快速复用处于 TIME_WAIT 状态的 TCP 连接，那为什么 Linux 默认是关闭状态呢？其实这题在变相问「**如果 TIME_WAIT 状态持续时间过短或者没有，会有什么问题？**」



##### 挥手丢失

###### 第一次

如果第一次挥手丢失了，那么客户端迟迟收不到被动方的 ACK 的话，也就会触发超时重传机制，重传 FIN 报文，重发次数由 `tcp_orphan_retries` 参数控制。

当**客户端**重传 FIN 报文的**次数超过 `tcp_orphan_retries`** 后，就不再发送 FIN 报文，则会在等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到第二次挥手，那么**直接进入到 `close` 状态**。

###### 第二次

当服务端收到客户端的第一次挥手后，就会先回一个 ACK 确认报文，此时服务端的连接进入到 `CLOSE_WAIT` 状态。

- **服务端：ACK 报文是不会重传的**
- **客户端**：触发超时重传机制，重传 FIN 报文，直到收到服务端的第二次挥手，或者达到最大的重传次数。

###### 第三次

- 当**服务端**重传第三次挥手报文的次数超过了 tcp_orphan_retries = 3，如果还是没能收到客户端的第四次挥手（ACK报文），那么服务端就会断开连接。
- **客户端**因为是通过 close 函数关闭连接的，处于 FIN_WAIT_2 状态是有时长限制的，如果 **tcp_fin_timeout**（默认值是 60 秒） 时间内还是没能收到服务端的第三次挥手（FIN 报文），那么客户端就会断开连接。

###### 第四次

如果第四次挥手的 ACK 报文没有到达服务端，

- **服务端**就会重发 FIN 报文，重发次数仍然由前面介绍过的 `tcp_orphan_retries` 参数控制。
- **客户端**在收到第三次挥手后，就会进入 TIME_WAIT 状态，开启时长为 2MSL（`MSL` 是 Maximum Segment Lifetime，**报文最大生存时间**） 的定时器，如果途中再次收到第三次挥手（FIN 报文）后，就会重置定时器，当等待 2MSL 时长后，客户端就会断开连接。



**MSL 应该要大于等于 TTL 消耗为 0 的时间**，以确保报文已被自然消亡。

**TTL 的值一般是 64，Linux 将 MSL 设置为 30 秒，意味着 Linux 认为数据报文经过 64 个路由器的时间不会超过 30 秒，如果超过了，就认为报文已经消失在网络中了**。看到 **2MSL时长** 这其实是相当于**至少允许报文丢失一次**

`2MSL` 的时间是从**客户端接收到 FIN 后发送 ACK 开始计时的**



##### 为什么需要 TIME_WAIT 状态？

- 防止历史连接中的数据，被后面相同四元组的连接错误的接收；

  这个时间足以**让两个方向上的数据包都被丢弃，**使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。

- 保证「被动关闭连接」的一方，能被正确的关闭；




###### TIME_WAIT 过多有什么危害？

过多的 TIME-WAIT 状态主要的危害有两种：

- 第一是占用系统资源，比如文件描述符、内存资源、CPU 资源、线程资源等；
- 第二是占用端口资源，端口资源也是有限的，一般可以开启的端口为 `32768～61000`，也可以通过 `net.ipv4.ip_local_port_range`参数指定范围。

###### 如何优化 TIME_WAIT？

这里给出优化 TIME-WAIT 的几个方式，都是有利有弊：

- 打开 net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps 选项；
- net.ipv4.tcp_max_tw_buckets
- 程序中使用 SO_LINGER ，应用强制使用 RST 关闭。



###### 服务器出现大量 TIME_WAIT 状态的原因有哪些？

首先要知道 TIME_WAIT 状态是主动关闭连接方才会出现的状态，所以如果服务器出现大量的 TIME_WAIT 状态的 TCP 连接，就是说明服务器主动断开了很多 TCP 连接。

问题来了，**什么场景下服务端会主动断开连接呢？**

- 第一个场景：HTTP 没有使用长连接

  **当服务端出现大量的 TIME_WAIT 状态连接的时候，可以排查下是否客户端和服务端都开启了 HTTP Keep-Alive**，因为任意一方没有开启 HTTP Keep-Alive，都会导致服务端在处理完一个 HTTP 请求后，就主动关闭连接，此时服务端上就会出现大量的 TIME_WAIT 状态的连接。

- 第二个场景：HTTP 长连接超时

- 第三个场景：HTTP 长连接的请求数量达到上限

   nginx 的 keepalive_requests 这个参数，**如果达到这个参数设置的最大值时，则 nginx 会主动关闭这个长连接**，那么此时服务端上就会出现 TIME_WAIT 状态的连接。调大 nginx 的 keepalive_requests 参数
   
   

###### 服务器出现大量 CLOSE_WAIT 状态的原因有哪些？

**当服务端出现大量 CLOSE_WAIT 状态的连接的时候，说明服务端的程序没有调用 close 函数关闭连接**。**通常都是代码的问题**



###### 如果已经建立了连接，但是客户端突然出现故障了怎么办？

启用TCP 的**保活机制**

 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：

```shell
net.ipv4.tcp_keepalive_time=7200   //s
net.ipv4.tcp_keepalive_intvl=75  
net.ipv4.tcp_keepalive_probes=9
```



###### 如果已经建立了连接，但是**服务端的进程**崩溃会发生什么？

TCP 的连接信息是由内核维护的,于是内核会发送第一次挥手 FIN 报文，后续的挥手过程也都是在内核完成，并不需要进程的参与

**服务端会发送 FIN 报文，与客户端进行四次挥手**。



##### 伪造一个能关闭 TCP 连接的 RST 报文，

必须同时满足**「四元组相同」和「序列号是对方期望的」**这两个条件。

###### killcx 的工具

​	killcx 工具是主动发送一个 SYN 报文，对方收到后会回复一个携带了正确序列号和确认号的 ACK 报文，这个 ACK 被称之为 Challenge ACK，这时就可以拿到对方下一次期望收到的序列号，然后将序列号填充到伪造的 RST 报文，并将其发送给对方，达到关闭 TCP 连接的效果。

​	**无论 TCP 连接是否活跃，都可以关闭**

###### tcpkill 的工具

- tcpkill 工具是在双方进行 TCP 通信时，拿到对方下一次期望收到的序列号，然后将序列号填充到伪造的 RST 报文，并将其发送给对方，达到关闭 TCP 连接的效果。
- **这种方式无法关闭非活跃的 TCP 连接**



#####  四次挥手中收到乱序的 FIN 包

在 FIN_WAIT_2 状态时，如果收到乱序的 FIN 报文，那么就被会加入到**「乱序队列」**，并不会进入到 TIME_WAIT 状态。

等再次收到前面被网络延迟的数据包时，会判断乱序队列有没有数据，然后会检测乱序队列中是否有可用的数据，如果能在乱序队列中找到与当前报文的序列号保持的顺序的报文，就会看该报文是否有 FIN 标志，如果发现有 FIN 标志，这时才会进入 TIME_WAIT 状态。

![watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_20,color_FFFFFF,t_70,g_se,x_16-20230309230147654](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272013667.png)



##### 在 TIME_WAIT 状态的 TCP 连接，收到 SYN 后会发生什么？

**关键是要看 SYN 的「序列号和时间戳」是否合法**，

- **合法 SYN**：客户端的 SYN 的「序列号」比服务端「期望下一个收到的序列号」要**大**，**并且** SYN 的「时间戳」比服务端「最后收到的报文的时间戳」要**大**。
- **非法 SYN**：客户端的 SYN 的「序列号」比服务端「期望下一个收到的序列号」要**小**，**或者** SYN 的「时间戳」比服务端「最后收到的报文的时间戳」要**小**。

上面 SYN 合法判断是基于双方都开启了 TCP 时间戳机制的场景，如果双方都没有开启 TCP 时间戳机制，则 SYN 合法判断如下：

- **合法 SYN**：客户端的 SYN 的「序列号」比服务端「期望下一个收到的序列号」要**大**。
- **非法 SYN**：客户端的 SYN 的「序列号」比服务端「期望下一个收到的序列号」要**小**

 收到合法 SYN

如果处于 TIME_WAIT 状态的连接收到「合法的 SYN 」后，**就会重用此四元组连接，跳过 2MSL 而转变为 SYN_RECV 状态，接着就能进行建立连接过程**。

 收到非法的 SYN

如果处于 TIME_WAIT 状态的连接收到「非法的 SYN 」后，就会**再回复一个第四次挥手的 ACK 报文，客户端收到后，发现并不是自己期望收到确认号（ack num），就回 RST 报文给服务端**。



##### 客户端出现问题

1. 如果「**客户端进程崩溃**」，客户端的进程在发生崩溃的时候，内核会发送 FIN 报文，与服务端进行四次挥手。

2. 但是，「**客户端主机宕机**」，那么是不会发生四次挥手的，具体后续会发生什么？还要看服务端会不会发送数据？

   - 如果服务端会发送数据，由于客户端已经不存在，收不到数据报文的响应报文，服务端的数据报文会超时重传，当重传总间隔时长达到一定阈值（内核会根据 tcp_retries2 设置的值计算出一个阈值）后，会断开 TCP 连接；
   - 如果服务端一直不会发送数据，再看服务端有没有开启 TCP keepalive 机制？
     - 如果有开启，服务端在一段时间没有进行数据交互时，会触发 TCP keepalive 机制，探测对方是否存在，如果探测到对方已经消亡，则会断开自身的 TCP 连接；
     - 如果没有开启，服务端的 TCP 连接会一直存在，并且一直保持在 ESTABLISHED 状态。

3. **客户端拔掉网线**后，并不会直接影响 TCP 连接状态。所以，拔掉网线后，TCP 连接是否还会存在，关键要看拔掉网线之后，有没有进行数据传输。

   有数据传输的情况：

   - 在客户端拔掉网线后，如果服务端发送了数据报文，那么在服务端重传次数没有达到最大值之前，客户端就插回了网线，那么双方原本的 TCP 连接还是能正常存在，就好像什么事情都没有发生。
   - 在客户端拔掉网线后，如果服务端发送了数据报文，在客户端插回网线之前，服务端重传次数达到了最大值时，服务端就会断开 TCP 连接。等到客户端插回网线后，向服务端发送了数据，因为服务端已经断开了与客户端相同四元组的 TCP 连接，所以就会回 RST 报文，客户端收到后就会断开 TCP 连接。至此， 双方的 TCP 连接都断开了。

   没有数据传输的情况：

   - 如果双方都没有开启 TCP keepalive 机制，那么在客户端拔掉网线后，如果客户端一直不插回网线，那么客户端和服务端的 TCP 连接状态将会一直保持存在。
   - 如果双方都开启了 TCP keepalive 机制，那么在客户端拔掉网线后，如果客户端一直不插回网线，TCP keepalive 机制会探测到对方的 TCP 连接没有存活，于是就会断开 TCP 连接。而如果在 TCP 探测期间，客户端插回了网线，那么双方原本的 TCP 连接还是能正常存在。



##### 三次挥手

当被动关闭方在 TCP 挥手过程中，「**没有数据要发送」并且「开启了 TCP 延迟确认机制」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手**

>  TCP 延迟确认机制

当发送没有携带数据的 ACK，它的网络效率也是很低的，因为它也有 40 个字节的 IP 头 和 TCP 头，但却没有携带数据报文。 为了解决 ACK 传输效率低问题，所以就衍生出了 **TCP 延迟确认**。 TCP 延迟确认的策略：

- 当有响应数据要发送时，ACK 会随着响应数据一起立刻发送给对方
- 当没有响应数据要发送时，ACK 将会延迟一段时间，以等待是否有响应数据可以一起发送
- 如果在延迟等待发送 ACK 期间，对方的第二个数据报文又到达了，这时就会立刻发送 ACK

------



#### 流量控制

​	TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。



##### 滑动窗口

**累计确认**或者**累计应答**。

接收方向发送方通告窗口大小时，是通过 `ACK` 报文来通告的。

​	TCP 头里有一个字段叫 `Window`，也就是窗口大小。这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。所以，通常窗口的大小是由接收方的窗口大小来决定的。

接收窗口的大小是**约等于**发送窗口的大小的。

发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408141927782.png" alt="16" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408141927527.jpeg" alt="20" style="zoom:50%;" />



##### 操作系统缓冲区与滑动窗口的关系

​	操作系统的缓冲区，会**被操作系统调整**。如果发生了先减少缓存，再收缩窗口，就会出现丢包的现象。

为了防止这种情况发生，TCP 规定是**不允许同时减少缓存又收缩窗口**的，而是采用先收缩窗口，过段时间再减少缓存，这样就可以避免了丢包情况



##### 窗口关闭

如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。

当发生窗口关闭时，接收方处理完数据后，会向发送方通告一个窗口非 0 的 ACK 报文，如果这个通告窗口的 ACK 报文在网络中丢失了，相互等待的过程，会造成了死锁的现象。

> TCP 是如何解决窗口关闭时，潜在的死锁现象呢？

为了解决这个问题，TCP 为每个连接设有一个持续定时器，**只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。**

如果持续计时器超时，就会发送**窗口探测 ( Window probe ) 报文**，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408141935588.png" alt="25" style="zoom:50%;" />

- 如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器；
- 如果接收窗口不是 0，那么死锁的局面就可以被打破了。

窗口探测的次数一般为 3 次，每次大约 30-60 秒（不同的实现可能会不一样）。如果 3 次过后接收窗口还是 0 的话，有的 TCP 实现就会发 `RST` 报文来中断连接。



##### 糊涂窗口综合症

​	如果接收方太忙了，来不及取走接收窗口里的数据，那么就会导致发送方的发送窗口越来越小。

到最后，**如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症**。

解决糊涂窗口综合症，就要同时解决两个问题就可以了：

- 让接收方不通告小窗口给发送方

- 让发送方避免发送小数据

  

> 怎么让接收方不通告小窗口呢？

接收方通常的策略如下:

当「窗口大小」小于 min( MSS，缓存空间/2 ) ，也就是小于 MSS 与 1/2 缓存大小中的最小值时，就会向发送方通告窗口为 `0`，也就阻止了发送方再发数据过来。

等到接收方处理了一些数据后，窗口大小 >= MSS，或者接收方缓存空间有一半可以使用，就可以把窗口打开让发送方发送数据过来。



> 怎么让发送方避免发送小数据呢？

使用 **Nagle 算法**，该算法的思路是延时处理，只有满足下面两个条件中的任意一个条件，才可以发送数据：

- 条件一：要等到窗口大小 >= `MSS` 并且 数据大小 >= `MSS`；
- 条件二：收到之前发送数据的 `ack` 回包；

只要上面两个条件都不满足，发送方一直在囤积数据，直到满足上面的发送条件。

Nagle 算法一定会有一个小报文，也就是在最开始的时候。

注意，如果接收方不能满足「不通告小窗口给发送方」，那么即使开了 Nagle 算法，也无法避免糊涂窗口综合症，因为如果对端 ACK 回复很快的话（达到 Nagle 算法的条件二），Nagle 算法就不会拼接太多的数据包，这种情况下依然会有小数据包的传输，网络总体的利用率依然很低。

所以，**接收方得满足「不通告小窗口给发送方」+ 发送方开启 Nagle 算法，才能避免糊涂窗口综合症**。

另外，Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。

可以在 Socket 设置 `TCP_NODELAY` 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应用自己的特点来关闭）

```c
setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)&value, sizeof(int));
```



##### 延迟确认

接收方TCP 延迟确认的策略：

- 当有响应数据要发送时，ACK 会随着响应数据一起立刻发送给对方
- 当没有响应数据要发送时，ACK 将会延迟一段时间，以等待是否有响应数据可以一起发送
- 如果在延迟等待发送 ACK 期间，对方的第二个数据报文又到达了，这时就会立刻发送 ACK

和Nagle 同时使用会造成额外的时延，这就会使得网络"很慢"的感觉。关闭一个即可



#### 丢包和重传

##### 丢包

1. 建立连接时丢包

2. 流量控制丢包

3. 网卡丢包

   1. RingBuffer过小导致丢包
   2. 网卡性能不足

4. 接收缓冲区丢包

5. 两端之间的网络丢包

   1. 两端之间那么长的一条链路都属于外部网络，这中间有各种路由器和交换机还有光缆，丢包也是很经常发生的。

      这些丢包行为发生在中间链路的某些个机器上

6. 应用层丢包

   1. TCP保证的可靠性，是**传输层的可靠性**，保证应用层的消息可靠性，就需要应用层自己去实现逻辑做保证。



查看丢包：

1. ping命令查看丢包
2. mtr命令
   1. mtr命令可以查看到你的机器和目的机器之间的每个节点的丢包情况



##### 超时重传

**ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文**。

**重传的 SYN 报文的序列号都是一样的**

每次超时后将**重传时间乘以2**的机制，这就是所谓的**指数回退**（Exponential Backoff）策略

`RTO` （Retransmission Timeout 超时重传时间）**值应该略大于报文往返 RTT 的值**。



##### 快速重传

快速重传的工作原理

1. **重复ACK计数**：
   - 发送方在收到三个连续的相同ACK（重复ACK）时，会认为这个ACK之后的数据包很可能已经丢失。
   - 这种情况下，发送方不会等待RTO超时，而是立即重传认为丢失的数据包。

快速重传的局限

- **误触发**：在某些网络状况较差的环境下，可能会产生伪重复ACK，从而触发不必要的重传操作。

快速重传是TCP中用于增强传输效率的一种重要机制，尤其在网络丢包率较高时，能够显著减少传输延迟。



##### `SACK`

（ Selective Acknowledgment）， **选择性确认**。

这种方式需要在 TCP 头部「选项」字段里加一个 `SACK` 的东西，它**可以将已收到的数据的信息发送给「发送方」**，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以**只重传丢失的数据**。

发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 `SACK` 信息发现只有 `200~299` 这段数据丢失，则重发时，就只选择了这个 TCP 段进行重复。

如果要支持 `SACK`，必须双方都要支持。在 Linux 下，可以通过 `net.ipv4.tcp_sack` 参数打开这个功能（Linux 2.4 后默认打开）

##### Duplicate SACK

Duplicate SACK 又称 `D-SACK`，其主要**使用了 SACK 来告诉「发送方」有哪些数据被重复接收了。**

在 Linux 下可以通过 `net.ipv4.tcp_dsack` 参数开启/关闭这个功能（Linux 2.4 后默认打开）





#### 拥塞控制

控制的目的就是避免「发送方」的数据填满**整个网络**。

> 那么怎么知道当前网络是否出现了拥塞呢？

其实只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是**发生了超时重传，就会认为网络出现了拥塞。**



##### **拥塞窗口 cwnd**



##### 慢启动

**当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1。**

1 2 4 8 16

慢启动门限 `ssthresh` （slow start threshold）状态变量。  一般来说 `ssthresh` 的大小是 `65535` 字节。

- 当 `cwnd` < `ssthresh` 时，使用慢启动算法。

- 当 `cwnd` >= `ssthresh` 时，就会使用「拥塞避免算法」。

  

##### 拥塞避免

**线性增长**  8 9 10 11 12



##### 拥塞发生

- 超时重传
  - `ssthresh` 设为 `cwnd/2`，
  - `cwnd` 重置为 `1` （是恢复为 cwnd 初始化值，我这里假定 cwnd 初始化值 1）
  - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408150826725.png" alt="29" style="zoom:50%;" />
  
- 快速重传

  - `cwnd = cwnd/2` ，也就是设置为原来的一半;
  - `ssthresh = cwnd`;
  - 进入快速恢复算法
    - 拥塞窗口 `cwnd = ssthresh + 3` （ 3 的意思是确认有 3 个数据包被收到了）；
    - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408150826895.png" alt="_E6_8B_A5_E5_A1_9E_E5_8F_91_E7_94_9F-_E5_BF_AB_E9_80_9F_E9_87_8D_E4_BC_A0.drawio" style="zoom:50%;" />

  











#### 问题

##### 有一个 IP 的服务端监听了一个端口，它的 TCP 的最大连接数是多少？



服务端通常固定在某个本地端口上监听，等待客户端的连接请求。

因此，客户端 IP 和端口是可变的，其理论值计算公式如下:

![format,png-20230309230436594](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408140738041.png)

对 IPv4，客户端的 IP 数最多为 `2` 的 `32` 次方，客户端的端口数最多为 `2` 的 `16` 次方，也就是服务端单机最大 TCP 连接数，约为 `2` 的 `48` 次方。

当然，服务端最大并发 TCP 连接数远不能达到理论上限，会受以下因素影响：

- 文件描述符限制

  ，每个 TCP 连接都是一个文件，如果文件描述符被占满了，会发生 Too many open files。Linux 对可打开的文件描述符的数量分别作了三个方面的限制：

  - **系统级**：当前系统可打开的最大数量，通过 `cat /proc/sys/fs/file-max` 查看；
  - **用户级**：指定用户可打开的最大数量，通过 `cat /etc/security/limits.conf` 查看；
  - **进程级**：单个进程可打开的最大数量，通过 `cat /proc/sys/fs/nr_open` 查看；

- **内存限制**，每个 TCP 连接都要占用一定内存，操作系统的内存是有限的，如果内存资源被占满后，会发生 OOM。



##### 每次建立 TCP 连接时，初始化的序列号都要求不一样

1. 为了防止历史报文被下一个相同四元组的连接接收（主要方面）；
2. 为了安全性，防止黑客伪造的相同序列号的 TCP 报文被对方接收；



#####  初始序列号 ISN 是如何随机产生的？

起始 `ISN` 是基于时钟的，每 4 微秒 + 1，转一圈要 4.55 个小时。

RFC793 提到初始化序列号 ISN 随机生成算法：ISN = M + F(localhost, localport, remotehost, remoteport)。

- `M` 是一个计时器，这个计时器每隔 4 微秒加 1。
- `F` 是一个 Hash 算法，根据源 IP、目的 IP、源端口、目的端口生成一个随机数值。要保证 Hash 算法不能被外部轻易推算得出，用 MD5 算法是一个比较好的选择。

可以看到，随机数是会基于时钟计时器递增的，基本不可能会随机成一样的初始化序列号。



##### 既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？

​	如果在 TCP 的整个报文（头部 + 数据）交给 IP 层进行分片，那么当如果一个 IP 分片丢失，因为ip层无超时重传机制，tcp层无法组装成一个完整的tcp段，就会触发超时重传，重发「整个 TCP 报文（头部 + 数据），导致整个 IP 报文的所有分片都得重传。

​	经过 TCP 层分片后，如果一个 TCP 分片丢失后，**进行重发时也是以 MSS 为单位**，而不用重传所有的分片，大大增加了重传的效率

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408140800746.png" alt="format,png-20230309230633447" style="zoom:50%;" />





##### TCP Keepalive 和 HTTP Keep-Alive 

HTTP 的 Keep-Alive 也叫 HTTP 长连接，该功能是由「应用程序」实现的，可以使得用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，减少了 HTTP 短连接带来的多次 TCP 连接建立和释放的开销。

TCP 的 Keepalive 也叫 TCP 保活机制，该功能是由「内核」实现的，当客户端和服务端长达一定时间没有进行数据交互时，内核为了确保该连接是否还有效，就会发送探测报文，来检测对方是否还在线，然后来决定是否要关闭该连接



##### 多个 TCP 服务进程绑定同一个端口

**如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”**。

> 重启 TCP 服务进程时，为什么会有“Address in use”的报错信息？

当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。****TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误。**

> 重启 TCP 服务进程时，如何避免“Address in use”的报错信息？

我们可以在调用 bind 前，对 socket 设置 SO_REUSEADDR 属性，可以解决这个问题。

```c
int on = 1;
setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));
```

因为 SO_REUSEADDR 作用是：**如果当前启动进程绑定的 IP+PORT 与处于TIME_WAIT 状态的连接占用的 IP+PORT 存在冲突，但是新启动的进程使用了 SO_REUSEADDR 选项，那么该进程就可以绑定成功**。

> **前面我提到过这个问题**：如果 TCP 服务进程 A 绑定的地址是 0.0.0.0 和端口 8888，而如果 TCP 服务进程 B 绑定的地址是 192.168.1.100 地址（或者其他地址）和端口 8888，那么执行 bind() 时候也会出错。

这个问题也可以由 SO_REUSEADDR 解决，因为它的**另外一个作用**：绑定的 IP地址 + 端口时，只要 IP 地址不是正好(exactly)相同，那么允许绑定。

比如，0.0.0.0:8888 和192.168.1.100:8888，虽然逻辑意义上前者包含了后者，但是 0.0.0.0 泛指所有本地 IP，而 192.168.1.100 特指某一IP，两者并不是完全相同，所以在对 socket 设置 SO_REUSEADDR 属性后，那么执行 bind() 时候就会绑定成功



##### 客户端端口选择流程

一般而言，客户端不建议使用 bind 函数，应该交由 connect 函数来选择端口会比较好，因为客户端的端口通常都没什么意义。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409011145299.png" alt="_E7_AB_AF_E5_8F_A3_E9_80_89_E6_8B_A9" style="zoom:33%;" />

#### 优化tcp

##### 连接优化

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408162053523.png" alt="24" style="zoom: 50%;" />

###### 客户端

根据网络的稳定性和目标服务器的繁忙程度修改 SYN 的重传次数，调整客户端的三次握手时间上限

###### 服务端

1. 当网络繁忙、不稳定时，报文丢失就会变严重，此时应该调大SYN+ACK 报文重发次数。反之则可以调小重发次数。**修改重发次数的方法是，调整 tcp_synack_retries 参数**：
2. TCP Fast Open 



##### 关闭优化

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408172145539.png" alt="39" style="zoom:50%;" />





###### 主动方的优化

主动发起 FIN 报文断开连接的一方，如果迟迟没收到对方的 ACK 回复，则会重传 FIN 报文，重传的次数由 `tcp_orphan_retries` 参数决定。

当主动方收到 ACK 报文后，连接就进入 FIN_WAIT2 状态，根据关闭的方式不同，优化的方式也不同：

- 如果这是 close 函数关闭的连接，那么它就是孤儿连接。如果 `tcp_fin_timeout` 秒内没有收到对方的 FIN 报文，连接就直接关闭。同时，为了应对孤儿连接占用太多的资源，`tcp_max_orphans` 定义了最大孤儿连接的数量，超过时连接就会直接释放。
- 反之是 shutdown 函数关闭的连接，则不受此参数限制；

当主动方接收到 FIN 报文，并返回 ACK 后，主动方的连接进入 TIME_WAIT 状态。这一状态会持续 1 分钟，为了防止 TIME_WAIT 状态占用太多的资源，`tcp_max_tw_buckets` 定义了最大数量，超过时连接也会直接释放。

当 TIME_WAIT 状态过多时，还可以通过设置 `tcp_tw_reuse` 和 `tcp_timestamps` 为 1 ，将 TIME_WAIT 状态的端口复用于作为客户端的新连接，注意该参数只适用于客户端。





关闭连接的方式通常有两种，分别是 RST 报文关闭和 FIN 报文关闭。

> 调用 close 函数和 shutdown 函数有什么区别？

调用了 close 函数意味着完全断开连接，**完全断开不仅指无法传输数据，而且也不能发送数据。 此时，调用了 close 函数的一方的连接叫做「孤儿连接」，如果你用 netstat -p 命令，会发现连接对应的进程名为空。**

使用 close 函数关闭连接是不优雅的。于是，就出现了一种优雅关闭连接的 `shutdown` 函数，**它可以控制只关闭一个方向的连接**：

![26](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408172159499.png)

第二个参数决定断开连接的方式，主要有以下三种方式：

- SHUT_RD(0)：**关闭连接的「读」这个方向**，如果接收缓冲区有已接收的数据，则将会被丢弃，并且后续再收到新的数据，会对数据进行 ACK，然后悄悄地丢弃。也就是说，对端还是会接收到 ACK，在这种情况下根本不知道数据已经被丢弃了。
- SHUT_WR(1)：**关闭连接的「写」这个方向**，这就是常被称为「半关闭」的连接。如果发送缓冲区还有未发送的数据，将被立即发送出去，并发送一个 FIN 报文给对端。
- SHUT_RDWR(2)：相当于 SHUT_RD 和 SHUT_WR 操作各一次，**关闭套接字的读和写两个方向**。







TIME_WAIT 状态优化

1. **Linux 提供了 tcp_max_tw_buckets 参数，当 TIME_WAIT 的连接数量超过该参数时，新关闭的连接就不再经历 TIME_WAIT 而直接关闭：**

   <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408172125819.png" alt="33" style="zoom:50%;" />

2. **在建立新连接时，复用处于 TIME_WAIT 状态的连接，那就是打开 tcp_tw_reuse 参数。但是需要注意，该参数是只用于客户端（建立连接的发起方），因为是在调用 connect() 时起作用的，而对于服务端（被动连接方）是没有用的。**

   - 需要打开对 TCP 时间戳的支持（对方也要打开 ）：

3. 在程序中设置 socket 选项，来设置调用 close 关闭连接行为。

   1. <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408172132709.png" alt="37" style="zoom:50%;" />

   如果 `l_onoff` 为非 0， 且 `l_linger` 值为 0，**那么调用 close 后，会立该发送一个 RST 标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了 TIME_WAIT 状态，直接关闭。**

   这种方式只推荐在客户端使用，服务端千万不要使用。因为服务端一调用 close，就发送 RST 报文的话，客户端就总是看到 TCP 连接错误 “connnection reset by peer”





###### 被动方

被动关闭的连接方应对非常简单，它在回复 ACK 后就进入了 CLOSE_WAIT 状态，等待进程调用 close 函数关闭连接。因此，出现大量 CLOSE_WAIT 状态的连接时，应当从应用程序中找问题。

当被动方发送 FIN 报文后，连接就进入 LAST_ACK 状态，在未等到 ACK 时，会在 `tcp_orphan_retries` 参数的控制下重发 FIN 报文



##### 优化数据传输的方式

Linux 会对缓冲区动态调节，我们应该把缓冲区的上限设置为带宽时延积。发送缓冲区的调节功能是自动打开的，而接收缓冲区需要把 tcp_moderate_rcvbuf 设置为 1 来开启。其中，调节的依据是 TCP 内存范围 tcp_mem。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181015370.png" alt="49" style="zoom:50%;" />



### UDP

- UDP 支持一对一、一对多、多对多的交互通信
- UDP 首部只有 8 个字节，并且是固定不变的，开销较小。
- UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序。
- UDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408140740138.png" alt="format,png-20230309230439961" style="zoom:50%;" />

TCP 和 UDP 可以使用同一个端口

传输层有两个传输协议分别是 TCP 和 UDP，在内核中是两个完全独立的软件模块。

当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408140745519.jpeg" alt="tcp_E5_92_8Cudp_E6_A8_A1_E5_9D_97" style="zoom:50%;" />











### linux命令

> 查看 TCP 的连接状态

`netstat -napt` 

![](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090835806.png)



> 怎么查看系统的 cwnd 初始化值？

Linux 针对每一个 TCP 连接的 cwnd 初始化值是 10，也就是 10 个 MSS，我们可以用 ss -nli 命令查看每一个 TCP 连接的 cwnd 初始化值，如下图

![cwnd](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408141950452.png)



------



## 3.网络层

主机到主机，传输单位是包（packet）

####  IPV4协议

（*Internet Protocol*）

IP 地址（IPv4 地址）由 `32` 位正整数来表示，IP 地址在计算机是以二进制的方式处理的。

而人类为了方便记忆采用了**点分十进制**

##### IP 报文头部的格式

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090838923.jpeg" alt="14" style="zoom:50%;" />

##### 分片

IP 协议会将传输层的报文作为数据部分，再加上 IP 包头组装成 IP 报文，如果 IP 报文大小超过 MTU（**以太网中一般为 1500 字节**）就会**再次进行分片**，得到一个即将发送到网络的 IP 报文。





##### **分类地址**

IP 地址分类成了 5 种类型，分别是 A 类、B 类、C 类、D 类、E 类。

D 类常被用于**多播**，E 类是预留的分类，暂时未使用。

多播用于**将包发送给特定组内的所有主机。**组播

- 224.0.0.0 ~ 224.0.0.255 为预留的组播地址，只能在局域网中，路由器是不会进行转发的。
- 224.0.1.0 ~ 238.255.255.255 为用户可用的组播地址，可以用于 Internet 上。
- 239.0.0.0 ~ 239.255.255.255 为本地管理组播地址，可供内部网在内部使用，仅在特定的本地范围内有效。



优点就是**简单明了、选路（基于网络地址）简单**

缺点

1. **同一网络下没有地址层次**，比如一个公司里用了 B 类地址，但是可能需要根据生产环境、测试环境、开发环境来划分地址层次，而这种 IP 分类是没有地址层次划分的功能，所以这就**缺少地址的灵活性**。

2. A、B、C类有个尴尬处境，就是**不能很好的与现实网络匹配**。

   C 类地址能包含的最大主机数量实在太少了，只有 254 个，估计一个网吧都不够用。

   而 B 类地址能包含的最大主机数量又太多了，6 万多台机器放在一个网络下面，一般的企业基本达不到这个规模，闲着的地址就是浪费。



<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408191816000.png" alt="7" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408191821491.png" alt="8" style="zoom:50%;" />

- 主机号全为 1 指定某个网络下的所有主机，用于广播
  - **在本网络内广播的叫做本地广播**。例如网络地址为 192.168.0.0/24 的情况下，广播地址是 192.168.0.255 。因为这个广播地址的 IP 包会被路由器屏蔽，所以不会到达 192.168.0.0/24 以外的其他链路上。
  - **在不同网络之间的广播叫做直接广播**
- 主机号全为 0 指定某个网络



##### 无分类地址 CIDR

IP 地址被划分为两部分，前面是**网络号**，后面是**主机号**。`/x` 表示前 x 位属于**网络号**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408191827221.png" alt="15" style="zoom:50%;" />

###### 子网掩码

IP 地址分成两种意义：

- 一个是**网络号**，负责标识该 IP 地址是属于哪个「子网」的；
- 一个是**主机号**，负责标识同一「子网」下的不同主机；

配合**子网掩码**算出 IP 地址 的网络号和主机号。



子网掩码还有一个作用，那就是**划分子网**。

**子网划分实际上是将主机地址分为两个部分：子网网络地址和子网主机地址**。形式如下：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408191829245.png" alt="18" style="zoom:50%;" />

公私IP

私有 IP 地址通常是内部的 IT 人员管理，公有 IP 地址是由 `ICANN` 组织管理，中文叫「互联网名称与数字地址分配机构」。

IANA 是 ICANN 的其中一个机构，它负责分配互联网 IP 地址，是按州的方式层层分配。

环回地址是在同一台计算机上的程序之间进行网络通信时所使用的一个默认地址。

计算机使用一个特殊的 IP 地址 **127.0.0.1 作为环回地址**。与该地址具有相同意义的是一个叫做 `localhost` 的主机名。



##### 路由和转发

> 假设客户端有多个网卡，就会有多个 IP 地址，那 IP 头部的源地址应该选择哪个 IP 呢？

当存在多个网卡时，在填写源地址 IP 时，就需要判断到底应该填写哪个地址。这个判断相当于在多块网卡中判断应该使用哪个一块网卡来发送包。

这个时候就需要根据**路由表**规则，来判断哪一个网卡作为源地址 IP。

在 Linux 操作系统，我们可以使用 `route -n` 命令查看当前系统的路由表。

![15](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090852298.png)

举个例子，根据上面的路由表，我们假设 Web 服务器的目标地址是 `192.168.10.200`。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090852087.png" alt="16" style="zoom:50%;" />

1. 首先先和第一条目的子网掩码（`Genmask`）进行 **与运算**，得到结果为 `192.168.10.0`，但是第一个条目的 `Destination` 是 `192.168.3.0`，两者不一致所以匹配失败。
2. 再与第二条目的子网掩码进行 **与运算**，得到的结果为 `192.168.10.0`，与第二条目的 `Destination 192.168.10.0` 匹配成功，所以将使用 `eth1` 网卡的 IP 地址作为 IP 包头的源地址。

那么假设 Web 服务器的目标地址是 `10.100.20.100`，那么依然依照上面的路由表规则判断，判断后的结果是和第三条目匹配。

第三条目比较特殊，它目标地址和子网掩码都是 `0.0.0.0`，这表示**默认网关**，如果其他所有条目都无法匹配，就会自动匹配这一行。并且后续就把包发给路由器，`Gateway` 即是路由器的 IP 地址。



###### 路由算法

**静态路由**适用于结构简单、变化少的网络，手动配置路径。

**动态路由**能够自动适应网络的变化，基于不同的算法自动选择最优路径。

- **距离矢量算法**如RIP适合小型网络，但可能出现路由环路和收敛慢的问题。
- **链路状态算法**如OSPF适合大型网络，收敛快且精确，但计算和带宽开销较大。
- **混合路由算法**如EIGRP结合了两者的优点，收敛快且效率高。

------



##### DHCP

动态获取 IP 地址

4 个步骤：

- 客户端首先发起 **DHCP 发现报文（DHCP DISCOVER）** 的 IP 数据报，由于客户端没有 IP 地址，也不知道 DHCP 服务器的地址，所以使用的是 UDP **广播**通信，其使用的广播目的地址是 255.255.255.255（端口 67） 并且使用 0.0.0.0（端口 68） 作为源 IP 地址。DHCP 客户端将该 IP 数据报传递给链路层，链路层然后将帧广播到所有的网络中设备。
- DHCP 服务器收到 DHCP 发现报文时，用 **DHCP 提供报文（DHCP OFFER）** 向客户端做出响应。该报文仍然使用 IP 广播地址 255.255.255.255，该报文信息携带服务器提供可租约的 IP 地址、子网掩码、默认网关、DNS 服务器以及 **IP 地址租用期**。
- 客户端收到一个或多个服务器的 DHCP 提供报文后，从中选择一个服务器，并向选中的服务器发送 **DHCP 请求报文（DHCP REQUEST**进行响应，回显配置的参数。
- 最后，服务端用 **DHCP ACK 报文**对 DHCP 请求报文进行响应，应答所要求的参数。



一旦客户端收到 DHCP ACK 后，交互便完成了，并且客户端能够在租用期内使用 DHCP 服务器分配的 IP 地址。

如果租约的 DHCP IP 地址快期后，客户端会向服务器发送 DHCP 请求报文：

- 服务器如果同意继续租用，则用 DHCP ACK 报文进行应答，客户端就会延长租期。
- 服务器如果不同意继续租用，则用 DHCP NACK 报文，客户端就要停止使用租约的 IP 地址。

可以发现，DHCP 交互中，**全程都是使用 UDP 广播通信**。

DHCP 中继代理，**对不同网段的 IP 地址分配也可以由一个 DHCP 服务器统一进行管理。**



##### NAT

**网络地址与端口转换 NAPT**

![img](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408200925214.jpeg)

，**两个私有 IP 地址都转换 IP 地址为公有地址 120.229.175.121，但是以不同的端口号作为区分。**

于是，生成一个 NAPT 路由器的转换表，就可以正确地转换地址跟端口的组合，令客户端 A、B 能同时与服务器之间进行通信。

问题：

- 外部无法主动与 NAT 内部服务器建立连接，因为 NAPT 转换表没有转换记录。
- 转换表的生成与转换操作都会产生性能开销。
- 通信过程中，如果 NAT 路由器重启了，所有的 TCP 连接都将被重置。

NAT 穿透技术

NAT 穿越技术拥有这样的功能，它能够让网络应用程序主动发现自己位于 NAT 设备之后，并且会主动获得 NAT 设备的公有 IP，并为自己建立端口映射条目，注意这些都是 NAT设备后的应用程序自动完成的。

说人话，就是客户端主动从 NAT 设备获取公有 IP 地址，然后自己建立端口映射条目，然后用这个条目对外通信，就不需要 NAT 设备来进行转换了



### IPV6

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408200913802.png" alt="29" style="zoom:50%;" />

![31](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408200914639.png)

- **取消了首部校验和字段。** 因为在数据链路层和传输层都会校验，因此 IPv6 直接取消了 IP 的校验。
- **取消了分片/重新组装相关字段。** 分片与重组是耗时的过程，IPv6 不允许在中间路由器进行分片与重组，这种操作只能在源与目标主机，这将大大提高了路由器转发的速度。
- **取消选项字段。** 选项字段不再是标准 IP 首部的一部分了，但它并没有消失，而是可能出现在 IPv6 首部中的「下一个首部」指出的位置上。删除该选项字段使的 IPv6 的首部成为固定长度的 `40` 字节。

IPv6地址用冒号分隔的八个四位十六进制数表示，例如：`2001:0db8:85a3:0000:0000:8a2e:0370:7334`。如果地址中有连续的零，可以使用“::”进行缩写

IPv6 不仅仅只是可分配的地址变多了，它还有非常多的亮点。

- IPv6 可自动配置，即使没有 DHCP 服务器也可以实现自动分配IP地址，真是**便捷到即插即用**啊。
- IPv6 包头包首部长度采用固定的值 `40` 字节，去掉了包头校验和，简化了首部结构，减轻了路由器负荷，大大**提高了传输的性能**。
- IPv6 有应对伪造 IP 地址的网络安全功能以及防止线路窃听的功能，大大**提升了安全性**。
- **...** （由你发现更多的亮点）

对于一对一通信的 IPv6 地址，主要划分了三类单播地址，每类地址的有效范围都不同。

- 在同一链路单播通信，不经过路由器，可以使用**链路本地单播地址**，IPv4 没有此类型
- 在内网里单播通信，可以使用**唯一本地地址**，相当于 IPv4 的私有 IP
- 在互联网通信，可以使用**全局单播地址**，相当于 IPv4 的公有 IP



### 其他协议



#### ICMP

（Internet Control Message Protocol）是互联网控制消息协议，是在TCP/IP协议族中的一个重要协议之一。ICMP主要用于在IP网络中发送控制消息，提供有关通信状态的信息。以下是ICMP的一些主要特点和用途：

1. **错误报告：** ICMP用于报告有关网络通信错误的信息。当发生错误时，ICMP会生成错误消息，如目的不可达、超时等，以通知源主机或路由器有关错误的详细信息。
2. **网络诊断：** ICMP也用于执行网络诊断任务。例如，通过使用ICMP的"ping"命令，可以测试与目标主机的连接是否正常。"traceroute"命令使用ICMP消息来跟踪数据包在网络中的路径。
3. **寻址和路由：** ICMP消息还可用于寻址和路由。ICMP Redirect消息可用于告知主机或路由器在其通信中使用更佳的路径。
4. **Echo请求和回应：** ICMP Echo请求和回应用于检测主机或设备是否可达。"ping"命令就是通过发送ICMP Echo请求，并等待目标主机的回应来测试网络连接的工具。
5. **MTU路径发现：** ICMP还用于实现MTU（Maximum Transmission Unit）路径发现，以确定在通信路径中的最大传输单元大小，以避免分片。



`ICMP` **互联网控制报文协议**（**Internet Control Message Protocol**）用于告知网络包传送过程中产生的错误以及各种控制信息。

- **确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等。**

- 分为两大类：
  - 一类是用于诊断的查询消息，也就是「**查询报文类型**」
  
    - ping —— 查询报文类型的使用
  
      **功能**：`ping` 是一种测试工具，主要用于检测网络连通性和测量往返时间（Round Trip Time, RTT）。
  
      **工作原理**：Ping 使用 ICMP 的 **回显请求（Echo Request）** 和 **回显应答（Echo Reply）** 报文来实现。主机发送一个回显请求报文到目标主机，目标主机会返回一个回显应答报文。通过计算请求发出到应答返回的时间，Ping 能测量网络的延迟。
  
      **报文类型**：
  
      - 回显请求（Echo Request）：ICMP 类型为 8。
      - 回显应答（Echo Reply）：ICMP 类型为 0。
  
  - 另一类是通知出错原因的错误消息，也就是「**差错报文类型**」
  
    - traceroute —— 差错报文类型的使用
  
      一种跟踪网络路径的工具，能够显示从源主机到目标主机经过的每一跳（即路由器）的 IP 地址，并测量每一跳的延迟。
  
      **工作原理**：Traceroute 通过发送带有不同 **TTL（Time To Live）** 值的 UDP 数据包来实现。每一跳路由器都会将 TTL 减 1，当 TTL 变为 0 时，路由器丢弃该数据包并返回一个 ICMP **超时（Time Exceeded）** 差错报文给源主机。通过不断增加 TTL 值，Traceroute 能逐跳记录路径上的每个路由器。
  
      **报文类型**：
  
      - 超时（Time Exceeded）：ICMP 类型为 11，代码为 0（TTL 到达 0）。
  
  - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408200930675.png" alt="41" style="zoom:50%;" />

#### `ARP` 

用于根据 IP 地址查询相应的以太网 MAC 地址。RARP 协议正好相反，它是**已知 MAC 地址求 IP 地址**。例如将打印机服务器等小型嵌入式设备接入到网络时就经常会用得到。

#### `IGMP `

是因特网组管理协议，工作在主机（组播成员）和最后一跳路由之间

- IGMP 报文向路由器申请加入和退出组播组，默认情况下路由器是不会转发组播包到连接中的主机，除非主机通过 IGMP 加入到组播组，主机申请加入到组播组时，路由器就会记录 IGMP 路由器表，路由器后续就会转发组播包到对应的主机了。
- IGMP 报文采用 IP 封装，IP 头部的协议号为 2，而且 TTL 字段值通常为 1，因为 IGMP 是工作在主机与连接的路由器之间。




### 路由器

路由器（Router）是一种网络设备，主要功能是将数据包**从一个网络传递到另一个网络**，决定数据包的最佳路径，并确保数据能够正确地到达目的地。路由器是连接不同网络、特别是连接局域网（LAN）与广域网（WAN）或互联网的重要设备。

#### 路由器的基本功能

1. **数据包转发**：
   
   - 路由器的各个端口都具有 MAC 地址和 IP 地址
   
   - 路由器的核心功能是根据目的IP地址将数据包从源网络转发到目的网络。路由器使用路由表来决定最佳的路径，并确保数据能通过这一路径到达目的地。
   
   - 
   
   - > 路由器的包接收操作
   
     首先，电信号到达网线接口部分，路由器中的模块会将电信号转成数字信号，然后通过包末尾的 `FCS` 进行错误校验。
     如果没问题则检查 MAC 头部中的**接收方 MAC 地址**，看看是不是发给自己的包，如果是就放到接收缓冲区中，否则就丢弃这个包。
     完成包接收操作之后，路由器就会**去掉**包开头的 MAC 头部。
   
2. **路由选择**：
   - 路由器可以使用多种路由协议（如RIP、OSPF、BGP）来动态地选择和更新最佳路径。路由协议帮助路由器了解网络拓扑的变化，并选择最优路径进行数据传输。

3. **网络地址转换（NAT）**：
   
   - 路由器常用NAT技术，将私有网络的内部IP地址转换为公共IP地址，允许多个设备共享一个公共IP地址访问互联网。NAT还提高了网络安全性，因为它隐藏了内部网络结构。
   
4. **防火墙功能**：
   - 路由器通常内置简单的防火墙功能，可以通过访问控制列表（ACL）来控制哪些数据包可以通过路由器，保护内部网络免受未经授权的访问。

5. **DHCP服务器**：
   - 许多家庭或小型企业的路由器还内置了DHCP（动态主机配置协议）服务器功能，自动为网络中的设备分配IP地址，从而简化网络管理。

6. **无线功能**：
   - 现代家庭路由器通常还集成了无线接入点（WAP）的功能，提供Wi-Fi连接，使设备能够通过无线方式连接到路由器和互联网。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408091143937.png" alt="24" style="zoom: 67%;" />



#### 路由器的工作原理

1. **路由表的维护**：
   - 路由器维护一张路由表，这张表包含网络中各个目的地的IP地址以及到达这些目的地的路径信息。路由表可以是静态配置的，也可以通过路由协议动态更新。

2. **数据包的接收和转发**：
   - 当一个数据包到达路由器时，路由器会检查数据包的目的IP地址，并在路由表中查找匹配的路径。然后，路由器将数据包转发到下一个合适的网络节点，直到数据包到达最终目的地。

3. **路由协议的运行**：
   - 路由协议如RIP（路由信息协议）、OSPF（开放最短路径优先）和BGP（边界网关协议）允许路由器与其他路由器交换路由信息，从而更新路由表并优化数据包传输路径。

4. **NAT的执行**：
   - 当内部网络设备试图访问互联网时，路由器使用NAT将内部私有IP地址转换为公共IP地址，并记录转换信息，以便返回的数据能够正确地路由回原始内部设备。

#### 路由器的类型

1. **家庭路由器**：
   - 家庭路由器通常是集成设备，提供有线和无线连接、NAT、DHCP、基本防火墙功能，专为家庭或小型办公室设计。

2. **企业级路由器**：
   - 企业级路由器具备更强大的处理能力和更丰富的功能，如支持多个WAN连接、VPN（虚拟专用网络）、高级路由协议、冗余配置、流量管理和高级防火墙功能，适用于大型企业网络。

3. **核心路由器**：
   - 核心路由器是互联网服务提供商（ISP）和大型企业网络的核心设备，负责处理高流量的路由选择，通常用于互联网骨干网，具有极高的速度和可靠性。

4. **边界路由器**：
   - 边界路由器用于连接企业内部网络与外部网络（如互联网），并负责网络间的数据包转发和安全管理，通常部署在网络边缘位置。

#### 路由器与其他网络设备的区别

- **交换机（Switch）**：交换机工作在OSI模型的第二层（数据链路层），主要在局域网内部转发数据帧，根据MAC地址来决定数据的去向。路由器则工作在第三层（网络层），处理数据包的IP地址，用于不同网络之间的通信。

- **集线器（Hub）**：集线器是更简单的设备，工作在OSI模型的第一层（物理层），只能广播接收到的信号到所有端口。路由器则根据路由表智能地转发数据包。

- **网关（Gateway）**：网关是一种通用术语，指任何设备或软件模块，用于连接两个不同的网络或协议。路由器通常被视为一种特殊的网关，专门用于IP网络之间的通信。

#### 路由器的应用

1. **家庭和小型办公网络**：
   - 在家庭或小型办公网络中，路由器通常充当网络的中心设备，连接所有的有线和无线设备，并提供与互联网的连接。

2. **企业网络**：
   - 企业级路由器用于连接多个局域网、管理复杂的网络流量，并确保网络的安全性和可靠性。

3. **ISP和大型网络**：
   - 在互联网的骨干网和大型网络中，路由器用于处理大量的网络流量，进行高效的路由选择，并支持高级功能如流量管理和服务质量（QoS）。



------



## 2.数据链路层

传输单位是帧（frame），点到点

工作在网卡这个层次，使用 MAC 地址来标识网络上的设备



### MAC 地址

里需要**发送方 MAC 地址**和**接收方目标 MAC 地址**，用于**两点之间的传输**。

![18](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090900634.png)

一般在 TCP/IP 通信里，MAC 包头的**协议类型**只使用：

- `0800` ： IP 协议

- `0806` ： ARP 协议

  

> MAC 发送方和接收方如何确认?

**发送方**：MAC 地址是在网卡生产时写入到 ROM 里的，只要将这个值读取出来写入到 MAC 头部就可以了。

**接收方**：

- 先查询 ARP 缓存，如果其中已经保存了对方的 MAC 地址，就不需要发送 ARP 查询，直接使用 ARP 缓存中的地址。

- 而当 ARP 缓存中不存在对方 MAC 地址时，则发送 ARP 广播查询。

  

### 以太网

以太网（Ethernet）是一种用于局域网（LAN）中的网络技术和标准，定义了计算机和其他设备在局域网中如何相互通信和数据传输
以太网最早由罗伯特·梅特卡夫（Robert Metcalfe）在1970年代发明，经过多次标准化，成为了现代有线网络的基础技术。

#### 以太网的基本概念

1. **帧（Frame）**：
   - 以太网将要传输的数据封装在一个个“帧”中。每个帧包含了源和目的MAC地址、数据负载和错误检测码（通常是CRC）。以太网帧是数据传输的基本单元。

2. **MAC地址（Media Access Control Address）**：
   - MAC地址是网络设备的唯一标识符，每个以太网设备都有一个全球唯一的MAC地址。MAC地址在以太网通信中用于标识发送方和接收方。

3. **冲突域与广播域**：
   - **冲突域**：在以太网中，如果两个设备同时发送数据，数据包可能会在网络中发生冲突，这种现象主要发生在共享媒体中（如早期的集线器网络）。交换机可以通过为每个端口创建独立的冲突域来避免这种问题。
   - **广播域**：广播域是指网络中所有能接收到同一个广播消息的设备范围。广播消息会发送给网络中的所有设备，而不仅仅是某个特定设备。

4. **物理层与数据链路层**：
   - 以太网技术涉及OSI模型的第一层（物理层）和第二层（数据链路层）。物理层定义了以太网电缆、连接器、信号传输等物理特性，而数据链路层定义了帧的格式、MAC地址、链路层协议等内容。

#### 以太网的工作原理

1. **数据封装**：
   - 当一台计算机或网络设备需要发送数据时，它会将数据分成若干个以太网帧。每个帧包含目标设备的MAC地址、源设备的MAC地址、要传输的数据和一些控制信息。
   - 最大传输单元（MTU）是 `1500` 字节
   
2. **介质访问控制（CSMA/CD）**：
   - 以太网使用一种叫做CSMA/CD（Carrier Sense Multiple Access with Collision Detection，载波监听多路访问/冲突检测）的机制来管理设备在共享介质上的通信。设备在发送数据之前会“监听”线路是否空闲，如果检测到冲突，设备会等待一段随机时间后重试。

3. **数据传输**：
   - 以太网帧通过网络电缆从发送设备传输到接收设备。使用交换机的网络会将帧精确地转发到目标设备所在的端口，而非广播给所有设备，从而提高效率。

4. **错误检测与纠正**：
   - 每个以太网帧都包含一个循环冗余校验码（CRC），用于检测在传输过程中是否出现了错误。如果接收设备检测到错误，它可以丢弃该帧并请求重传。

#### 以太网的类型与标准

1. **以太网标准**：
   - 以太网标准由IEEE 802.3工作组定义，不同的标准支持不同的传输速率和物理介质，如：
     - **10BASE-T**：10 Mbps速率，以双绞线为传输介质。
     - **100BASE-TX**：100 Mbps速率，俗称“快速以太网”。
     - **1000BASE-T**：1 Gbps速率，俗称“千兆以太网”。
     - **10GBASE-T**：10 Gbps速率，适用于高性能网络和数据中心。

2. **物理介质**：
   - 以太网通常使用双绞线电缆（如Cat 5e、Cat 6、Cat 7）或光纤作为传输介质。双绞线用于短距离传输（通常在100米以内），而光纤用于长距离传输。

3. **全双工与半双工**：
   - **半双工**：数据只能在一个方向上传输，每次只能有一个设备发送数据，这种模式在早期的共享介质以太网中常见。
   - **全双工**：数据可以同时在两个方向上传输，避免了冲突，提高了网络效率，这是现代交换机网络的常用模式。

#### 以太网的应用

以太网广泛应用于各种规模的局域网中，从家庭网络到企业网络，再到数据中心。它因其高效、可靠、成本低、兼容性好等特点，成为了全球最常用的有线网络技术。此外，随着以太网技术的不断发展，高速以太网（如千兆以太网和10G以太网）被广泛应用于对带宽和性能要求较高的环境中，如视频流、云计算和大数据处理。

#### 以太网与其他网络技术的比较

- **Wi-Fi（无线局域网）**：与无线技术（如Wi-Fi）相比，以太网通常提供更稳定、更高速的连接，且不易受到干扰。但以太网需要布线，安装和扩展可能不如Wi-Fi灵活。
  
- **令牌环网**：以太网与令牌环网等早期局域网技术相比，具有更高的传输速率和更简单的实现方式，令牌环网如今已很少使用。

以太网已成为构建局域网的标准方式，其技术不断演进，以适应日益增长的网络需求。





###  网卡

**物理层（第一层）**：

- 网卡通过物理层与传输介质（如铜线、光纤、无线信号等）直接交互。它负责将数字数据转换为可以传输的电信号或无线信号，或反过来将接收到的信号转换回数字数据。

**数据链路层（第二层）**：

- 在数据链路层，网卡处理MAC地址，并负责帧的封装与解封装，错误检测（如CRC校验），以及控制数据在局域网中的流动。网卡使用MAC地址来唯一标识网络设备，并通过数据链路层协议（如以太网协议）进行通信。

  

控制网卡还需要靠**网卡驱动程序**。

网卡驱动获取网络包之后，会将其**复制**到网卡内的**缓存区**中，接着会在其**开头加上报头和起始帧分界符，在末尾加上用于检测错误的帧校验序列**。

- 起始帧分界符是一个用来表示包起始位置的标记
- 末尾的 `FCS`（帧校验序列）用来检查包传输过程是否有损坏

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090906449.png" alt="_E6_95_B0_E6_8D_AE_E5_8C_85.drawio" style="zoom:50%;" />

在接收数据时，会将数据暂存到`RingBuffer`接收缓冲区中，然后等着内核触发软中断慢慢收走



### 交换机（Switch）

是计算机网络中的一种设备，用于连接多个设备（如计算机、服务器、打印机等），并通过数据帧的转发实现局域网（LAN）内的通信。交换机通过学习和记录每个设备的MAC地址，能够有效地将数据转发到目标设备，从而优化网络性能。

#### 交换机的基本功能

1. **帧转发**：
   - 交换机工作在OSI模型的第二层（数据链路层），它接收来自一个端口的数据帧，并根据帧中的目标MAC地址决定将帧转发到哪个端口。只有目标设备所在的端口会接收到数据帧，这与集线器（Hub）不同，集线器会将数据广播到所有端口。

2. **MAC地址表**：
   - 交换机会自动学习和记录每个设备的MAC地址，并将这些地址与对应的端口映射起来。这些信息存储在交换机的MAC地址表中。当交换机接收到数据帧时，它会查找MAC地址表来确定该帧的转发路径。

3. **全双工通信**：
   - 现代交换机支持全双工通信，即可以同时进行发送和接收数据，从而提高网络的效率和速度。

4. **冲突域分割**：
   - 交换机为每个连接的设备创建了一个独立的冲突域，这意味着不同设备之间的通信不会互相干扰，从而减少了网络中的冲突和冲突域内的数据碰撞。

#### 交换机的工作原理

1. **数据帧进入交换机**：
   - 当设备A发送数据给设备B时，数据帧会通过设备A连接的端口进入交换机。

2. **交换机检查目标MAC地址**：
   - **交换机的端口不具有 MAC 地址**
   - 交换机检查数据帧中的目标MAC地址，然后在其MAC地址表中查找对应的端口。
   
3. **数据帧的转发**：
   - 如果交换机在MAC地址表中找到目标MAC地址，它会将数据帧转发到对应的端口，数据仅发送到目标设备。否则，交换机会将数据帧广播到所有端口（除去源端口），以找到目标设备，并更新MAC地址表。
   - ![23](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408091112826.jpeg)
   
4. **学习过程**：
   - 交换机会不断学习每个设备的MAC地址，并更新其MAC地址表，以便未来能更高效地转发数据帧。

#### 交换机的类型

1. **非管理型交换机（Unmanaged Switch）**：
   - 非管理型交换机通常用于小型网络，不需要配置和管理，插上电源即可使用，适合家庭或小型办公室。

2. **管理型交换机（Managed Switch）**：
   - 管理型交换机提供了更多的功能和控制选项，可以通过命令行或图形界面进行配置和管理，如VLAN设置、端口监控、流量优先级设置等，适用于大型或企业级网络。

3. **PoE交换机（Power over Ethernet Switch）**：
   - PoE交换机不仅能传输数据，还能通过网线为连接的设备（如IP摄像头、无线接入点）供电，简化了网络布线。

#### 交换机与其他网络设备的区别

- **集线器（Hub）**：集线器工作在OSI模型的第一层（物理层），不能区分设备，它会将接收到的数据广播到所有端口，导致网络效率低下，容易造成冲突。

- **路由器（Router）**：路由器工作在OSI模型的第三层（网络层），用于连接不同的网络，并根据IP地址进行数据包的路由选择。交换机主要用于局域网内的通信，而路由器用于不同网络之间的通信。

- **网桥（Bridge）**：网桥也工作在数据链路层，类似于交换机，但通常只有少数端口，主要用于连接两个局域网或分割网络的冲突域。

#### 交换机的作用

交换机是构建局域网的核心设备，它通过高效的数据转发、减少冲突、支持多设备并行通信等功能，提高了网络的性能和稳定性。在现代网络中，交换机不仅用于传统的有线局域网，还广泛应用于无线网络、数据中心和企业网络环境中。



### linux命令

> 查看 ARP 缓存内容

在 Linux 系统中，我们可以使用 `arp -a` 命令来查看 ARP 缓存的内容。

![20](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408090904262.png)

------



## 1.物理层





------



## 路由和网关

是构建网络和服务架构中的两个重要概念，它们有着不同的功能和作用：

路由（Routing）：

- **功能**：路由是指确定数据包在网络中传输的路径的过程。在网络通信中，路由器根据特定的规则（比如 IP 地址、端口号等），将数据包从源地址传输到目的地址。
- **在网络中的应用**：路由在网络层面上起作用，负责决定数据包应该经过哪条路径，以实现数据的传输。它可以根据不同的条件和规则选择最佳路径，确保数据能够快速、安全地到达目的地。

网关（Gateway）：

- **功能**：网关是一个在不同网络或协议之间转换数据格式、传输协议、安全协议等的设备或服务。在网络通信中，网关充当中间人，将请求从一个系统发送到另一个系统，并确保通信的顺利进行。
- **在网络中的应用**：网关通常作为连接不同网络或协议的接口，用于处理和转发流量。在软件架构中，网关也可以指 API 网关，它用于管理、监控和控制 API 流量，并提供安全、路由、转换等功能。

区别：

1. **功能不同**：路由主要负责数据包在网络中的传输路径选择，而网关则涉及不同网络、协议或服务之间的数据转换和协议适配。
2. **作用范围不同**：路由操作发生在网络层，决定数据包传输的路径，而网关可以处于不同的层级，用于不同系统之间的通信和数据转换。
3. **应用领域不同**：路由主要应用于网络通信，而网关可以用于构建 API 网关、安全网关等在软件架构中的应用。

在实际应用中，路由和网关通常一起使用，网关可以管理和控制特定系统或服务的流量，并使用路由来决定这些流量的传输路径，以实现数据的安全、快速传输。







------



## linux命令



### 网络性能指标

通常是以 4 个指标来衡量网络的性能，分别是带宽、延时、吞吐率、PPS（Packet Per Second），它们表示的意义如下：

- *带宽*，表示链路的最大传输速率，单位是 b/s （比特 / 秒），带宽越大，其传输能力就越强。
- *延时*，表示请求数据包发送后，收到对端响应，所需要的时间延迟。不同的场景有着不同的含义，比如可以表示建立 TCP 连接所需的时间延迟，或一个数据包往返所需的时间延迟。
- *吞吐率*，表示单位时间内成功传输的数据量，单位是 b/s（比特 / 秒）或者 B/s（字节 / 秒），吞吐受带宽限制，带宽越大，吞吐率的上限才可能越高。
- *PPS*，全称是 Packet Per Second（包 / 秒），表示以网络包为单位的传输速率，一般用来评估系统对于网络的转发能力。

当然，除了以上这四种基本的指标，还有一些其他常用的性能指标，比如：

- *网络的可用性*，表示网络能否正常通信；
- *并发连接数*，表示 TCP 连接数量；
- *丢包率*，表示所丢失数据包数量占所发送数据组的比率；
- *重传率*，表示重传网络包的比例；



### ipconfig/all

注释：Config命令是我们经常使用的命令，它可以查看网络连接的情况，比如本机的ip地址，子网掩码，dns配置，dhcp配置等等 /all参数就是显示所有配置的参数。

### ping

```Bash
常用参数选项
ping IP -t--连续对IP地址执行Ping命令，直到被用户以Ctrl+C中断。
-a 以IP地址格式来显示目标主机的网络地址
-l 2000--指定Ping命令中的数据长度为2000字节，而不是缺省的323字节。 
-n--执行特定次数的Ping命令     
-f 在包中发送“不分段”标志。该包将不被路由上的网关分段。 
-i ttl 将“生存时间”字段设置为 ttl 指定的数值。 
-v tos 将“服务类型”字段设置为 tos 指定的数值。 
-r count 在“记录路由”字段中记录发出报文和返回报文的路由。指定的 Count 值最小可以是 1，最大可9 。 
-s count 指定由 count 指定的转发次数的时间邮票。 
-j computer-list 经过由 computer-list 指定的计算机列表的路由报文。中间网关可能分隔连续的计算机（松散的源路由）。允许的最大 IP 地址数目是 9 。 
-k computer-list 经过由 computer-list 指定的计算机列表的路由报文。中间网关可能分隔连续的计算机（严格源路由）。允许的最大 IP 地址数目是 9 。 
-w timeout 
以毫秒为单位指定超时间隔。 
destination-list 指定要校验连接的远程计算机。  
```

### stat

查看文件信息

```Shell
File: filename
Size: 1234                    Blocks: 8          IO Block: 4096   regular file
Device: 801h/2049d        Inode: 567891       Links: 1
Access: (0644/-rw-r--r--)  Uid: ( 1000/ username)   Gid: ( 1000/ groupname)
Access: 2023-11-27 12:34:56.789012345 +0100
Modify: 2023-11-25 09:08:07.654321098 +0100
Change: 2023-11-25 09:08:07.654321098 +0100
 Birth: 2023-11-20 17:43:21.123456789 +0100
```

### netstat

"netstat"（Network Statistics，网络统计）是一个网络工具，用于在计算机上检查网络连接、路由表、网络接口和网络统计信息。这个工具通常在命令行界面中使用，可以帮助您了解计算机的网络连接情况和网络性能。

以下是一些常见的 "netstat" 命令用法：

1. **查看活动的网络连接**：

```Plaintext
netstat -a
```

1. 这将显示所有活动的网络连接，包括本地计算机与远程计算机之间的连接，以及它们的状态（例如，ESTABLISHED、LISTENING、TIME_WAIT 等）。
2. **显示网络接口信息**：

```Plaintext
netstat -i
```

1. 这将列出所有网络接口的信息，包括接口的名称、数据包接收和发送情况，错误等。
2. **显示路由表**：

```Plaintext
netstat -r
```

1. 这将显示操作系统的路由表，包括到达不同目标网络的路由信息。
2. **显示网络统计信息**：

```Plaintext
netstat -s
```

1. 这将提供有关网络协议的统计信息，如TCP、UDP、ICMP等的数据包统计。
2. **显示 PID（进程标识符）和应用程序名称**：

```Plaintext
netstat -b
```

1. 这将显示每个网络连接的关联进程的PID和应用程序名称。这对于查找哪个进程使用了特定的端口或网络连接很有帮助。

"netstat" 命令通常在Windows和Unix/Linux操作系统中可用，可以用于网络故障排除、监控网络连接、查找特定端口的使用情况以及诊断网络性能问题。请注意，具体的 "netstat" 命令选项和输出可能因操作系统而异。



### tracert

用于跟踪数据包从本地计算机发送到目标主机的路径，显示数据包在途中经过的路由器和网络节点。它可以帮助识别网络连接中的瓶颈和问题，以及查看数据包在网络中的传输延迟。 Tracert 命令用 IP 生存时间 (TTL) 字段和 ICMP 错误消息来确定从一个主机到网络上其他主机的路由。

-d：指定不将 IP 地址解析到主机名称。 -hmaximum_hops：指定跃点数以跟踪到称target_name的主机的路由。 -j host-list：指定tracert实用程序数据包所采用路径中的路由器接口列表。 -w timeout：等待timeout为每次回复所指定的毫秒数。

```Bash
tracert -d 8.130.16.80
```



### nslookup

"nslookup" 是一个网络工具，通常用于查询域名系统（DNS）以获取与主机名相关的 IP 地址或域名服务器记录。它可用于检查域名解析是否正确，了解特定主机名的 IP 地址，或者查找特定 IP 地址对应的域名。

NSLOOKUP是NT、2000中连接DNS服务器，查询域名信息的一个非常有用的命令是由 local DNS 的 cache 中直接读出来的, 而不是 local DNS 向真正负责这个 domain 的 name server 问来的。

Nslookup 必须要安装了 TCP/IP 协议的网络环境之后才能使用。

示例：

1. **查询域名对应的 IP 地址**：

```Plaintext
nslookup example.com
```

1. 这将返回域名 "example.com" 对应的 IP 地址。
2. **查询IP地址对应的域名**：

```Plaintext
nslookup 192.168.1.1
```

1. 这将返回IP地址 "192.168.1.1" 对应的域名。
2. **查找域名服务器记录**：

```Plaintext
nslookup -type=NS example.com
```

1. 这将返回域名 "example.com" 的域名服务器记录，显示哪些域名服务器负责解析该域名。
2. **指定自定义DNS服务器**：

```Plaintext
nslookup example.com 8.8.8.8
```

1. 这将使用Google的DNS服务器（8.8.8.8）来查询域名 "example.com" 的IP地址。

```Bash
(1)nslookup 显示，正在工作的DNS服务器的主机名为ns.sdjnptt.net.cn,它的IP地址是202.102.128.68

(2)把123.235.44.38地址反向解析成www.Baidu.com如图所示：
C:\Users\to'm>nslookup -q=ptr 123.235.44.38
服务器:  UnKnown
Address:  192.168.163.168

*** UnKnown 找不到 38.44.235.123.in-addr.arpa.: Non-existent domain
(3)如果出现下面这些，说明测试主机在目前的网络中，根本没有找到可以使用的 DNS 服务器
*** Can't find server name for domain: No response from server 
*** Can't repairpc.nease.net : Non-existent domain
(4)如果出现下面这些，这种情况说明网络中 DNS 服务器 ns-px.online.sh.cn 在工作，却不能实现域名www.Baidu.com的正确解析。
Server：ns-px.online.sh.cn 
Address： 202.96.209.5 
*** ns-px.online.sh.cn can't find www.baidu.com Non-existent domain
```

### curl

C:\Users\to'm>curl -4 ifconfig.me 39.144.178.52 C:\Users\to'm>curl ifconfig.me 2409:8949:c23:2e2b:5d2c:ede:a5d5:19dd C:\Users\to'm>



## 思维导图



![第一章](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408271719885.png)

![第二章](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408271720687.png)

![第三章](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408271720267.png)

![第四章](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408271720546.png)

![第五章](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408271720212.png)

# 操作系统



## 硬件

### 冯诺依曼模型

**运算器、控制器、存储器、输入设备、输出设备**

![_E5_86_AF_E8_AF_BA_E4_BE_9D_E6_9B_BC_E6_A8_A1_E5_9E_8B](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408150901264.png)



<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408151654065.png" alt="_E6_8C_87_E4_BB_A4_E5_91_A8_E6_9C_9F_E5_B7_A5_E4_BD_9C_E7_BB_84_E4_BB_B6" style="zoom: 50%;" />

数据和指令是分开区域存放的，存放指令区域的地方称为「正文段」。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408151658561.png" alt="_E6_95_B0_E6_8D_AE_E6_AE_B5_E4_B8_8E_E6_AD_A3_E6_96_87_E6_AE_B5" style="zoom:50%;" />

### CPU

#### 寄存器种类：

- *通用寄存器*，用来存放需要进行运算的数据，比如需要进行加和运算的两个数据。
- *程序计数器*，用来**存储 CPU 要执行下一条指令「所在的内存地址」**，注意不是存储了下一条要执行的指令
- *指令寄存器*，用来存放当前正在执行的指令，也就是指令本身，指令被执行完成之前，指令都存储在这里



#### 指令的类型

指令从功能角度划分，可以分为 5 大类：

- *数据传输类型的指令*，比如 `store/load` 是寄存器与内存间数据传输的指令，`mov` 是将一个内存地址的数据移动到另一个内存地址的指令；
- *运算类型的指令*，比如加减乘除、位运算、比较大小等等，它们最多只能处理两个寄存器中的数据；
- *跳转类型的指令*，通过修改程序计数器的值来达到跳转执行指令的过程，比如编程中常见的 `if-else`、`switch-case`、函数调用等。
- *信号类型的指令*，比如发生中断的指令 `trap`；
- *闲置类型的指令*，比如指令 `nop`，执行后 CPU 会空转一个周期；
- ![_E7_A8_8B_E5_BA_8F_E7_9A_84CPU_E6_89_A7_E8_A1_8C_E6_97_B6_E9_97_B4_E5_85_AC_E5_BC_8F2](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408151656261.png)



#### 线程调度

在 Linux 内核中，进程和线程都是用 `task_struct` 结构体表示的，区别在于线程的 task_struct 结构体里部分资源是共享了进程已创建的资源，比如内存地址空间、代码段、文件描述符等，所以 Linux 中的线程也被称为轻量级进程，

Linux 内核里的调度器，调度的对象就是 `task_struct`，接下来我们就把这个数据结构统称为**任务**。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161123313.png" alt="_E4_BB_BB_E5_8A_A1" style="zoom:50%;" />



每个 CPU 都有自己的**运行队列（\*Run Queue, rq\*）**优先级如下：Deadline > Realtime > Fair

**实时任务总是会比普通任务优先被执行**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161135783.png" alt="_E8_B0_83_E5_BA_A6_E7_B1_BB" style="zoom:50%;" />

Deadline 和 Realtime 这两个调度类，都是应用于实时任务的，这两个调度类的调度策略合起来共有这三种，它们的作用如下：

- *SCHED_DEADLINE*：是按照 deadline 进行调度的，距离当前时间点最近的 deadline 的任务会被优先调度；
- *SCHED_FIFO*：对于相同优先级的任务，按先来先服务的原则，但是优先级更高的任务，可以抢占低优先级的任务，也就是优先级高的可以「插队」；
- *SCHED_RR*：对于相同优先级的任务，轮流着运行，每个任务都有一定的时间片，当用完时间片的任务会被放到队列尾部，以保证相同优先级任务的公平性，但是高优先级的任务依然可以抢占低优先级的任务；

而 Fair 调度类是应用于普通任务，都是由 CFS 调度器管理的，分为两种调度策略：

- *SCHED_NORMAL*：普通任务使用的调度策略；
- *SCHED_BATCH*：后台任务的调度策略，不和终端进行交互，因此在不影响其他需要交互的任务，可以适当降低它的优先级





##### 完全公平调度

对于普通任务来说，公平性最重要，**完全公平调度**（Completely Fair Scheduling），基于 CFS 的调度算法，。

这个算法的理念是想让分配给每个任务的 CPU 时间是一样，于是它为每个任务安排一个虚拟运行时间 vruntime，如果一个任务在运行，其运行的越久，该任务的 vruntime 自然就会越大，而没有被运行的任务，vruntime 是不会变化的。

那么，**在 CFS 算法调度的时候，会优先选择 vruntime 少的任务**，以保证每个任务的公平性。

在计算虚拟运行时间 vruntime 还要考虑普通任务的**权重值**，

注意权重值并不是优先级的值，内核中会有一个 nice 级别与权重值的转换表，nice 级别越低的权重值就越大，至于 nice 值是什么，我们后面会提到。 于是就有了以下这个公式：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/vruntime.png)

##### 调整优先级

如果我们启动任务的时候，没有特意去指定优先级的话，默认情况下都是普通任务，普通任务的调度类是 Fair，由 CFS 调度器来进行管理。CFS 调度器的目的是实现任务运行的公平性，也就是保障每个任务的运行的时间是差不多的。

如果你想让某个普通任务有更多的执行时间，可以调整任务的 `nice` 值，从而让优先级高一些的任务执行更多时间。nice 的值能设置的范围是 `-20～19`， 值越低，表明优先级越高，因此 -20 是最高优先级，19 则是最低优先级，默认优先级是 0。

是不是觉得 nice 值的范围很诡异？事实上，nice 值并不是表示优先级，而是表示**优先级的修正数值**，它与优先级（priority）的关系是这样的：priority(new) = priority(old) + nice。内核中，priority 的范围是 0~139，值越低，优先级越高，其中前面的 0~99 范围是提供给实时任务使用的，而 nice 值是映射到 100~139，这个范围是提供给普通任务用的，因此 nice 值调整的是普通任务的优先级。

![_E4_BC_98_E5_85_88_E7_BA_A7](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161151751.png)

##### Linux指令

我们可以在启动任务的时候，可以指定 nice 的值，比如将 mysqld 以 -3 优先级：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161153385.png" alt="nice" style="zoom:80%;" />

如果想修改已经运行中的任务的优先级，则可以使用 `renice` 来调整 nice 值：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161153483.png" alt="renice (1)" style="zoom:67%;" />

nice 调整的是普通任务的优先级，所以不管怎么缩小 nice 值，任务永远都是普通任务，如果某些任务要求实时性比较高，那么你可以考虑改变任务的优先级以及调度策略，使得它变成实时任务，比如：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161154299.png" alt="chrt (1)" style="zoom:67%;" />



### 中断

在计算机中，中断是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的中断处理程序来响应请求。

中断处理程序，要**尽可能快的执行完**，这样可以减少对正常进程运行调度地影响。

中断处理程序分为两部分：

- 上半部，对应硬中断，由硬件触发中断，主要是负责耗时短的工作，特点是快速执行；

- 下半部，对应软中断，由内核触发中断，用来异步处理上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行；

  内核（Kernel）不是硬件

  - **软件中断**其实指的就是系统调用等从用户态到内核态的行为。

还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，而软中断（下半部）是以内核线程的方式执行，并且**每一个 CPU 都对应一个软中断内核线程**，名字通常为「ksoftirqd/CPU 编号」，比如 0 号 CPU 对应的软中断内核线程的名字是 `ksoftirqd/0`

不过，软中断不只是包括硬件设备中断处理程序的下半部，一些**内核自定义事件**也属于软中断，比如内核调度等、RCU 锁（内核里常用的一种锁）等





### **存储器**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408160904955.png" alt="_E5_AD_98_E5_82_A8_E5_8C_BA_E5_88_86_E7_BA_A7" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408160941171.png" alt="_E8_AE_BF_E9_97_AE_E9_80_9F_E5_BA_A6_E8_A1_A8_E6_A0_BC" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408160914568.png" alt="_E7_BC_93_E5_AD_98_E4_BD_93_E7_B3_BB1" style="zoom:33%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408160915502.png" alt="_E5_AD_98_E5_82_A8_E5_99_A8_E6_88_90_E6_9C_AC_E7_9A_84_E5_AF_B9_E6_AF_94" style="zoom:50%;" />



#### 寄存器

的访问速度非常快，一般要求在**半个 CPU 时钟周期**内完成读写，CPU 时钟周期跟 CPU 主频息息相关，比如 2 GHz 主频的 CPU，那么它的时钟周期就是 1/2G，也就是 0.5ns（纳秒）

#### CPU Cache

 用的是一种叫 **SRAM（\*Static Random-Access\* Memory，静态随机存储器）** 的芯片。

在 SRAM 里面，一个 bit 的数据，通常需要 6 个晶体管，所以 SRAM 的存储密度不高，同样的物理空间下，能存储的数据是有限的，不过也因为 SRAM 的电路简单，所以访问速度非常快。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408160942528.png" alt="Cache_E7_9A_84_E6_95_B0_E6_8D_AE_E7_BB_93_E6_9E_84" style="zoom:50%;" />

##### L1 高速缓存

分成**指令缓存**和**数据缓存**。

L1 高速缓存的访问速度几乎和寄存器一样快，通常只需要 `2~4` 个时钟周期，而大小在几十 KB 到几百 KB 不等。

##### L2 高速缓存

同样每个 CPU 核心都有，通常大小在几百 KB 到几 MB 不等，访问速度则更慢，速度在 `10~20` 个时钟周期。

##### L3 高速缓存

L3 高速缓存通常是多个 CPU 核心共用的，通常大小在几 MB 到几十 MB 不等，具体值根据 CPU 型号而定。

访问速度相对也比较慢一些，访问速度在 `20~60`个时钟周期。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408160908386.png" alt="CPU-Cache" style="zoom: 33%;" />

##### 访问

CPU 在从 CPU Cache 读取数据的时候，并不是读取 CPU Cache Line 中的整个数据块，而是读取 CPU 所需要的一个数据片段，这样的数据统称为一个**字（Word）**



**直接映射 Cache（\*Direct Mapped Cache\*）**

如果内存中的数据已经在 CPU Cache 中了，那 CPU 访问一个内存地址的时候，会经历这 4 个步骤：

1. 根据内存地址中索引信息，计算在 CPU Cache 中的索引，也就是找出对应的 CPU Cache Line 的地址；
2. 找到对应 CPU Cache Line 后，判断 CPU Cache Line 中的有效位，确认 CPU Cache Line 中数据是否是有效的，如果是无效的，CPU 就会直接访问内存，并重新加载数据，如果数据有效，则往下执行；
3. 对比内存地址中组标记和 CPU Cache Line 中的组标记，确认 CPU Cache Line 中的数据是我们要访问的内存数据，如果不是的话，CPU 就会直接访问内存，并重新加载数据，如果是的话，则往下执行；
4. 根据内存地址中偏移量信息，从 CPU Cache Line 的数据块中，读取对应的字。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408160947153.png" alt="_E7_9B_B4_E6_8E_A5Cache_E6_98_A0_E5_B0_84" style="zoom:50%;" />



CPU 跑得更快的代码，就需要写出缓存命中率高的代码，CPU L1 Cache 分为数据缓存和指令缓存，因而需要分别提高它们的缓存命中率：

- 对于数据缓存，我们在遍历数据的时候，应该按照内存布局的顺序操作，这是因为 CPU Cache 是根据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时，性能能得到有效的提升；
- 对于指令缓存，有规律的条件分支语句能够让 CPU 的分支预测器发挥作用，进一步提高执行的效率；

另外，对于多核 CPU 系统，线程可能在不同 CPU 核心来回切换，这样各个核心的缓存命中率就会受到影响，于是要想提高线程的缓存命中率，可以考虑把线程绑定 CPU 到某一个 CPU 核心。



##### 数据写入方式

###### 写直达

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408160958433.png" alt="_E5_86_99_E7_9B_B4_E8_BE_BE" style="zoom:50%;" />

###### 写回

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408160958469.png" alt="_E5_86_99_E5_9B_9E1" style="zoom:50%;" />

##### 缓存一致性

现在 CPU 都是多核的，由于 L1/L2 Cache 是多个核心各自独有的，那么会带来多核心的**缓存一致性**

- 第一点，某个 CPU 核心里的 Cache 数据更新时，必须要传播到其他核心的 Cache，这个称为**写传播（\*Write Propagation\*）**
  - 最常见实现的方式是**总线嗅探（\*Bus Snooping\*）**。
    - 当某个 CPU 核心更新了 Cache 中的数据，要把该事件广播通知到其他核心。
- 第二点，某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为**事务的串行化（\*Transaction Serialization\*）**。
  - 要实现事务串行化，要做到 2 点：
    - CPU 核心对于 Cache 中数据的操作，需要同步给其他 CPU 核心；
    - 要引入「锁」的概念，如果两个 CPU 核心里有相同数据的 Cache，那么对于这个 Cache 数据的更新，只有拿到了「锁」，才能进行对应的数据更新。

###### MESI 协议 

MESI 协议实现CPU 缓存一致性

- *Modified*，已修改
- *Exclusive*，独占
- *Shared*，共享
- *Invalidated*，已失效

「已修改」状态就是脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存里。

「已失效」状态，表示的是这个 Cache Block 里的数据已经失效了，不可以读取该状态的数据。

「独占」和「共享」状态都代表 Cache Block 里的数据是干净的，也就是说，这个时候 Cache Block 里的数据和内存里面的数据是一致性的。

「独占」和「共享」的差别在于，独占状态的时候，数据只存储在一个 CPU 核心的 Cache 里，而其他 CPU 核心的 Cache 没有该数据。这个时候，如果要向独占的 Cache 写数据，就可以直接自由地写入，而不需要通知其他 CPU 核心，因为只有你这有这个数据，就不存在缓存一致性的问题了，于是就可以随便操作该数据。

另外，在「独占」状态下的数据，如果有其他核心从内存读取了相同的数据到各自的 Cache ，那么这个时候，独占状态下的数据就会变成共享状态。

那么，「共享」状态代表着相同的数据在多个 CPU 核心的 Cache 里都有，所以当我们要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后再更新当前 Cache 里面的数据。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161010566.png" alt="MESI_E5_8D_8F_E8_AE_AE" style="zoom:50%;" />

##### 伪共享

因为多个线程同时读写**同一个 Cache Line 的不同变量**时，而导致 CPU Cache 失效的现象

​	 1 号和 2 号 CPU 核心持续交替的分别修改变量 A 和 B，但是因为同时归属于一个 Cache Line ，这个 Cache Line 中的任意数据被修改后，都会相互影响，导致不断失效

伪共享（False Sharing）是并发编程中一个常见的性能问题，特别是在多线程环境下。它发生在多个线程同时修改共享缓存行中的不同数据时，由于缓存一致性协议，导致缓存行在多个处理器之间频繁地无效和重新加载，从而严重影响系统性能。

###### 避免伪共享的方法：

1. **数据填充（Padding）**：
   - 通过在共享数据之间插入填充数据（如字节或无关的变量）来强制将这些数据放在不同的缓存行中。这样，线程操作的数据就不会在同一缓存行中，避免了伪共享的发生。
   - 在C和C++中，常见的做法是使用`__declspec(align(N))`或`alignas(N)`指令来对齐结构体或变量。

2. **将相关数据组合在一起（Data Structuring）**：
   - 尽量将相关的数据组合在一个结构体或数组中，避免不相关的数据共享同一个缓存行。
   - 例如，可以将一个线程使用的数据保存在同一个结构体或数组的同一区域中，以确保这些数据在同一个缓存行中，减少不必要的缓存行切换。

3. **避免频繁写入共享变量**：
   - 尽量减少多个线程对同一缓存行中不同数据的写入操作。如果可能，使用局部变量代替共享变量，减少写入操作对缓存行的影响。

4. **使用缓存对齐（Cache Alignment）**：
   - 确保数据在内存中的位置是按缓存行对齐的，这样可以减少伪共享的机会。许多编译器支持通过特定指令强制数据对齐到缓存行边界。

5. **分离读写操作（Read-Write Separation）**：
   - 将频繁读取和写入的数据分开存储。通过将写入操作和读取操作的数据分离到不同的缓存行中，可以减少伪共享的影响。

6. **使用线程本地存储（Thread-Local Storage, TLS）**：
   - 将需要频繁访问的数据放到线程本地存储中，使得每个线程有自己独立的数据副本，避免多个线程共享同一数据。



#### 内存

 **DRAM （\*Dynamic Random Access Memory\*，动态随机存取存储器）**

相比 SRAM，DRAM 的密度更高，功耗更低，有更大的容量，而且造价比 SRAM 芯片便宜很多。

DRAM 存储一个 bit 数据，只需要一个晶体管和一个电容就能存储，但是因为数据会被存储在电容里，电容会不断漏电，所以需要「定时刷新」电容，才能保证数据不会被丢失

DRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问的速度会更慢，内存速度大概在 `200~300` 个 时钟周期之间



####  SSD/HDD 硬盘

SSD（*Solid-state disk*） 就是我们常说的固体硬盘，结构和内存类似，但是它相比内存的优点是断电后数据还是存在的，而内存、寄存器、高速缓存断电后数据都会丢失。内存的读写速度比 SSD 大概快 `10~1000` 倍。

当然，还有一款传统的硬盘，也就是机械硬盘（*Hard Disk Drive, HDD*），它是通过物理读写的方式来访问数据的，因此它访问速度是非常慢的，它的速度比内存慢 `10W` 倍左右。

由于 SSD 的价格快接近机械硬盘了，因此机械硬盘已经逐渐被 SSD 替代了。



#### 磁盘

中间圆的部分是磁盘的盘片，一般会有多个盘片，每个盘面都有自己的磁头。盘片中的每一层分为多个磁道，每个磁道分多个扇区，每个扇区是 `512` 字节。那么，多个具有相同编号的磁道形成一个圆柱，称之为磁盘的柱面，如上图里中间的样子。



##### 磁盘调度算法

1. **FCFS**：简单但效率低，可能导致较长寻道时间。
2. **SSTF**：优先处理距离最近的请求，但可能导致饥饿问题。
3. **SCAN**：磁头像电梯一样在磁盘上来回移动，减少频繁跳跃问题。
4. **C-SCAN**：类似 SCAN，但只单向处理请求，回到另一端时不处理，公平性更高。
5. **LOOK**：与 SCAN 类似，但只在有请求的范围内移动。
6. **C-LOOK**：与 LOOK 类似，但像 C-SCAN 一样循环处理请求。



- 先来先服务算法
- 最短寻道时间优先算法
- 扫描算法*SCAN* 
- 循环扫描算法*CSCAN* 
- LOOK 与 C-LOOK 算法
  - **磁头在移动到「最远的请求」位置，然后立即反向移动。**
  - 针对 SCAN 算法的优化则叫 LOOK 算法，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，**反向移动的途中会响应请求**。
  - 针 C-SCAN 算法的优化则叫 C-LOOK，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，**反向移动的途中不会响应请求**。



## 操作系统结构

操作系统是管理计算机硬件和软件资源的系统软件，为用户和应用程序提供一个友好的操作环境。

**组成部分**：

1. **内核**：操作系统的核心部分，提供底层功能。
2. **系统库**：一组函数库，供应用程序调用，简化程序开发。
3. **系统工具和实用程序**：包括各种管理和维护工具，如任务管理器、磁盘管理工具等。
4. **用户界面**：提供与用户交互的界面，可能是命令行界面（CLI）或图形用户界面（GUI）。



**Linux操作系统**：

- **内核**：Linux内核负责底层硬件控制和资源管理。
- **系统库**：如GNU C库（glibc）提供标准函数库。
- **系统工具和实用程序**：如`bash`、`ls`、`grep`等工具。
- **用户界面**：可能是命令行界面（如`bash`）或图形界面（如GNOME、KDE）。

**Windows操作系统**：

- **内核**：Windows NT内核负责底层资源管理。

- **系统库**：如Win32 API提供标准函数库。

- **系统工具和实用程序**：如任务管理器、文件资源管理器等工具。

- **用户界面**：图形用户界面，通常是Windows桌面环境。

  



### 内核（Kernel）

内核是操作系统的核心部分，**应用连接硬件设备的桥梁**，负责管理系统资源和为上层软件提供底层服务。

主要为分为：

- 宏内核，包含多个模块，整个内核像一个完整的程序；
- 微内核，有一个最小版本的内核，一些模块和服务则由用户态管理；
- 混合内核，是宏内核和微内核的结合体，内核中抽象出了微内核的概念，也就是内核中会有一个小型的内核，其他模块就在这个基础上搭建，整个内核是个完整的程序；

**主要功能**：

1. **进程管理**：调度和管理进程，确保进程的创建、执行和终止。
2. **内存管理**：管理系统的内存，包括内存分配和回收、虚拟内存的实现等。
   - 内存分成了两个区域：
     - 内核空间，这个内存空间只有内核程序可以访问；
     - 用户空间，这个内存空间专门给应用程序使用；

3. **文件系统管理**：提供对文件系统的访问，包括文件的读取、写入、创建和删除。
4. **设备驱动管理**：控制和管理硬件设备，通过驱动程序与硬件交互。
5. **系统调用接口**：提供一组系统调用接口，供用户程序访问内核服务。



#### Linux 内核

- *MultiTask*，多任务

  - 「同时」可以是并发（单核）或并行（多核）

- *SMP*，对称多处理

  - 每个 CPU 的地位是相等的，对资源的使用权限也是相同的，多个 CPU 共享同一个内存，每个 CPU 都可以访问完整的内存和硬件资源。

- *ELF*，可执行文件链接格式

  - Linux 操作系统中可执行文件的存储格式
  - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161640978.png" alt="Elf (1)" style="zoom:25%;" />

- *Monolithic Kernel*，宏内核

  - 意味着 Linux 的内核是一个完整的可执行程序，且拥有最高的权限。

    宏内核的特征是系统内核的所有模块，比如进程调度、内存管理、文件系统、设备驱动等，都运行在内核态。

    不过，Linux 也实现了动态加载内核模块的功能，例如大部分设备驱动是以可加载模块的形式存在的，与内核其他模块解藕，让驱动开发和驱动加载更为方便、灵活。

#### windows

Windows NT，NT 全称叫 New Technology ，**Window 的内核设计是混合型内核**

Windows 的可执行文件格式叫 PE，称为**可移植执行文件**，扩展名通常是`.exe`、`.dll`、`.sys`等。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161648057.png" alt="pe" style="zoom: 50%;" />

![windowNT (1)](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161647053.png)



### "陷阱"（Trap）

在计算机科学和操作系统中是一个通用的概念，它表示计算机系统中的异常事件或中断。陷阱是一种用于处理和响应异常情况的机制，通常包括以下几个方面：

1. **异常事件**：陷阱通常用于捕获和处理计算机系统中发生的异常情况，如除零错误、内存访问违规、非法指令等。当这些异常事件发生时，系统会引发一个陷阱。
2. **系统调用**：在操作系统中，陷阱还用于实现系统调用。程序可以通过系统调用请求操作系统提供特定的服务，如文件操作、进程管理等。当程序执行系统调用时，会触发一个特定的陷阱，操作系统会根据请求执行相应的操作。
3. **中断**：硬件设备（如键盘、鼠标、定时器等）可以通过触发中断信号来通知计算机处理器需要执行某些操作。中断通常用陷阱来处理，操作系统会调用适当的中断处理程序来响应中断事件。
4. **陷阱处理程序**：陷阱通常与一个或多个陷阱处理程序相关联，这些处理程序定义了在异常情况下系统应该如何响应。处理程序可能会记录错误、终止程序、执行恢复操作等。

总的来说，陷阱是一种用于处理异常情况和中断事件的机制，可以确保计算机系统在遇到问题时有适当的应对措施。陷阱的概念在操作系统、编程和计算机体系结构中都有广泛的应用。不同的操作系统和硬件体系结构可能会有不同的陷阱类型和陷阱处理程序。

陷阱表

通常在计算机系统、操作系统和编程领域中使用。它指的是一种数据结构或机制，用于处理异常、中断和陷阱事件的管理和分发。这些事件包括操作系统内部的错误、系统调用、外部硬件中断等。

陷阱表通常由操作系统或处理器管理，并包括一系列条目，每个条目与特定的异常或中断事件相关联。每个条目通常包括以下信息：

1. 事件类型：描述异常或中断的类型，例如除零错误、内存访问违规、系统调用等。
2. 处理程序地址：指定了当事件发生时要执行的处理程序或函数的地址。这个处理程序通常被称为"中断处理程序"或"异常处理程序"。
3. 处理程序参数：传递给处理程序的附加信息，可能包括有关事件的上下文信息、错误代码等。

当发生与陷阱表中的某个事件相关的异常或中断时，操作系统或处理器会查找陷阱表，找到相应的条目，并执行相应的处理程序。这允许操作系统或应用程序对各种异常事件做出适当的响应，例如记录错误、恢复系统状态、终止程序或执行其他必要的操作。

陷阱表在操作系统的设计中起到了关键作用，它们使得操作系统能够有效地管理和响应不同类型的异常和中断，从而提高了系统的可靠性和稳定性。



## 内存管理

### 虚拟内存

**单片机的 CPU 是直接操作内存的「物理地址」**。

**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

- 我们程序所使用的内存地址叫做**虚拟内存地址**（*Virtual Memory Address*）
- 实际存在硬件里面的空间地址叫**物理内存地址**（*Physical Memory Address*）。

进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181041646.png" alt="72ab76ba697e470b8ceb14d5fc5688d9" style="zoom:50%;" />

> 虚拟内存有什么作用？

- **第一，虚拟内存可以使得进程对运行内存超过物理内存大小**，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。
- 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就**解决了多进程之间地址冲突的问题**。
- 第三，页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，**操作系统提供了更好的安全性**。



### linux内存管理

Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是所有的段的起始地址都是一样的。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181052335.png" alt="3a6cb4e3f27241d3b09b4766bb0b1124-20230309234553726" style="zoom:33%;" />

是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181053002.png" alt="48403193b7354e618bf336892886bcff" style="zoom:50%;" />



从**低到高**分别是 6 种不同的内存段：

- 代码段，包括二进制可执行代码；
- 数据段，包括已初始化的静态常量和全局变量；
- BSS 段，包括未初始化的静态变量和全局变量；
- 堆段，包括动态分配的内存，从低地址开始向上增长；
- 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（[跟硬件和内核版本有关 (opens new window)](http://lishiwen4.github.io/linux/linux-process-memory-location)）；
- 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 `8 MB`。当然系统也提供了参数，以便我们自定义大小；

代码段下面还有一段内存空间的（灰色部分），这一块区域是「保留区」，之所以要有保留区这是因为在大多数的系统里，我们认为比较小数值的地址不是一个合法地址，例如，我们通常在 C 的代码里会将无效的指针赋值为 NULL。因此，这里会出现一段不可访问的内存保留区，防止程序因为出现 bug，导致读或写了一些小内存地址的数据，而使得程序跑飞。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181053859.png" alt="32_E4_BD_8D_E8_99_9A_E6_8B_9F_E5_86_85_E5_AD_98_E5_B8_83_E5_B1_80" style="zoom:50%;" />

#### malloc() 

并不是系统调用，而是 C 库里的函数，用于动态分配内存。**malloc() 分配的是虚拟内存**

malloc 申请内存的时候，会有两种方式向操作系统**申请堆内存**。

- 方式一：通过 brk() 系统调用从堆分配内存
  - 方式一实现的方式很简单，就是通过 brk() 函数将「**堆**顶」指针向高地址移动，获得新的内存空间。
- 方式二：通过 mmap() 系统调用在文件映射区域分配内存；
  - 方式二通过 mmap() 系统调用中「私有匿名映射」的方式，在**文件映射区**分配一块内存，也就是从文件映射区“偷”了一块内存。

malloc() 源码里默认定义了一个阈值：

- 如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；
- 如果用户分配的内存大于 128 KB，则通过 mmap() 申请内存；

注意，不同的 glibc 版本定义的阈值也是不同的



####  malloc(1)

 会分配多大的虚拟内存？

malloc() 在分配内存的时候，并不是老老实实按用户预期申请的字节数来分配内存空间大小，而是**会预分配更大的空间作为内存池**。

具体会预分配多大的空间，跟 malloc 使用的内存管理器有关系，我们就以 malloc 默认的内存管理器（Ptmalloc2）来分析。

**malloc(1) 实际上预分配 132K 字节的内存**。



#### free 释放内存

「malloc 申请的内存，free 释放内存会归还给操作系统吗？」这个问题，我们可以做个总结了：

- malloc 通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**；
- malloc 通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**。



#### 为什么不全部使用 mmap 来分配内存？

**频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大**。



### 内存分配过程

应用程序通过 malloc 函数申请内存的时候，实际上申请的是虚拟内存，此时并不会分配物理内存。

当应用程序读写了这块虚拟内存，CPU 就会去访问这个虚拟内存， 这时会发现这个虚拟内存没有映射到物理内存， CPU 就会产生**缺页中断**，进程会从用户态切换到内核态，并将缺页中断交给内核的 Page Fault Handler （缺页中断函数）处理。

缺页中断处理函数会看是否有空闲的物理内存，如果有，就直接分配物理内存，并建立虚拟内存与物理内存之间的映射关系。

如果没有空闲的物理内存，那么内核就会开始进行**回收内存**的工作，回收的方式主要是两种：直接内存回收和后台内存回收。

- **后台内存回收**（kswapd）：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收内存的过程**异步**的，不会阻塞进程的执行。
- **直接内存回收**（direct reclaim）：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是**同步**的，会阻塞进程的执行。

可被回收的内存类型有文件页和匿名页：

- **文件页**（File-backed Page）：内核缓存的磁盘数据（Buffer）和内核缓存的文件数据（Cache）都叫作文件页。大部分文件页，都可以直接释放内存，以后有需要时，再从磁盘重新读取就可以了。而那些被应用程序修改过，并且暂时还没写入磁盘的数据（也就是脏页），就得先写入磁盘，然后才能进行内存释放。所以，**回收干净页的方式是直接释放内存，回收脏页的方式是先写回磁盘后再释放内存**。
- **匿名页**（Anonymous Page）：这部分内存没有实际载体，不像文件缓存有硬盘文件这样一个载体，比如堆、栈数据等。这部分内存很可能还要再次被访问，所以不能直接释放内存，它们**回收的方式是通过 Linux 的 Swap 机制**，Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。

文件页和匿名页的回收都是基于 LRU 算法，也就是优先回收不常访问的内存。LRU 回收算法，实际上维护着 active 和 inactive 两个双向链表，其中：

- **active_list** 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；
- **inactive_list** 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；

针对回收内存导致的性能影响，常见的解决方式。

- 设置 /proc/sys/vm/swappiness，调整文件页和匿名页的回收倾向，尽量倾向于回收文件页；
- 设置 /proc/sys/vm/min_free_kbytes，调整 kswapd 内核线程异步回收内存的时机；
- 设置 /proc/sys/vm/zone_reclaim_mode，调整 NUMA 架构下内存回收策略，建议设置为 0，这样在回收本地内存之前，会在其他 Node 寻找空闲内存，从而避免在系统还有很多空闲内存的情况下，因本地 Node 的本地内存不足，发生频繁直接内存回收导致性能下降的问题；

如果直接内存回收后，空闲的物理内存仍然无法满足此次物理内存的申请，那么内核就会放最后的大招了 ——**触发 OOM （Out of Memory）机制**。

OOM Killer 机制会根据算法选择一个占用物理内存较高的进程，然后将其杀死，以便释放内存资源，如果物理内存依然不足，OOM Killer 会继续杀死占用物理内存较高的进程，直到释放足够的内存位置。

我们可以通过调整进程的 /proc/[pid]/oom_score_adj 值，来降低被 OOM killer 杀掉的概率。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408271451084.png" alt="2f61b0822b3c4a359f99770231981b07" style="zoom:50%;" />



### 内存分段

程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（\*Segmentation\*）的形式把这些段分离出来。**

分段机制下的虚拟地址由两部分组成，**段选择因子**和**段内偏移量**。

段选择因子和段内偏移量：

- **段选择子**就保存在段寄存器里面。段选择子里面最重要的是**段号**，用作段表的索引。**段表**里面保存的是这个**段的基地址、段的界限和特权等级**等。
- 虚拟地址中的**段内偏移量**应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

不足之处：

- 外部**内存碎片**的问题。
- **内存交换的效率低**的问题。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181042409.png" alt="a9ed979e2ed8414f9828767592aadc21" style="zoom:50%;" />





### 内存分页

#### 介绍

在 Linux 下，每一页的大小为 `4KB`。

而当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。

**只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。**

- **有内部内存碎片**



<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181046402.png" alt="08a8e315fedc4a858060db5cb4a654af" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181047913.png" alt="7884f4d8db4949f7a5bb4bbd0f452609" style="zoom:50%;" />

#### 页表项

页表项通常有如下图的字段：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291553871.png" alt="_E9_A1_B5_E8_A1_A8_E9_A1_B9_E5_AD_97_E6_AE_B5" style="zoom:50%;" />

- *状态位*：用于表示该页是否有效，也就是说是否在物理内存中，供程序访问时参考。
- *访问字段*：用于记录该页在一段时间被访问的次数，供页面置换算法选择出页面时参考。
- *修改位*：表示该页在调入内存后是否有被修改过，由于内存中的每一页都在磁盘上保留一份副本，因此，如果没有修改，在置换该页时就不需要将该页写回到磁盘上，以减少系统的开销；如果已经被修改，则将该页重写到磁盘上，以保证磁盘中所保留的始终是最新的副本。
- *硬盘地址*：用于指出该页在硬盘上的地址，通常是物理块号，供调入该页时使用。



#### **多级页表**

64 位的系统，两级分页肯定不够了，就变成了四级目录，分别是：

- 全局页目录项 PGD（*Page Global Directory*）；
- 上层页目录项 PUD（*Page Upper Directory*）；
- 中间页目录项 PMD（*Page Middle Directory*）；
- 页表项 PTE（*Page Table Entry*）；

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181048130.png" alt="_E5_9B_9B_E7_BA_A7_E5_88_86_E9_A1_B5" style="zoom: 33%;" />

####  **TLB**

（*Translation Lookaside Buffer*） ，通常称为页表缓存、转址旁路缓存、快表等。

局限性原理

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181048469.png" alt="a3cdf27646b24614a64cfc5d7ccffa35" style="zoom:33%;" />



### 段页式

地址结构就由**段号、段内页号和页内位移**三部分组成。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181051635.png" alt="8904fb89ae0c49c4b0f2f7b5a0a7b099" style="zoom:50%;" />



### Intel内存

**页式内存管理的作用是在由段式内存管理所映射而成的地址上再加上一层地址映射。**

- 程序所使用的地址，通常是没被段式内存管理映射的地址，称为逻辑地址；
- 通过段式内存管理映射的地址，称为线性地址，也叫虚拟地址；

![bc0aaaf379fc4bc8882efd94b9052b64](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408181052134.png)



### 避免预读失效和缓存污染

Redis 的缓存淘汰算法则是通过**实现 LFU 算法**来避免「缓存污染」而导致缓存命中率下降的问题（Redis 没有预读机制）。

MySQL 和 Linux 操作系统是通过**改进 LRU 算法**来避免「预读失效和缓存污染」而导致缓存命中率下降的问题。

#### Linux 操作系统的缓存

在应用程序读取文件的数据的时候，Linux 操作系统是会对读取的文件数据进行缓存的，会缓存在文件系统中的 **Page Cache**（如下图中的页缓存）

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408271638494.png" alt="_E8_99_9A_E6_8B_9F_E6_96_87_E4_BB_B6_E7_B3_BB_E7_BB_9F" style="zoom:50%;" />

#### MySQL 的缓存

MySQL 的数据是存储在磁盘里的，为了提升数据库的读写性能，Innodb 存储引擎设计了一个**缓冲池**（Buffer Pool），Buffer Pool 属于内存空间里的数据。

有了缓冲池后：

- 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。
- 当修改数据时，首先是修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页，最后由后台线程将脏页写入到磁盘。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408271639925.png" alt="_E7_BC_93_E5_86_B2_E6_B1_A0.drawio" style="zoom:50%;" />



#### 改进LRU 

传统的 LRU 算法的实现思路是这样的：

- 当访问的页在内存里，就直接把该页对应的 LRU 链表节点移动到链表的头部。
- 当访问的页不在内存里，除了要把该页放入到 LRU 链表的头部，还要淘汰 LRU 链表末尾的页。

为了避免「预读失效」造成的影响，Linux 和 MySQL 对传统的 LRU 链表做了改进：

- Linux 操作系统实现两个了 LRU 链表：**活跃 LRU 链表（active list）和非活跃 LRU 链表（inactive list）**。
  - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408271641032.png" alt="active_inactive_list2.drawio" style="zoom:50%;" />
- MySQL Innodb 存储引擎是在一个 LRU 链表上划分来 2 个区域：**young 区域 和 old 区域**。
  - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408271642237.png" alt="lrutwo.drawio" style="zoom:50%;" />

但是如果还是使用「只要数据被访问一次，就将数据加入到活跃 LRU 链表头部（或者 young 区域）」这种方式的话，那么**还存在缓存污染的问题**。

为了避免「缓存污染」造成的影响，Linux 操作系统和 MySQL Innodb 存储引擎分别提高了升级为热点数据的门槛：

- Linux 操作系统：在内存页被访问**第二次**的时候，才将页从 inactive list 升级到 active list 里。

- MySQL Innodb：在内存页被访问第二次的时候，并不会马上将该页从 old 区域升级到 young 区域，因为还要进行

  停留在 old 区域的时间判断：

  - 如果第二次的访问时间与第一次访问的时间**在 1 秒内**（默认值），那么该页就**不会**被从 old 区域升级到 young 区域；
- 如果第二次的访问时间与第一次访问的时间**超过 1 秒**，那么该页就**会**从 old 区域升级到 young 区域；

通过提高了进入 active list （或者 young 区域）的门槛后，就很好了避免缓存污染带来的影响。



> 为什么第二次访问和第一次访问之间的间隔短却不会从 old 上升到 young？访问的频率高不应该上升到 young 吗？

**短期频繁访问的页**：如果一个内存页在 `old` 区域内被访问多次，但每次访问的时间间隔很短（默认是 1 秒以内），InnoDB 认为这个页的**访问行为可能是短期的、临时的**。例如，这种情况可能是由于某些批量操作、突发查询或者扫描导致的。将这些页立即升级到 `young` 区域，可能会导致 `young` 区域被大量短期使用的页占据，进而挤占真正长期频繁访问的页，影响缓存命中率。

**长期稳定访问的页**：如果一个内存页在 `old` 区域被访问，但访问间隔超过 1 秒（默认值），InnoDB 认为这个页更有可能是一个稳定的、长期被频繁访问的页。因此，这个页更值得升级到 `young` 区域，享受更多的缓存时间





### 内存页面置换算法

**当出现缺页异常，需调入新页面而内存已满时，选择被置换的物理页面**，

- 最佳页面置换算法（*OPT*）

  - 置换在「未来」最长时间不访问的页面，无法实现

- 先进先出置换算法（*FIFO*）

- 最近最久未使用的置换算法（*LRU*） 时间

- 最不常用置换算法（*LFU*）次数

- 时钟页面置换算法（*Lock*）

  - 把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。

    当发生缺页中断时，算法首先检查表针指向的页面：

    - 如果它的访问位位是 0 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；
    - 如果访问位是 1 就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；

  



<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291551328.png" alt="_E8_99_9A_E6_8B_9F_E5_86_85_E5_AD_98_E7_AE_A1_E7_90_86_E6_B5_81_E7_A8_8B" style="zoom:50%;" />



### Page Cache 与文件持久化的一致性&可靠性

#### Page Cache 的优势

**1.加快数据访问**

**2.减少 I/O 次数，提高系统磁盘 I/O 吞吐量**

#### Page Cache 的劣势

1. 最直接的缺点是需要占用额外物理内存空间，物理内存在比较紧俏的时候可能会导致频繁的 swap 操作，最终导致系统的磁盘 I/O 负载的上升。
2. 另一个缺陷是对应用层并没有提供很好的管理 API，几乎是透明管理。应用层即使想优化 Page Cache 的使用策略也很难进行。因此一些应用选择在用户空间实现自己的 page 管理，而不使用 page cache，例如 MySQL InnoDB 存储引擎以 16KB 的页进行管理。
3. 最后一个缺陷是在某些应用场景下比 Direct I/O 多一次磁盘读 I/O 以及磁盘写 I/O。



#### 文件持久化的一致性&可靠性

任何系统引入缓存，就会引发一致性问题：内存中的数据与磁盘中的数据不一致

Linux 提供多种机制来保证数据一致性，但无论是单机上的内存与磁盘一致性，还是分布式组件中节点 1 与节点 2 、节点 3 的数据一致性问题，理解的关键是 trade-off：**吞吐量与数据一致性保证是一对矛盾。**

**文件 = 数据 + 元数据**。。因此，我们说保证文件一致性其实包含了两个方面：数据一致+元数据一致。



当前 Linux 下以两种方式实现文件一致性：

1. **Write Through（写穿）**：向用户层提供特定接口，应用程序可主动调用接口来保证文件一致性；
2. **Write back（写回）**：系统中存在定期任务（表现形式为内核线程），周期性地同步文件系统中文件脏数据块，这是**默认**的 Linux 一致性方案；

上述两种方式最终都依赖于系统调用，主要分为如下三种系统调用：

| 方法              | 含义                                                         |
| :---------------- | :----------------------------------------------------------- |
| fsync(intfd)      | fsync(fd)：将 fd 代表的文件的脏数据和脏元数据全部刷新至磁盘中。 |
| fdatasync(int fd) | fdatasync(fd)：将 fd 代表的文件的脏数据刷新至磁盘，同时对必要的元数据刷新至磁盘中，这里所说的必要的概念是指：对接下来访问文件有关键作用的信息，如文件大小，而文件修改时间等不属于必要信息 |
| sync()            | sync()：则是对系统中所有的脏的文件数据元数据刷新至磁盘中     |

上述三种系统调用可以分别由用户进程与内核进程发起。下面我们研究一下内核线程的相关特性。

1. 创建的针对回写任务的内核线程数由系统中持久存储设备决定，为每个存储设备创建单独的刷新线程；
2. 关于多线程的架构问题，Linux 内核采取了 Lighthttp 的做法，即系统中存在一个管理线程和多个刷新线程（每个持久存储设备对应一个刷新线程）。管理线程监控设备上的脏页面情况，若设备一段时间内没有产生脏页面，就销毁设备上的刷新线程；若监测到设备上有脏页面需要回写且尚未为该设备创建刷新线程，那么创建刷新线程处理脏页面回写。而刷新线程的任务较为单调，只负责将设备中的脏页面回写至持久存储设备中。
3. 刷新线程刷新设备上脏页面大致设计如下：
   - 每个设备保存脏文件链表，保存的是该设备上存储的脏文件的 inode 节点。所谓的回写文件脏页面即回写该 inode 链表上的某些文件的脏页面；
   - 系统中存在多个回写时机，第一是应用程序主动调用回写接口（fsync，fdatasync 以及 sync 等），第二管理线程周期性地唤醒设备上的回写线程进行回写，第三是某些应用程序/内核任务发现内存不足时要回收部分缓存页面而事先进行脏页面回写，设计一个统一的框架来管理这些回写任务非常有必要。

Write Through 与 Write back 在持久化的可靠性上有所不同：

- Write Through 以牺牲系统 I/O 吞吐量作为代价，向上层应用确保一旦写入，数据就已经落盘，不会丢失；
- Write back 在系统发生宕机的情况下无法确保数据已经落盘，因此存在数据丢失的问题。不过，在程序挂了，例如被 kill -9，Page Cache 中的数据操作系统还是会确保落盘



## 进程管理

### 进程

#### 进程的状态

**在一个进程的活动期间至少具备三种基本状态，即运行状态、就绪状态、阻塞状态。**

各个状态的意义：

- 运行状态（*Running*）：该时刻进程占用 CPU；
- 就绪状态（*Ready*）：可运行，由于其他进程处于运行状态而暂时停止运行；
- 阻塞状态（*Blocked*）：该进程正在等待某一事件发生（如等待输入/输出操作的完成）而暂时停止运行，这时，即使给它CPU控制权，它也无法运行；

当然，进程还有另外两个基本状态：

- 创建状态（*new*）：进程正在被创建时的状态；

- 结束状态（*Exit*）：进程正在从系统中消失时的状态；

  

**挂起状态：描述进程没有占用实际的物理内存空间的情况**。

- 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现；
- 就绪挂起状态：进程在外存（硬盘），但只要进入内存，即刻立刻运行；

导致进程挂起的原因不只是因为进程所使用的内存空间不在物理内存，还包括如下情况：

- 通过 sleep 让进程间歇性挂起，其工作原理是设置一个定时器，到期后唤醒进程。
- 用户希望挂起一个程序的执行，比如在 Linux 中用 `Ctrl+Z` 挂起进程

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272156090.jpeg" alt="img" style="zoom:67%;" />



#### 进程的控制结构

在操作系统中，是用**进程控制块**（*process control block，PCB*）数据结构来描述进程的。

**PCB 是进程存在的唯一标识**

> PCB 具体包含什么信息呢？

**进程描述信息：**

- 进程标识符：标识各个进程，每个进程都有一个并且唯一的标识符；
- 用户标识符：进程归属的用户，用户标识符主要为共享和保护服务；

**进程控制和管理信息：**

- 进程当前状态，如 new、ready、running、waiting 或 blocked 等；
- 进程优先级：进程抢占 CPU 时的优先级；

**资源分配清单：**

- 有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I/O 设备信息。

**CPU 相关信息：**

- CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程重新执行时，能从断点处继续执行。

> 每个 PCB 是如何组织的呢？

通常是通过**链表**的方式进行组织，把具有**相同状态的进程链在一起，组成各种队列**。比如：

- 将所有处于就绪状态的进程链在一起，称为**就绪队列**；
- 把所有因等待某事件而处于等待状态的进程链在一起就组成各种**阻塞队列**；
- 另外，对于运行队列在单核 CPU 系统中则只有一个运行指针了，因为单核 CPU 在某个时间，只能运行一个程序。

那么，就绪队列和阻塞队列链表的组织形式如下图：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272202132.png" alt="12-PCB_E7_8A_B6_E6_80_81_E9_93_BE_E8_A1_A8_E7_BB_84_E7_BB_87" style="zoom:50%;" />

除了链接的组织方式，还有索引方式，它的工作原理：将同一状态的进程组织在一个索引表中，索引表项指向相应的 PCB，不同状态对应不同的索引表。

一般会选择链表，因为可能面临进程创建，销毁等调度导致进程状态发生变化，所以链表能够更加灵活的插入和删除

#### 进程的控制

我们熟知了进程的状态变迁和进程的数据结构 PCB 后，再来看看进程的**创建、终止、阻塞、唤醒**的过程，这些过程也就是进程的控制。

**01 创建进程**

操作系统允许一个进程创建另一个进程，而且允许子进程继承父进程所拥有的资源。

创建进程的过程如下：

- 申请一个空白的 PCB，并向 PCB 中填写一些控制和管理进程的信息，比如进程的唯一标识等；
- 为该进程分配运行时所必需的资源，比如内存资源；
- 将 PCB 插入到就绪队列，等待被调度运行；

**02 终止进程**

进程可以有 3 种终止方式：正常结束、异常结束以及外界干预（信号 `kill` 掉）。

当子进程被终止时，其在父进程处继承的资源应当还给父进程。而当父进程被终止时，该父进程的子进程就变为孤儿进程，会被 1 号进程收养，并由 1 号进程对它们完成状态收集工作。

终止进程的过程如下：

- 查找需要终止的进程的 PCB；
- 如果处于执行状态，则立即终止该进程的执行，然后将 CPU 资源分配给其他进程；
- 如果其还有子进程，则应将该进程的子进程交给 1 号进程接管；
- 将该进程所拥有的全部资源都归还给操作系统；
- 将其从 PCB 所在队列中删除；

**03 阻塞进程**

当进程需要等待某一事件完成时，它可以调用阻塞语句把自己阻塞等待。而一旦被阻塞等待，它只能由另一个进程唤醒。

阻塞进程的过程如下：

- 找到将要被阻塞进程标识号对应的 PCB；
- 如果该进程为运行状态，则保护其现场，将其状态转为阻塞状态，停止运行；
- 将该 PCB 插入到阻塞队列中去；

**04 唤醒进程**

进程由「运行」转变为「阻塞」状态是由于进程必须等待某一事件的完成，所以处于阻塞状态的进程是绝对不可能叫醒自己的。

如果某进程正在等待 I/O 事件，需由别的进程发消息给它，则只有当该进程所期待的事件出现时，才由发现者进程用唤醒语句叫醒它。

唤醒进程的过程如下：

- 在该事件的阻塞队列中找到相应进程的 PCB；
- 将其从阻塞队列中移出，并置其状态为就绪状态；
- 把该 PCB 插入到就绪队列中，等待调度程序调度；

进程的阻塞和唤醒是一对功能相反的语句，如果某个进程调用了阻塞语句，则必有一个与之对应的唤醒语句



#### 进程的上下文切换

CPU 上下文切换就是先把前一个任务的 **CPU 上下文**（**CPU 寄存器和程序计数器**）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。

 CPU 上下文切换分成：**进程上下文切换、线程上下文切换和中断上下文切换**。

进程是由内核管理和调度的，所以进程的切换只能发生在内核态。

所以，**进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。**

通常，会把交换的信息保存在进程的 PCB，当要运行另外一个进程的时候，我们需要从这个进程的 PCB 取出上下文，然后恢复到 CPU 中，这使得这个进程可以继续执行，如下图所示：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272229522.png" alt="13-_E8_BF_9B_E7_A8_8B_E4_B8_8A_E4_B8_8B_E6_96_87_E5_88_87_E6_8D_A2" style="zoom:50%;" />

> 发生进程上下文切换有哪些场景？

- 为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，进程就从运行状态变为就绪状态，系统从就绪队列选择另外一个进程运行；
- 进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行；
- 当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度；
- 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行；
- 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序；



#### 僵尸和孤儿进程

**僵尸进程：** 子进程退出后，未被父进程及时回收，保留在进程表中，可能导致资源浪费（特别是进程号资源）。

**孤儿进程：** 父进程先于子进程退出，子进程被 init 进程收养，通常不会带来问题。



#### 进程间通信

##### 管道

**所谓的管道，就是内核里面的一串缓存**。从管道的一段写入的数据，实际上是缓存在内核中的，另一端读取，也就是从内核中读取这段数据。另外，管道传输的数据是无格式的流且大小受限。



###### 匿名管道

**对于匿名管道，它的通信范围是存在父子关系的进程**

```bash
$ ps auxf | grep mysql
```

上面命令行里的「`|`」竖线就是一个**管道**，它的功能是将前一个命令（`ps auxf`）的输出，作为后一个命令（`grep mysql`）的输入

管道传输数据是单向的，上面这种管道是没有名字，所以「`|`」表示的管道称为**匿名管道**，用完了就销毁。

###### **命名管道**

也被叫做 `FIFO`，因为数据是先进先出的传输方式。

**对于命名管道，它可以在不相关的进程间也能相互通信**

在使用命名管道前，先需要通过 `mkfifo` 命令来创建，并且指定管道名字：

```bash
$ mkfifo myPipe
```

myPipe 就是这个管道的名称，基于 Linux 一切皆文件的理念，所以管道也是以文件的方式存在

接下来，我们往 myPipe 这个管道写入数据：

```bash
$ echo "hello" > myPipe  // 将数据写进管道
                         // 停住了 ...
```

操作了后，你会发现命令执行后就停在这了，这是因为管道里的内容没有被读取，只有当管道里的数据被读完后，命令才可以正常退出。

于是，我们执行另外一个命令来读取这个管道里的数据：

```bash
$ cat < myPipe  // 读取管道里的数据
hello
```

可以看到，管道里的内容被读取出来了，并打印在了终端上，另外一方面，echo 那个命令也正常退出了。

###### 优缺点

- **管道这种通信方式效率低，不适合进程间频繁地交换数据**。
- 好处，自然就是简单，同时也我们很容易得知管道里的数据已经被另一个进程读取了。



###### 原理

> 那管道如何创建呢，背后原理是什么？

匿名管道的创建，需要通过下面这个系统调用：

```c
int pipe(int fd[2])
```

这里表示创建一个匿名管道，并返回了两个描述符，一个是管道的读取端描述符 `fd[0]`，另一个是管道的写入端描述符 `fd[1]`。注意，这个**匿名管道是特殊的文件，只存在于内存，不存于文件系统中。**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408282031193.png" alt="5-_E7_AE_A1_E9_81_93-pipe" style="zoom:50%;" />

使用 `fork` 创建子进程，**创建的子进程会复制父进程的文件描述符**，这样就做到了两个进程各有两个「 `fd[0]` 与 `fd[1]`」，两个进程就可以通过各自的 fd 写入和读取同一个管道文件实现跨进程通信了。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408282153315.png" alt="6-_E7_AE_A1_E9_81_93-pipe-fork" style="zoom:50%;" />

管道只能一端写入，另一端读出，所以上面这种模式容易造成混乱，因为父进程和子进程都可以同时写入，也都可以读出。那么，为了避免这种情况，通常的做法是：

- 父进程关闭读取的 fd[0]，只保留写入的 fd[1]；
- 子进程关闭写入的 fd[1]，只保留读取的 fd[0]；

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408282153773.png" alt="7-_E7_AE_A1_E9_81_93-pipe-fork-_E5_8D_95_E5_90_91_E9_80_9A_E4_BF_A1" style="zoom:50%;" />

在 shell 里面执行 `A | B`命令的时候，A 进程和 B 进程都是 shell 创建出来的子进程，A 和 B 之间不存在父子关系，它俩的父进程都是 shell。

![8-_E7_AE_A1_E9_81_93-pipe-shell](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408282153389.png)





##### 消息队列

**消息队列是保存在内核中的消息链表**

在发送数据时，会分成一个一个独立的数据单元，也就是**消息体**（数据块），消息体是用户自定义的数据类型，消息的发送方和接收方要**约定**好消息体的数据类型，所以每个消息体都是**固定大小**的存储块，不像管道是无格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。

消息队列**生命周期随内核**，如果没有释放消息队列或者没有关闭操作系统，消息队列会一直存在，而前面提到的匿名管道的生命周期，是随进程的创建而建立，随进程的结束而销毁。

- **消息队列不适合比较大数据的传输**，因为在内核中每个消息体都有一个最大长度的限制，同时所有队列所包含的全部消息体的总长度也是有上限。在 Linux 内核中，会有两个宏定义 `MSGMAX` 和 `MSGMNB`，它们以字节为单位，分别定义了一条消息的最大长度和一个队列的最大长度。
- **消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销**，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程



##### 共享内存

**共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中**。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去，大大提高了进程间通信的速度。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408282159145.png" alt="9-_E5_85_B1_E4_BA_AB_E5_86_85_E5_AD_98" style="zoom:50%;" />



##### 信号量

信号量其实是一个**整型的计数器**，主要用于实现**进程**间的**互斥与同步**，而不是用于缓存进程间通信的数据。

**信号量表示资源的数量**，控制信号量的方式有两种原子操作：

- 一个是 **P 操作**，这个操作会把信号量减去 1，**相减后如果信号量 < 0，则表明资源已被占用**，进程需阻塞等待；相减后如果信号量 >= 0，则表明还有资源可使用，进程可正常继续执行。
- 另一个是 **V 操作**，这个操作会把信号量加上 1，**相加后如果信号量 <= 0，则表明当前有阻塞中的进程**，于是会将该进程唤醒运行；相加后如果信号量 > 0，则表明当前没有阻塞中的进程；

P 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。

- 信号初始化为 `1`，就代表着是**互斥信号量**
- 信号初始化为 `0`，就代表着是**同步信号量**，它可以保证进程 A 应在进程 B 之前执行。



##### 信号

对于**异常情况**下的工作模式，就需要用「信号」的方式来通知进程。

来源：

- 硬件来源（如键盘 Cltr+C ）
  - 运行在 shell 终端的进程，我们可以通过**键盘**输入某些组合键的时候，给进程发送信号。例如
    - Ctrl+C 产生 `SIGINT` 信号，表示终止该进程；
    - Ctrl+Z 产生 `SIGTSTP` 信号，表示停止该进程，但还未结束；

- 软件来源（如 kill 命令）
  - 进程在后台运行，可以通过 `kill` 命令的方式给进程发送信号，但前提需要知道运行中的进程 PID 号，例如：
    - kill -9 1050 ，表示给 PID 为 1050 的进程发送 `SIGKILL` 信号，用来立即结束该进程；

信号是进程间通信机制中**唯一的异步通信机制**，因为可以在任何时候发送信号给某一进程，一旦有信号产生，我们就有下面这几种，用户进程对信号的处理方式。

**1.执行默认操作**。Linux 对每种信号都规定了默认操作，例如，上面列表中的 SIGTERM 信号，就是终止进程的意思。

**2.捕捉信号**。我们可以为信号定义一个信号处理函数。当信号发生时，我们就执行相应的信号处理函数。

**3.忽略信号**。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 `SIGKILL` 和 `SEGSTOP`，它们用于在任何时候中断或结束某一进程



在 Linux 操作系统中， 为了响应各种各样的事件，提供了几十种信号，分别代表不同的意义。我们可以通过 `kill -l` 命令，查看所有的信号



##### Socket

**跨网络与不同主机上的进程**之间通信，就需要 Socket 通信了。还可以在**同主机上进程**间通信。

创建 socket 的系统调用：

```c
int socket(int domain, int type, int protocal)
```

三个参数分别代表：

- domain 参数用来指定协议族，比如 AF_INET 用于 IPV4、AF_INET6 用于 IPV6、AF_LOCAL/AF_UNIX 用于本机；
- type 参数用来指定通信特性，比如 SOCK_STREAM 表示的是字节流，对应 TCP、SOCK_DGRAM 表示的是数据报，对应 UDP、SOCK_RAW 表示的是原始套接字；
- protocal 参数原本是用来指定通信协议的，但现在基本废弃。因为协议已经通过前面两个参数指定完成，protocol 目前一般写成 0 即可；

根据创建 socket 类型的不同，通信的方式也就不同：

- 实现 TCP 字节流通信： socket 类型是 AF_INET 和 SOCK_STREAM；

- 实现 UDP 数据报通信：socket 类型是 AF_INET 和 SOCK_DGRAM；

- 实现本地进程间通信： 

  - 对于本地字节流 socket，其 socket 类型是 AF_LOCAL 和 SOCK_STREAM。

    对于本地数据报 socket，其 socket 类型是 AF_LOCAL 和 SOCK_DGRAM。

    本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端口，而是**绑定一个本地文件**，这也就是它们之间的最大区别





### 线程

#### 定义

**线程是进程当中的一条执行流程。**

同一个进程内多个线程之间可以共享代码段、数据段、打开的文件等资源，但每个线程各自都有一套独立的寄存器和栈，这样可以确保线程的控制流是相对独立的。

线程的优点：

- 一个进程中可以同时存在多个线程；
- 各个线程之间可以并发执行；
- 各个线程之间可以共享地址空间和文件等资源；

线程的缺点：

- 当进程中的一个线程崩溃时，会导致其所属进程的所有线程崩溃（这里是针对 C/C++ 语言，Java语言中的线程奔溃不会造成进程崩溃

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272232417.png" alt="16-_E5_A4_9A_E7_BA_BF_E7_A8_8B_E5_86_85_E5_AD_98_E7_BB_93_E6_9E_84" style="zoom:50%;" />

#### 线程与进程的比较

线程与进程的比较如下：

- 进程是资源（包括内存、打开的文件等）分配的单位，线程是 CPU 调度的单位；
- 进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈；
- 线程同样具有就绪、阻塞、执行三种基本状态，同样具有状态之间的转换关系；
- 线程能减少并发执行的时间和空间开销；

对于，线程相比进程能减少开销，体现在：

- 线程的创建时间比进程快，因为进程在创建的过程中，还需要资源管理信息，比如内存管理信息、文件管理信息，而线程在创建的过程中，不会涉及这些资源管理信息，而是共享它们；
- 线程的终止时间比进程快，因为线程释放的资源相比进程少很多；
- 同一个进程内的线程切换比进程切换快，因为线程具有相同的地址空间（虚拟内存共享），这意味着同一个进程的线程都具有同一个页表，那么在切换的时候不需要切换页表。而对于进程之间的切换，切换的时候要把页表给切换掉，而页表的切换过程开销是比较大的；
- 由于同一进程的各线程间共享内存和文件资源，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更高了；

所以，不管是时间效率，还是空间效率线程比进程都要高。



####  线程的上下文切换

所谓操作系统的任务调度，实际上的调度对象是线程

- 当两个线程不是属于同一个进程，则切换的过程就跟进程上下文切换一样；
- **当两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据**；



#### 线程的实现

主要有三种线程的实现方式：

- **用户线程（\*User Thread\*）**：在用户空间实现的线程，不是由内核管理的线程，是由用户态的线程库来完成线程的管理；
- **内核线程（\*Kernel Thread\*）**：在内核中实现的线程，是由内核管理的线程；
- **轻量级进程（\*LightWeight Process\*）**：在内核中来支持用户线程；

用户线程和内核线程的对应关系。

- **多对一**
- **一对一**
- **多对多**



> 用户线程如何理解？存在什么优势和缺陷？

用户线程是基于用户态的线程管理库来实现的，那么**线程控制块（\*Thread Control Block, TCB\*）** 也是在库里面来实现的，对于操作系统而言是看不到这个 TCB 的，它只能看到整个进程的 PCB。

所以，**用户线程的整个线程管理和调度，操作系统是不直接参与的，而是由用户级线程库函数来完成线程的管理，包括线程的创建、终止、同步和调度等。**

用户级线程的模型，也就类似前面提到的**多对一**的关系，即多个用户线程对应同一个内核线程，如下图所示：

用户线程的**优点**：

- 每个进程都需要有它私有的线程控制块（TCB）列表，用来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），TCB 由用户级线程库函数来维护，可用于不支持线程技术的操作系统；
- 用户线程的切换也是由线程库函数来完成的，无需用户态与内核态的切换，所以速度特别快；

用户线程的**缺点**：

- 由于操作系统不参与线程的调度，如果一个线程发起了系统调用而阻塞，那进程所包含的用户线程都不能执行了。
- 当一个线程开始运行后，除非它主动地交出 CPU 的使用权，否则它所在的进程当中的其他线程无法运行，因为用户态的线程没法打断当前运行中的线程，它没有这个特权，只有操作系统才有，但是用户线程不是由操作系统管理的。
- 由于时间片分配给进程，故与其他进程比，在多线程执行时，每个线程得到的时间片较少，执行会比较慢；

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272254768.jpeg" alt="img" style="zoom:33%;" />



> 那内核线程如何理解？存在什么优势和缺陷？

**内核线程是由操作系统管理的，线程对应的 TCB 自然是放在操作系统里的，这样线程的创建、终止和管理都是由操作系统负责。**

内核线程的模型，也就类似前面提到的**一对一**的关系，即一个用户线程对应一个内核线程，如下图所示：

内核线程的**优点**：

- 在一个进程当中，如果某个内核线程发起系统调用而被阻塞，并不会影响其他内核线程的运行；
- 分配给线程，多线程的进程获得更多的 CPU 运行时间；

内核线程的**缺点**：

- 在支持内核线程的操作系统中，由内核来维护进程和线程的上下文信息，如 PCB 和 TCB；
- 线程的创建、终止和切换都是通过系统调用的方式来进行，因此对于系统来说，系统开销比较大；

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272254478.png" alt="21-_E7_BA_BF_E7_A8_8BPCB-_E4_B8_80_E5_AF_B9_E4_B8_80_E5_85_B3_E7_B3_BB" style="zoom: 50%;" />

> 最后的轻量级进程如何理解？

**轻量级进程（\*Light-weight process，LWP\*）是内核支持的用户线程，一个进程可有一个或多个 LWP，每个 LWP 是跟内核线程一对一映射的，也就是 LWP 都是由一个内核线程支持，而且 LWP 是由内核管理并像普通进程一样被调度**。

在大多数系统中，**LWP与普通进程的区别也在于它只有一个最小的执行上下文和调度程序所需的统计信息**。一般来说，一个进程代表程序的一个实例，而 LWP 代表程序的执行线程，因为一个执行线程不像进程那样需要那么多状态信息，所以 LWP 也不带有这样的信息。

在 LWP 之上也是可以使用用户线程的，那么 LWP 与用户线程的对应关系就有三种：

- `1 : 1`，即一个 LWP 对应 一个用户线程；
- `N : 1`，即一个 LWP 对应多个用户线程；
- `M : N`，即多个 LWP 对应多个用户线程；

接下来针对上面这三种对应关系说明它们优缺点。先看下图的 LWP 模型：

**1 : 1 模式**

一个线程对应到一个 LWP 再对应到一个内核线程，如上图的进程 4，属于此模型。

- 优点：实现并行，当一个 LWP 阻塞，不会影响其他 LWP；
- 缺点：每一个用户线程，就产生一个内核线程，创建线程的开销较大。

**N : 1 模式**

多个用户线程对应一个 LWP 再对应一个内核线程，如上图的进程 2，线程管理是在用户空间完成的，此模式中用户的线程对操作系统不可见。

- 优点：用户线程要开几个都没问题，且上下文切换发生用户空间，切换的效率较高；
- 缺点：一个用户线程如果阻塞了，则整个进程都将会阻塞，另外在多核 CPU 中，是没办法充分利用 CPU 的。

**M : N 模式**

根据前面的两个模型混搭一起，就形成 `M:N` 模型，该模型提供了两级控制，首先多个用户线程对应到多个 LWP，LWP 再一一对应到内核线程，如上图的进程 3。

- 优点：综合了前两种优点，大部分的线程上下文发生在用户空间，且多个线程又可以充分利用多核 CPU 的资源。

**组合模式**

如上图的进程 5，此进程结合 `1:1` 模型和 `M:N` 模型。开发人员可以针对不同的应用特点调节内核线程的数目来达到物理并行性和逻辑并行性的最佳方案。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272256542.png" alt="22-LWP" style="zoom: 33%;" />



**Go 语言中的协程（Goroutine）并不完全等同于传统意义上的轻量级进程（Lightweight Process，LWP）**，但它们有一些相似之处。下面是对 Go 协程（Goroutine）及其与轻量级进程关系的详细说明：

Go 协程（Goroutine）

- **定义**：Go 协程是 Go 语言中一种非常轻量级的并发执行单元。与线程相比，Go 协程的创建和销毁成本非常低，因此在 Go 程序中，可以轻松创建成千上万的协程。
  
- **调度**：Go 协程的调度由 Go 运行时（runtime）管理，采用了一种称为 M:N 调度模型。在这种模型中，M 个 Go 协程可以映射到 N 个操作系统线程上运行。Go 运行时会智能地管理这些映射，并在操作系统线程之间调度协程的执行。

- **栈空间**：Go 协程使用的是动态增长的栈，初始大小非常小（通常为几 KB），当需要更多栈空间时，Go 运行时会自动扩展它。这与操作系统线程（通常使用固定大小的栈）不同，使得 Go 协程更加节省资源。

Go 协程 vs. 轻量级进程

1. **调度**：
   - **轻量级进程（LWP）**：由操作系统内核调度，通常与操作系统线程同义。它们在内核层面由操作系统管理。
   - **Go 协程**：由 Go 运行时调度，不依赖操作系统的线程调度机制。Go 协程的调度是用户态的，运行时决定了协程的切换和管理。

2. **资源使用**：
   - **轻量级进程**：与传统线程类似，通常有较高的创建、销毁和上下文切换开销。
   - **Go 协程**：非常轻量，栈空间初始很小，创建和销毁成本低，可以在一个进程中运行成千上万个 Go 协程。

3. **隔离性**：
   - **轻量级进程**：线程之间共享同一进程的地址空间，因此在资源使用上并不完全隔离，容易出现竞争条件和同步问题。
   - **Go 协程**：虽然也是共享进程的内存空间，但由于调度在用户态进行，并且 Go 语言提供了更高层次的并发模型（如 `channel`），协程之间的通信和同步更加安全和高效。

4. **并发模型**：
   - **轻量级进程**：通常采用传统的锁和信号量机制进行同步，容易出现死锁、资源争夺等问题。
   - **Go 协程**：Go 提供了基于 `channel` 的通信方式，避免了大部分显式锁的使用，简化了并发编程的难度。



#### 多线程冲突



##### 锁

1. 忙等待锁   

   - 实现：自旋锁

   无忙等待锁

2. 共享锁（Shared Lock）和独占锁（Exclusive Lock）

3. **乐观锁（Optimistic Lock）**

   - **先修改完共享资源**，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作。通过**版本号验证**
   - 乐观锁全程并没有加锁**，**所以它也叫**无锁编程**。
   - 冲突概率非常低**，**且加锁成本非常高的场景时，才考虑使用乐观锁

   悲观锁（Pessimistic Lock）访问共享资源前，**先要上锁**。

4. 互斥锁

5. 读写锁

   - **读写锁适用于能明确区分读操作和写操作的场景**。读多写少
   - 分为「读优先锁」、「写优先锁」、公平读写锁
   - 公平读写锁比较简单的一种方式是：维护一个队列（通常是FIFO队列）来保存等待获取锁的线程。无论是读线程还是写线程，当它们请求锁时，都会被加入到这个队列中。避免了饥饿现象。

6. 分布式锁

7. 意向锁（Intent Lock）

**最基本的锁**：自旋锁与互斥锁

- **互斥锁**加锁失败后，线程会**释放 CPU** ，给其他线程；

  - **对于互斥锁加锁失败而阻塞的现象，是由操作系统内核实现的**。当加锁失败时，内核会将线程置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后，于是就可以继续执行。

  - 会有**两次线程上下文切换的成本**

  - 上下切换的耗时有大佬统计过，大概在几十纳秒到几微秒之间，如果你锁住的代码执行时间比较短，那可能上下文切换的时间都比你锁住的代码执行时间还要长。

    所以，**如果你能确定被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁，否则使用互斥锁。**

- **自旋锁**加锁失败后，线程会**忙等待**，直到它拿到锁；

  - 通过 CPU 提供的 `CAS` 函数（*Compare And Swap*），在「用户态」完成加锁和解锁操作，不会主动产生线程上下文切换，所以相比互斥锁来说，会快一些，开销也小一些。
  - 自旋锁开销少，在多核系统下一般不会主动产生线程切换，适合异步、协程等在用户态切换请求的编程方式，但如果被锁住的代码执行时间过长，自旋的线程会长时间占用 CPU 资源，



###### 忙等待锁和无忙等待锁

忙等待锁

现代 CPU 体系结构提供的特殊**原子操作指令 —— 测试和置位（\*Test-and-Set\*）指令**。

如果用 C 代码表示 Test-and-Set 指令，形式如下：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408290959723.png" alt="13-TestAndSet" style="zoom: 25%;" />

以运用 Test-and-Set 指令来实现「忙等待锁」，代码如下：

很明显，当获取不到锁时，线程就会一直 while 循环，不做任何事情，所以就被称为「忙等待锁」，也被称为**自旋锁（\*spin lock\*）**。

这是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。在单处理器上，需要**抢占式的调度器**（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408290959749.png" alt="14-_E8_87_AA_E6_97_8B_E9_94_81" style="zoom:33%;" />

无忙等待锁

当没获取到锁的时候，就把当前线程放入到锁的**等待队列**，然后执行调度程序，把 CPU 让给其他线程执行。





###### 锁和信号量

都是用于控制多个进程或线程对共享资源的访问的同步机制。它们有一些相似之处，但也有一些关键的区别。

锁（Lock）：

1. **互斥性：** 锁是一种互斥机制，一次只能有一个进程或线程持有锁。当一个进程或线程持有锁时，其他试图获取锁的进程或线程会被阻塞。
2. **二进制信号量：** 锁可以看作是二进制信号量的一种特例，即只有两个状态，锁定和未锁定。在常见的情况下，锁被用于实现互斥。
3. **可重入性：** 有些锁支持可重入性，即同一线程可以多次获取同一个锁而不会发生死锁。这对于递归调用中需要多次获取锁的情况很有用。

信号量（Semaphore）：

1. **计数器：** 信号量是一个带有计数器的同步机制，计数器的值可以大于1。计数器表示同一时刻可以有多少个进程或线程访问共享资源。
2. **多值：** 信号量的计数器允许表示多值，而不仅仅是锁定或未锁定的两个状态。这使得信号量可以用于控制多个资源的访问。
3. **阻塞和唤醒：** 信号量通常包含两个操作：`P`（等待）和`V`（释放）。`P`操作尝试获取资源，如果计数器为正，成功获取；否则，会被阻塞。`V`操作释放资源，使计数器增加，唤醒一个等待的进程。
4. **用途广泛：** 信号量不仅可以用于互斥，还可以用于控制并发访问资源的数量。

在实际应用中，选择使用锁还是信号量取决于需要的同步语义和对资源访问的控制方式。锁通常用于互斥访问共享资源的场景，而信号量更适用于控制资源的数量、实现生产者-消费者问题等场景。



###### 死锁

死锁只有**同时满足**以下四个条件才会发生：

- 互斥条件；
- 持有并等待条件；
- 不可剥夺条件；
- 环路等待条件



###### **避免死锁**：

预防策略

预防策略是指在设计和实现系统时，采取一些措施，以避免死锁的发生。具体来说，可以采用以下方法：

- 破坏互斥条件：将互斥访问的资源改为**共享访问**；
- 破坏占有和等待条件：进程在申请资源时，要求先**释放已经占有的资源；**
- 破坏不可剥夺条件：对于一些必须占用的资源，**允许进行资源抢占**；
- 破坏环路等待条件：通过**资源有序分配法**等方法，避免进程之间因互相等待所形成的环路。

避免策略

避免策略是指在系统运行时，通过动态地避免有可能导致死锁的操作，以避免死锁的发生。具体来说，可以采用以下方法：

- **银行家算法**：通过预测进程在未来的资源需求，避免出现死锁；
- **超时机制**：如果进程无法在一定时间内获得所需资源，则放弃已有的资源，避免死锁的发生；
- **死锁检测与恢复**：通过周期性地检测系统状态，发现死锁后采取恢复措施。

需要注意的是，预防策略可以预防大部分死锁情况，但也会带来一定的系统性能损失；而避免策略虽然可以减少性能损失，但也不能避免所有死锁情况的发生。

其他

- **自旋锁与无锁算法**：在某些情况下，可以使用自旋锁或无锁算法来替代传统的锁机制。自旋锁通过忙等待的方式避免了复杂的锁竞争，而无锁算法则通过原子操作和 CAS（Compare-And-Swap）等技术来避免锁的使用，从而减少死锁的发生。

- 大粒度锁（Coarse-Grained Locking）：有时可以通过使用大粒度的锁来代替多个细粒度的锁，从而简化锁的管理。然而，这可能会降低系统的并发性，需要权衡。

- 避免嵌套锁定：尽量避免一个线程在持有一个锁的同时试图获取另一个锁，因为这会增加发生死锁的可能性。可以通过重新设计代码逻辑或将关键代码块放在同一个锁内来避免这种情况。





##### 互斥

**互斥**是指防止多个线程或进程同时访问同一个共享资源，确保每次只有一个线程可以访问该资源。互斥的主要目的是防止多个线程在修改共享资源时导致的数据竞争和不一致问题。

由于多线程执行操作共享变量的这段代码可能会导致竞争状态，因此我们将此段代码称为**临界区（\*critical section\*），它是访问共享资源的代码片段，一定不能给多线程同时执行。**

###### 锁



###### 信号量

##### 同步

**同步**是指协调多个线程或进程的执行顺序，以确保它们在访问共享资源时按照预期的顺序执行，避免数据的不一致和竞态条件。同步可以通过各种机制来实现，例如条件变量、信号量和事件

###### 信号量

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291007504.png" alt="19-_E4_BA_92_E6_96_A5_E4_BF_A1_E5_8F_B7_E9_87_8F_E5_90_8C_E6_AD_A5_E5_AE_9E_E7_8E_B0-_E5_90_83_E9_A5_AD_E4_BE_8B_E5_AD_90" style="zoom: 25%;" />







###### 哲学家就餐问题

一张圆桌上坐着5名哲学家，每两个哲学家之间的桌上摆一根筷子，桌子的中间是一碗米饭。哲学家们倾注毕生的精力用于思考和进餐，哲学家在思考时，并不影响他人。只有当哲学家饥饿时，才试图拿起左、右两根筷子(一根一根地拿起)。如果筷子已在他人手上，则需等待。饥饿的哲学家只有同时拿起两根筷子才可以开始进餐，当进餐完毕后，放下筷子继续思考。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291016490.png" alt="23-_E5_93_B2_E5_AD_A6_E5_AE_B6_E8_BF_9B_E9_A4_90_E6_A8_A1_E5_9E_8B" style="zoom:33%;" />

1. 可以对哲学家进程施加一些限制条件，比如最多允许四个哲学家同时进餐。这样可以保证至少有一个哲学家是可以拿到左右两只筷子的
2. 要求奇数号哲学家先拿左边的筷子，然后再拿右边的筷子，而偶数号哲学家刚好相反。用这种方法可以保证如果相邻的两个奇偶号哲学家都想吃饭，那么只会有其中一个可以拿起第一只筷子，另一个会直接阻塞。这就避免了占有一支后再等待另一只的情况
3. 仅当一个哲学家左右两支筷子都可用时才允许他抓起筷子





###### 读者-写者问题

描述：

- 「读-读」允许：同一时刻，允许多个读者同时读
- 「读-写」互斥：没有写者时读者才能读，没有读者时写者才能写
- 「写-写」互斥：没有其他写者时，写者才能写

解决策略：

1. **第一种读者优先策略**：

   - 读者可以立即访问资源，即使有写者在等待。
   - 可能导致写者饥饿，因为如果持续有读者到达，写者可能永远得不到机会。

2. **第二种写者优先策略**：

   - 如果有写者在等待，新的读者必须等待，直到写者完成操作。
   - 可能导致读者饥饿。

3. **公平策略**：

   - 通过使用队列或其他调度策略，保证读者和写者都不会饿死，公平地轮流访问资源。
   - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291020170.png" alt="34-_E8_AF_BB_E8_80_85_E5_86_99_E8_80_85-_E6_96_B9_E6_A1_88_E4_B8_89_E7_A4_BA_E4_BE_8B" style="zoom: 25%;" />

   

###### 生产者-消费者问题

生产者-消费者问题描述：

- **生产者**在生成数据后，放在一个缓冲区中；
- **消费者**从缓冲区取出数据处理；
- 任何时刻，**只能有一个**生产者或消费者可以访问缓冲区；

我们对问题分析可以得出：

- 任何时刻只能有一个线程操作缓冲区，说明操作缓冲区是临界代码，**需要互斥**；
- 缓冲区空时，消费者必须等待生产者生成数据；缓冲区满时，生产者必须等待消费者取出数据。说明生产者和消费者**需要同步**。

那么我们需要三个信号量，分别是：

- 互斥信号量 `mutex`：用于互斥访问缓冲区，初始化值为 1；
- 资源信号量 `fullBuffers`：用于消费者询问缓冲区是否有数据，有数据则读取数据，初始化值为 0（表明缓冲区一开始为空）；
- 资源信号量 `emptyBuffers`：用于生产者询问缓冲区是否有空位，有空位则生成数据，初始化值为 n （缓冲区大小）；

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291010094.png" alt="21-_E7_94_9F_E4_BA_A7_E8_80_85_E6_B6_88_E8_B4_B9_E8_80_85_E4_BB_A3_E7_A0_81_E7_A4_BA_E4_BE_8B" style="zoom:25%;" />



#### 线程崩溃了，进程也会崩溃吗？

在 Go 语言中，线程的概念被抽象为更轻量级的**goroutine**。当一个 goroutine 崩溃时，不一定会导致整个进程崩溃，但这取决于具体的情况和代码的处理方式。

Goroutine 崩溃的影响

1. **Panic**：
   - 在 Go 语言中，当 goroutine 遇到严重错误（如数组越界或显式调用 `panic`）时，会触发 `panic`。
   - `panic` 会引发当前 goroutine 的崩溃，并执行该 goroutine 中的延迟函数（defer）链。如果没有被恢复（recovered），`panic` 会导致整个进程崩溃。

2. **Recover**：
   - Go 提供了 `recover` 函数，可以在 `defer` 中**捕获并处理** `panic`，从而防止 `panic` 导致整个进程崩溃。
   - 如果 `panic` 被 `recover` 捕获并处理，则 goroutine 会正常结束或继续执行其他代码，而不会影响其他 goroutine 或整个进程。

```go
package main

import (
    "fmt"
)

func safeGoroutine() {
    defer func() {
        if r := recover(); r != nil {
            fmt.Println("Recovered from panic:", r)
        }
    }()
    panic("This is a panic!")
}

func main() {
    go safeGoroutine()

    fmt.Println("Main function continues execution")
    // 添加一些等待逻辑以确保 goroutine 执行完毕
}
```

在这个示例中，即使 `safeGoroutine` 中触发了 `panic`，也不会导致整个进程崩溃，因为 `panic` 被 `recover` 捕获并处理了。`main` 函数继续执行，其它 goroutine 也不会受到影响。





###  调度

####  调度时机

在进程的生命周期中，当进程从一个运行状态到另外一状态变化的时候，其实会触发一次调度。

另外，如果硬件时钟提供某个频率的周期性中断，那么可以根据如何处理时钟中断 ，把调度算法分为两类：

- **非抢占式调度算法**挑选一个进程，然后让该进程运行直到被阻塞，或者直到该进程退出，才会调用另外一个进程，也就是说不会理时钟中断这个事情。
- **抢占式调度算法**挑选一个进程，然后让该进程只运行某段时间，如果在该时段结束时，该进程仍然在运行时，则会把它挂起，接着调度程序从就绪队列挑选另外一个进程。这种抢占式调度处理，需要在时间间隔的末端发生**时钟中断**，以便把 CPU 控制返回给调度程序进行调度，也就是常说的**时间片机制**。



#### 调度原则

*原则一*：如果运行的程序，发生了 I/O 事件的请求，那 CPU 使用率必然会很低，因为此时进程在阻塞等待硬盘的数据返回。这样的过程，势必会造成 CPU 突然的空闲。所以，**为了提高 CPU 利用率，在这种发送 I/O 事件致使 CPU 空闲的情况下，调度程序需要从就绪队列中选择一个进程来运行。**

*原则二*：有的程序执行某个任务花费的时间会比较长，如果这个程序一直占用着 CPU，会造成系统吞吐量（CPU 在单位时间内完成的进程数量）的降低。所以，**要提高系统的吞吐率，调度程序要权衡长任务和短任务进程的运行完成数量。**

*原则三*：从进程开始到结束的过程中，实际上是包含两个时间，分别是进程运行时间和进程等待时间，这两个时间总和就称为周转时间。进程的周转时间越小越好，**如果进程的等待时间很长而运行时间很短，那周转时间就很长，这不是我们所期望的，调度程序应该避免这种情况发生。**

*原则四*：处于就绪队列的进程，也不能等太久，当然希望这个等待的时间越短越好，这样可以使得进程更快的在 CPU 中执行。所以，**就绪队列中进程的等待时间也是调度程序所需要考虑的原则。**

*原则五*：对于鼠标、键盘这种交互式比较强的应用，我们当然希望它的响应时间越快越好，否则就会影响用户体验了。所以，**对于交互式比较强的应用，响应时间也是调度程序需要考虑的原则。**



针对上面的五种调度原则，总结成如下：

- **CPU 利用率**：调度程序应确保 CPU 是始终匆忙的状态，这可提高 CPU 的利用率；
- **系统吞吐量**：吞吐量表示的是单位时间内 CPU 完成进程的数量，长作业的进程会占用较长的 CPU 资源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；
- **周转时间**：周转时间是进程运行+阻塞时间+等待时间的总和，一个进程的周转时间越小越好；
- **等待时间**：这个等待时间不是阻塞状态的时间，而是进程处于就绪队列的时间，等待的时间越长，用户越不满意；
- **响应时间**：用户提交请求到系统第一次产生响应所花费的时间，在交互式系统中，响应时间是衡量调度算法好坏的主要标准。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272301415.png" alt="23-_E4_BA_94_E7_A7_8D_E8_B0_83_E5_BA_A6_E8_A7_84_E5_88_99" style="zoom:67%;" />

#### 调度算法

不同的调度算法适用的场景也是不同的。

接下来，说说在**单核 CPU 系统**中常见的调度算法。

> 01 先来先服务调度算法

最简单的一个调度算法，就是非抢占式的**先来先服务（\*First Come First Serve, FCFS\*）算法**了。

**每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。**

这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。

FCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。

> 02 最短作业优先调度算法

**最短作业优先（\*Shortest Job First, SJF\*）调度算法**同样也是顾名思义，它会**优先选择运行时间最短的进程来运行**，这有助于提高系统的吞吐量。

> 03 高响应比优先调度算法

前面的「先来先服务调度算法」和「最短作业优先调度算法」都没有很好的权衡短作业和长作业。

那么，**高响应比优先 （\*Highest Response Ratio Next, HRRN\*）调度算法**主要是权衡了短作业和长作业。

**每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行**，「响应比优先级」的计算公式：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272303516.png" alt="26-_E5_93_8D_E5_BA_94_E6_AF_94_E5_85_AC_E5_BC_8F" style="zoom:50%;" />

但一个进程要求服务的时间是不可预知的，所以，高响应比优先调度算法是「理想型」的调度算法，现实中是实现不了的。

> 04 时间片轮转调度算法

最古老、最简单、最公平且使用最广的算法就是**时间片轮转（\*Round Robin, RR\*）调度算法**。

**每个进程被分配一个时间段，称为时间片（\*Quantum\*），即允许该进程在该时间段中运行。**

- 如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配给另外一个进程；
- 如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；

另外，时间片的长度就是一个很关键的点：

- 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；
- 如果设得太长又可能引起对短作业进程的响应时间变长。将

一般来说，时间片设为 `20ms~50ms` 通常是一个比较合理的折中值。

> 05 最高优先级调度算法

前面的「时间片轮转算法」做了个假设，即让所有的进程同等重要，也不偏袒谁，大家的运行时间都一样。

但是，对于多用户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能**从就绪队列中选择最高优先级的进程进行运行，这称为最高优先级（\*Highest Priority First，HPF\*）调度算法**。

进程的优先级可以分为，静态优先级和动态优先级：

- 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；
- 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是**随着时间的推移增加等待进程的优先级**。

该算法也有两种处理优先级高的方法，非抢占式和抢占式：

- 非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。
- 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。

但是依然有缺点，可能会导致低优先级的进程永远不会运行。

> 06 多级反馈队列调度算法

**多级反馈队列（\*Multilevel Feedback Queue\*）调度算法**是「时间片轮转算法」和「最高优先级算法」的综合和发展。

顾名思义：

- 「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。
- 「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；

来看看，它是如何工作的：

- 设置了多个队列，赋予每个队列不同的优先级，每个**队列优先级从高到低**，同时**优先级越高时间片越短**；
- 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；
- 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；

可以发现，对于短作业可能可以在第一级队列很快被处理完。对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也变更长了，所以该算法很好的**兼顾了长短作业，同时有较好的响应时间。**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408272306816.png" alt="28-_E5_A4_9A_E7_BA_A7_E9_98_9F_E5_88_97" style="zoom:50%;" />











## 文件系统

### 定义

**文件 = 数据 + 元数据**。

文件系统的**基本数据单位是文件**，**基本操作单位是数据块**。，它的目的是对磁盘上的文件进行组织管理，那组织的方式不同，就会形成不同的文件系统。

Linux 最经典的一句话是：「**一切皆文件**」，不仅普通的文件和目录，就连块设备、管道、socket 等，也都是统一交给文件系统管理的。

Linux 文件系统会为每个文件分配两个**数据结构**：**索引节点（\*index node\*）和目录项（\*directory entry\*）**，它们主要用来记录文件的元信息和目录层次结构。

- 索引节点，也就是 *inode*，用来记录文件的元信息，比如 inode 编号、文件大小、访问权限、创建时间、修改时间、**数据在磁盘的位置**等等。索引节点是文件的**唯一**标识，它们之间一一对应，也同样都会被存储在硬盘中，所以**索引节点同样占用磁盘空间**。
- 目录项，也就是 *dentry*，用来记录文件的名字、**索引节点指针**以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，**目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存**。

由于索引节点唯一标识一个文件，而目录项记录着文件的名字，所以目录项和索引节点的关系是多对一，也就是说，一个文件可以有多个别名。比如，硬链接的实现就是多个目录项中的索引节点指向同一个文件。

注意，目录也是文件，也是用索引节点唯一标识，和普通文件不同的是，普通文件在磁盘里面保存的是文件数据，而目录文件在磁盘里面保存子目录或文件。

> 目录项和目录是一个东西吗？

虽然名字很相近，但是它们不是一个东西，**目录是个文件，持久化存储在磁盘，而目录项是内核一个数据结构，缓存在内存。**

如果查询目录频繁从磁盘读，效率会很低，所以内核会把已经读过的目录用目录项这个数据结构缓存在内存，下次再次读到相同的目录时，只需从内存读就可以，大大提高了文件系统的效率。

注意，目录项这个数据结构不只是表示目录，也是可以表示文件的。

> 那文件数据是如何存储在磁盘的呢？

磁盘读写的最小单位是**扇区**，扇区的大小只有 `512B` 大小，很明显，如果每次读写都以这么小为单位，那这读写的效率会非常低。

所以，文件系统把多个扇区组成了一个**逻辑块**，每次读写的最小单位就是逻辑块（数据块），Linux 中的逻辑块大小为 `4KB`，也就是一次性读写 8 个扇区，这将大大提高了磁盘的读写的效率。



磁盘进行格式化的时候，会被分成三个存储区域，分别是**超级块、索引节点区和数据块区。**

- *超级块*，用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等。
- *索引节点区*，用来存储索引节点；
- *数据块区*，用来存储文件或目录数据；

加载进内存的时机是不同的：

- 超级块：当文件系统挂载时进入内存；
- 索引节点区：当文件被访问时进入内存；

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291710658.png" alt="_E7_9B_AE_E5_BD_95_E9_A1_B9_E5_92_8C_E7_B4_A2_E5_BC_95_E5_85_B3_E7_B3_BB_E5_9B_BE" style="zoom: 50%;" />

### 虚拟文件系统

文件系统的种类众多，而操作系统希望**对用户提供一个统一的接口**，于是在用户层与文件系统层引入了中间层，这个中间层就称为**虚拟文件系统（\*Virtual File System，VFS\*）。**

VFS 定义了一组所有文件系统都支持的数据结构和标准接口，这样程序员不需要了解文件系统的工作原理，只需要了解 VFS 提供的统一接口即可。

Linux 支持的文件系统也不少，根据存储位置的不同，可以把文件系统分为三类：

- *磁盘的文件系统*，它是直接把数据存储在磁盘中，比如 Ext 2/3/4、XFS 等都是这类文件系统。
- *内存的文件系统*，这类文件系统的数据不是存储在硬盘的，而是占用内存空间，我们经常用到的 `/proc` 和 `/sys` 文件系统都属于这一类，读写这类文件，实际上是读写内核中相关的数据。
- *网络的文件系统*，用来访问其他计算机主机数据的文件系统，比如 NFS、SMB 等等。

文件系统首先要先挂载到某个目录才可以正常使用，比如 Linux 系统在启动时，会把文件系统挂载到根目录

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291712051.png" alt="_E8_99_9A_E6_8B_9F_E6_96_87_E4_BB_B6_E7_B3_BB_E7_BB_9F" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408300013223.jpeg" alt="img" style="zoom:50%;" />

**Page Cache 用于缓存文件的页数据，buffer cache 用于缓存块设备（如磁盘）的块数据。**

- 页是逻辑上的概念，因此 Page Cache 是与文件系统同级的；
- 块是物理上的概念，因此 buffer cache 是与块设备驱动程序同级的。

**在 2.4 版本内核之后，两块缓存近似融合在了一起：如果一个文件的页加载到了 Page Cache，那么同时 buffer cache 只需要维护块指向页的指针就可以了**。只有那些没有文件表示的块，或者绕过了文件系统直接操作（如dd命令）的块，才会真正放到 buffer cache 里。

因此，**我们现在提起 Page Cache，基本上都同时指 Page Cache 和 buffer cache 两者，本文之后也不再区分，直接统称为 Page Cache**。



### 文件系统的结构

前面提到 Linux 是用位图的方式管理空闲空间，用户在创建一个新文件时，Linux 内核会通过 inode 的位图找到空闲可用的 inode，并进行分配。要存储数据时，会通过块的位图找到空闲的块，并分配，但仔细计算一下还是有问题的。

数据块的位图是放在磁盘块里的，假设是放在一个块里，一个块 4K，每位表示一个数据块，共可以表示 `4 * 1024 * 8 = 2^15` 个空闲块，由于 1 个数据块是 4K 大小，那么最大可以表示的空间为 `2^15 * 4 * 1024 = 2^27` 个 byte，也就是 128M。

也就是说按照上面的结构，如果采用「一个块的位图 + 一系列的块」，外加「一个块的 inode 的位图 + 一系列的 inode 的结构」能表示的最大空间也就 128M，这太少了，现在很多文件都比这个大。

在 Linux 文件系统，把这个结构称为一个**块组**，那么有 N 多的块组，就能够表示 N 大的文件。

下图给出了 Linux Ext2 整个文件系统的结构和块组的内容，文件系统都由大量块组组成，在硬盘上相继排布：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291953284.png" alt="_E5_9D_97_E7_BB_84" style="zoom:50%;" />

最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下：

- *超级块*，包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。
- *块组描述符*，包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。
- *数据位图和 inode 位图*， 用于表示对应的数据块或 inode 是空闲的，还是被使用中。
- *inode 列表*，包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。
- *数据块*，包含文件的有用数据。

你可以会发现每个块组里有很多重复的信息，比如**超级块和块组描述符表，这两个都是全局信息，而且非常的重要**，这么做是有两个原因：

- 如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。
- 通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。

不过，Ext2 的后续版本采用了稀疏技术。该做法是，超级块和块组描述符表不再存储到文件系统的每个块组中，而是只写入到块组 0、块组 1 和其他 ID 可以表示为 3、 5、7 的幂的块组中









### 打开文件

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291802807.png" alt="_E5_86_99_E5_88_B0_E7_A3_81_E7_9B_98_E8_BF_87_E7_A8_8B" style="zoom:50%;" />

```c
fd = open(name, flag); # 打开文件
...
write(fd,...);         # 写数据
...
close(fd);             # 关闭文件
```

上面简单的代码是读取一个文件的过程：

- 首先用 `open` 系统调用打开文件，`open` 的参数中包含文件的路径名和文件名。
- 使用 `write` 写数据，其中 `write` 使用 `open` 所返回的**文件描述符**，并不使用文件名作为参数。
- 使用完文件后，要用 `close` 系统调用关闭文件，避免资源的泄露。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291802050.png" alt="_E6_96_87_E4_BB_B6_E6_89_93_E5_BC_80_E8_A1_A8" style="zoom:50%;" />

操作系统在打开文件表中维护着打开文件的状态和信息：

- 文件指针：系统跟踪上次读写位置作为当前文件位置指针，这种指针对打开文件的某个进程来说是唯一的；
- 文件打开计数器：文件关闭时，操作系统必须重用其打开文件表条目，否则表内空间不够用。因为多个进程可能打开同一个文件，所以系统在删除打开文件条目之前，必须等待最后一个进程关闭文件，该计数器跟踪打开和关闭的数量，当该计数为 0 时，系统关闭文件，删除该条目；
- 文件磁盘位置：绝大多数文件操作都要求系统修改文件数据，该信息保存在内存中，以免每个操作都从磁盘中读取；
- 访问权限：每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开文件表中，以便操作系统能允许或拒绝之后的 I/O 请求；



#### PIO（Programmed Input/Output）和 I/O（Input/Output）

都涉及计算机系统中的输入和输出操作，但它们在执行方式上有所不同。

- **PIO（Programmed Input/Output）**： PIO 是一种基本的 I/O 操作方式，它由 CPU 直接控制。在 PIO 中，CPU 通过程序控制向外部设备发送或接收数据。在这种模式下，CPU 需要不断地轮询设备状态以及数据准备好的情况，然后才能执行数据传输。这种方式会占用大量的 CPU 时间，并且效率较低。
- **I/O（Input/Output）**： I/O 是一个更广泛的概念，它表示计算机系统与外部设备之间进行数据传输和交换的过程。I/O 操作可以通过不同的方式进行，包括 PIO、DMA（Direct Memory Access，直接内存访问）等。与 PIO 不同，DMA 允许外部设备直接与内存进行数据交换，而无需 CPU 不断干预，这提高了数据传输的效率。

总的来说，PIO 是一种特定的 I/O 操作方式，而 I/O 是一个更宽泛的概念，表示计算机系统中数据输入和输出的整体过程，包括不同方式的数据传输和交换。



### 目录的存储

目录其实也是个文件，你甚至可以通过 `vim` 打开它，它也有 inode，inode 里面也是指向一些块。

和普通文件不同的是，**普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。**

在目录文件的块中，最简单的保存格式就是**列表**，就是一项一项地将目录下的文件信息（如文件名、文件 inode、文件类型等）列在表里。

列表中每一项就代表该目录下的文件的文件名和对应的 inode，通过这个 inode，就可以找到真正的文件。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292024878.png" alt="_E7_9B_AE_E5_BD_95_E5_93_88_E5_B8_8C_E8_A1_A8 (1)" style="zoom:50%;" />

通常，第一项是「`.`」，表示当前目录，第二项是「`..`」，表示上一级目录，接下来就是一项一项的文件名和 inode。

如果一个目录有超级多的文件，我们要想在这个目录下找文件，按照列表一项一项的找，效率就不高了。

于是，保存目录的格式改成**哈希表**，对文件名进行哈希计算，把哈希值保存起来

Linux 系统的 ext 文件系统就是采用了哈希表，来保存目录的内容，这种方法的优点是查找非常迅速，插入和删除也较简单，不过需要一些预备措施来避免哈希冲突。

目录查询是通过在磁盘上反复搜索完成，需要不断地进行 I/O 操作，开销较大。所以，为了减少 I/O 操作，把当前使用的文件目录**缓存在内存**，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数，提高了文件系统的访问速度。



### 文件的存储

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291817902.png" alt="_E6_96_87_E4_BB_B6_E5_AD_98_E5_82_A8_E6_96_B9_E5_BC_8F_E6_AF_94_E8_BE_83" style="zoom: 67%;" />

#### 连续空间存放方式

- **文件存放在磁盘「连续的」物理空间中**。**读写效率很高**，因为一次磁盘寻道就可以读出整个文件。

  使用连续存放的方式有一个前提，必须先知道一个文件的大小，**文件头里需要指定「起始块的位置」和「长度」**，有了这两个信息就可以很好的表示文件存放方式是一块连续的磁盘空间。

  读写效率高，**但是有「磁盘空间碎片」和「文件长度不易扩展」的缺陷。**

#### 非连续空间存放方式

1. 「链表方式」

   1. 「**隐式链表**」

      实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置

      - ****缺点在于无法直接访问数据块，只能通过指针顺序访问文件，以及数据块指针消耗了一定的存储空间。**
      - 隐式链接分配的**稳定性较差**，系统在运行过程中由于软件或者硬件错误**导致链表中的指针丢失或损坏，会导致文件数据的丢失。**

   2. 「**显式链接**」

      把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中，该表在整个磁盘仅设置一张，每个表项中存放链接指针，指向下一个数据块号。

      内存中的这样一个表格称为**文件分配表（\*File Allocation Table，FAT\*）**。

      不仅显著地**提高了检索速度**，而且**大大减少了访问磁盘的次数**。缺点是**不适用于大磁盘**。

2. 「索引方式」

   每个文件创建一个「**索引数据块**」，里面存放的是**指向文件数据块的指针列表**，文件头需要包含指向「索引数据块」的指针

   ​	优点在于：

   - 文件的创建、增大、缩小很方便；
   - 不会有碎片的问题；
   - 支持顺序读写和随机读写；

   缺陷之一就是存储索引带来的开销。

3. **链式索引块**：**在索引数据块留出一个存放下一个索引数据块的指针**

4. **多级索引块**：实现方式是**通过一个索引块来存放多个索引数据块**



#### Unix 文件的实现方式

#####  Linux Ext 2/3 

它是根据文件的大小，存放的方式会有所变化：

- 如果存放文件所需的数据块小于 10 块，则采用直接查找的方式；
- 如果存放文件所需的数据块超过 10 块，则采用一级间接索引方式；
- 如果前面两种方式都不够存放大文件，则采用二级间接索引方式；
- 如果二级间接索引也不够存放大文件，这采用三级间接索引方式；

那么，文件头（*Inode*）就需要包含 13 个指针：

- 10 个指向数据块的指针；
- 第 11 个指向索引块的指针；
- 第 12 个指向二级索引块的指针；
- 第 13 个指向三级索引块的指针；

所以，这种方式能很灵活地支持小文件和大文件的存放，虽然解决大文件的存储，但是**对于大文件的访问，需要大量的查询，效率比较低。**

##### Ext4 的改进措施

1. **Extents（区段）机制**：
   - **Extents** 是 Ext4 引入的一种数据存储方式，它替代了传统的多级间接索引。Extents 将连续的数据块表示为一个区段，从而减少了存储大文件时需要的指针数量和查找复杂性。
   - 每个 Extent 记录一个连续的磁盘块范围及其对应的文件偏移，这大大减少了对磁盘块的查找次数。
   - **好处**：对于大文件，使用 Extents 可以显著减少需要查找的索引块数量，提高了文件读取的效率。

2. **预分配（Preallocation）**：
   - Ext4 支持文件预分配，即在文件写入之前提前为文件分配一大块连续的磁盘空间。这避免了文件增长时频繁分配和管理新数据块，减少了文件碎片，提高了访问速度。
   - 通过预分配，文件系统可以在写入过程中尽可能多地使用连续的磁盘空间，从而减少文件的碎片化。

3. **多块分配（Multiblock Allocation）**：
   - Ext4 在分配数据块时可以一次分配多个块（与传统的单块分配不同），这使得文件系统在写入大文件时可以获得更高的写入性能和更好的磁盘空间利用率。
   - 多块分配结合 Extents，进一步优化了大文件的存储和访问性能。

4. **延迟分配（Delayed Allocation）**：
   - Ext4 通过延迟分配（写时分配）的方式，先将写入的数据缓存在内存中，等到实际写入磁盘时再分配数据块。这样，文件系统可以更智能地分配连续的数据块，提高磁盘空间的利用率并减少碎片。
   - 延迟分配也与 Extents 机制相结合，提高了文件的写入效率和存储性能。

总结

- **传统的多级间接索引**在 Ext2/3 文件系统中被广泛使用，能够支持小文件和大文件的存储，但对于大文件的访问，特别是在需要通过多级索引进行查找时，性能可能会下降。
- **Ext4** 文件系统通过引入 Extents、预分配、多块分配和延迟分配等机制，优化了大文件的存储和访问效率。Extents 特别有效地减少了大文件的索引开销，提高了读取和写入大文件时的性能。

这些改进使 Ext4 能够更好地支持现代大文件的存储需求，同时保持对小文件的高效处理。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291850481.png" alt="Unix_20_E5_A4_9A_E7_BA_A7_E7_B4_A2_E5_BC_95" style="zoom:33%;" />

### 空闲空间管理

存一个数据块，我应该放在硬盘上的哪个位置

- 空闲表法

  - 空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，适用于建立**连续文件**

  - 当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。

    这种方法仅当有少量的空闲区时才有较好的效果。![_E7_A9_BA_E9_97_B2_E8_A1_A8_E6_B3_95](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408291926192.png)

- 空闲链表法

  - 这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，

- 位图法

  -  Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理

  - 当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：

    ```text
    1111110011111110001110110111111100111 ...
    ```





### 软链接和硬链接

有时候我们希望给某个文件**取别名**，那么在 Linux 中可以通过**硬链接（\*Hard Link\*）** 和**软链接（\*Symbolic Link\*）** 

硬链接是**多个目录项中的「索引节点」指向一个文件**，也就是指向**同一个 inode**，但是 inode 是不可能跨越文件系统的，每个文件系统都有各自的 inode 数据结构和列表，所以**硬链接是不可用于跨文件系统的**。由于多个目录项都是指向一个 inode，那么**只有删除文件的所有硬链接以及源文件时，系统才会彻底删除该文件。**

软链接相当于重新创建一个文件，这个文件有**独立的 inode**，但是这个**文件的内容是另外一个文件的路径**，所以访问软链接的时候，实际上相当于访问到了另外一个文件，所以**软链接是可以跨文件系统的**，甚至**目标文件被删除了，链接文件还是在的，只不过指向的文件找不到了而已。**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292057300.png" alt="_E7_A1_AC_E9_93_BE_E6_8E_A5-2" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292057101.png" alt="_E8_BD_AF_E9_93_BE_E6_8E_A5" style="zoom:50%;" />





### 文件 I/O

文件的读写方式各有千秋，对于文件的 I/O 分类也非常多，常见的有

- 缓冲与非缓冲 I/O
- 直接与非直接 I/O
- 阻塞与非阻塞 I/O VS 同步与异步 I/O

接下来，分别对这些分类讨论讨论。

#### 缓冲与非缓冲 I/O

文件操作的标准库是可以实现数据的缓存，那么**根据「是否利用标准库缓冲」，可以把文件 I/O 分为缓冲 I/O 和非缓冲 I/O**：

- 缓冲 I/O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件。
- 非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存。

这里所说的「缓冲」特指标准库内部实现的缓冲。

比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，减少系统调用的次数，毕竟系统调用是有 CPU 上下文切换的开销的。



#### 直接与非直接 I/O

那么，**根据是「否利用操作系统的缓存--页缓存」，可以把文件 I/O 分为直接 I/O 与非直接 I/O**：

- 直接 I/O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。
- 非直接 I/O，读操作时，数据从内核缓存中拷贝给用户程序，写操作时，数据从用户程序拷贝给内核缓存，再由内核决定什么时候写入数据到磁盘。

如果你在使用文件操作类的系统调用函数时，指定了 `O_DIRECT` 标志，则表示使用直接 I/O。如果没有设置过，默认使用的是非直接 I/O。

> 如果用了非直接 I/O 进行写数据操作，内核什么情况下才会把缓存数据写入到磁盘？

以下几种场景会触发内核缓存的数据写入磁盘：

- 在调用 `write` 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上；
- 用户主动调用 `sync`，内核缓存会刷到磁盘上；
- 当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上；
- 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；



#### 阻塞与非阻塞 I/O VS 同步与异步 I/O

I/O 是分为两个过程的：

1. 数据准备的过程
2. 数据从内核空间拷贝到用户进程缓冲区的过程

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292141290.png" alt="_E5_90_8C_E6_AD_A5VS_E5_BC_82_E6_AD_A5IO" style="zoom:50%;" />

先来看看**阻塞 I/O**，当用户程序执行 `read` ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，`read` 才会返回。

注意，**阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程**。过程如下图：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292138580.png" alt="_E9_98_BB_E5_A1_9E_20I_O" style="zoom:33%;" />

**非阻塞 I/O**，非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，`read` 调用才可以获取到结果。过程如下图：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292138939.png" alt="_E9_9D_9E_E9_98_BB_E5_A1_9E_20I_O_20" style="zoom:33%;" />

注意，**这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。**

举个例子，访问管道或 socket 时，如果设置了 `O_NONBLOCK` 标志，那么就表示使用的是非阻塞 I/O 的方式访问，而不做任何设置的话，默认是阻塞 I/O。



应用程序每次轮询内核的 I/O 是否准备好，感觉有点傻乎乎，因为轮询的过程中，应用程序啥也做不了，只是在循环。

为了解决这种傻乎乎轮询方式，于是 **I/O 多路复用**技术就出来了，如 select、poll，它是通过 I/O 事件分发，当内核数据准备好时，再以事件通知应用程序进行操作。

这个做法大大改善了 CPU 的利用率，因为当调用了 I/O 多路复用接口，如果没有事件发生，那么当前线程就会发生阻塞，这时 CPU 会切换其他线程执行任务，等内核发现有事件到来的时候，会唤醒阻塞在 I/O 多路复用接口的线程，然后用户可以进行后续的事件处理。

整个流程要比阻塞 IO 要复杂，似乎也更浪费性能。但 **I/O 多路复用接口最大的优势在于，用户可以在一个线程内同时处理多个 socket 的 IO 请求**（参见：[I/O 多路复用：select/poll/epoll (opens new window)](https://xiaolincoding.com/os/8_network_system/selete_poll_epoll.html)）。用户可以注册多个 socket，然后不断地调用 I/O 多路复用接口读取被激活的 socket，即可达到在同一个线程内同时处理多个 IO 请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。

下图是使用 select I/O 多路复用过程。注意，`read` 获取数据的过程（数据从内核态拷贝到用户态的过程），也是一个**同步的过程**，需要等待：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292139155.png" alt="_E5_9F_BA_E4_BA_8E_E9_9D_9E_E9_98_BB_E5_A1_9E_20I_O_20_E7_9A_84_E5_A4_9A_E8_B7_AF_E5_A4_8D_E7_94_A8" style="zoom:33%;" />

​	无论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用**都是同步调用。因为它们在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。**

真正的**异步 I/O** 是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待。

当我们发起 `aio_read` 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。过程如下图：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292141121.png" alt="_E5_BC_82_E6_AD_A5_20I_O" style="zoom: 50%;" />



## 设备管理

### 基本

每个设备都有一个**设备控制器（\*Device Control\*）**

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292353390.png" alt="I_O_E7_B3_BB_E7_BB_9F_E7_BB_93_E6_9E_84" style="zoom:50%;" />

设备控制器里有芯片，它可执行自己的逻辑，也有自己的寄存器，用来与 CPU 进行通信，比如：

- 通过写入这些寄存器，操作系统可以命令设备发送数据、接收数据、开启或关闭，或者执行某些其他操作。
- 通过读取这些寄存器，操作系统可以了解设备的状态，是否准备好接收一个新的命令等。

实际上，控制器是有三类寄存器：

- *数据寄存器*，CPU 向 I/O 设备写入需要传输的数据，比如要打印的内容是「Hello」，CPU 就要先发送一个 H 字符给到对应的 I/O 设备。
- *命令寄存器*，CPU 发送一个命令，告诉 I/O 设备，要进行输入/输出操作，于是就会交给 I/O 设备去工作，任务完成后，会把状态寄存器里面的状态标记为完成。
- *状态寄存器*，目的是告诉 CPU ，现在已经在工作或工作已经完成，如果已经在工作状态，CPU 再发送数据或者命令过来，都是没有用的，直到前面的工作已经完成，状态寄存标记成已完成，CPU 才能发送下一个字符和命令。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408292354307.png" alt="_E8_AE_BE_E5_A4_87_E6_8E_A7_E5_88_B6_E5_99_A8" style="zoom:50%;" />

CPU 通过读写设备控制器中的寄存器控制设备，这可比 CPU 直接控制输入输出设备，要方便和标准很多。

另外， 输入输出设备可分为两大类 ：**块设备（\*Block Device\*）\**和\**字符设备（\*Character Device\*）**。

- *块设备*，把数据存储在固定大小的块中，每个块有自己的地址，硬盘、USB 是常见的块设备。
- *字符设备*，以字符为单位发送或接收一个字符流，字符设备是不可寻址的，也没有任何寻道操作，鼠标是常见的字符设备。

块设备通常传输的数据量会非常大，于是控制器设立了一个可读写的**数据缓冲区**。

- CPU 写入数据到控制器的缓冲区时，当缓冲区的数据囤够了一部分，才会发给设备。
- CPU 从控制器的缓冲区读取数据时，也需要缓冲区囤够了一部分，才拷贝到内存。

这样做是为了，减少对设备的频繁操作。



### 通信方法

那 CPU 是如何与设备的控制寄存器和数据缓冲区进行通信的？存在两个方法：

- *端口 I/O*，每个控制寄存器被分配一个 I/O 端口，可以通过特殊的汇编指令操作这些寄存器，比如 `in/out` 类似的指令。
- *内存映射 I/O*，将所有控制寄存器映射到内存空间中，这样就可以像读写内存一样读写数据缓冲区

### 存储系统 I/O 软件分层

三个层次的作用是：

- 文件系统层，包括虚拟文件系统和其他文件系统的具体实现，它向上为应用程序统一提供了标准的文件访问接口，向下会通过通用块层来存储和管理磁盘数据。
- 通用块层，包括块设备的 I/O 队列和 I/O 调度器，它会对文件系统的 I/O 请求进行排队，再通过 I/O 调度器，选择一个 I/O 发给下一层的设备层。
- 设备层，包括硬件设备、设备控制器和驱动程序，负责最终物理设备的 I/O 操作。

有了文件系统接口之后，不但可以通过文件系统的命令行操作设备，也可以通过应用程序，调用 `read`、`write` 函数，就像读写文件一样操作设备，所以说设备在 Linux 下，也只是一个特殊的文件。

但是，除了读写操作，还需要有检查特定于设备的功能和属性。于是，需要 `ioctl` 接口，它表示输入输出控制接口，是用于配置和修改特定设备属性的通用接口。

另外，存储系统的 I/O 是整个系统最慢的一个环节，所以 Linux 提供了不少缓存机制来提高 I/O 的效率。

- 为了提高文件访问的效率，会使用**页缓存、索引节点缓存、目录项缓存**等多种缓存机制，目的是为了减少对块设备的直接调用。
- 为了提高块设备的访问效率， 会使用**缓冲区**，来缓存块设备的数据

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408300013223.jpeg" alt="img" style="zoom:50%;" />



### I/O 控制方式

每种设备都有一个设备控制器，控制器相当于一个小 CPU

第一种**轮询等待**的方法，让 CPU 一直查寄存器的状态，直到状态标记为完成，会占用 CPU 的全部时间。

第二种方法 —— **中断**

一种**软中断**，例如代码调用 `INT` 指令触发，一种是**硬件中断**，就是硬件通过中断控制器触发的。



#### DMA 

但中断的方式对于频繁读写数据的磁盘，并不友好，这样 CPU 容易经常被打断，会占用 CPU 大量的时间。对于这一类设备的问题的解决方法是使用 **DMA（\*Direct Memory Access\*）** 功能，它可以使得设备在 CPU 不参与的情况下，能够自行完成把设备 I/O 数据放入到内存。那要实现 DMA 功能要有 「DMA 控制器」硬件的支持。

- CPU 需对 DMA 控制器下发指令，告诉它想读取多少数据，读完的数据放在内存的某个地方就可以了；
- 接下来，DMA 控制器会向磁盘控制器发出指令，通知它从磁盘读数据到其内部的缓冲区中，接着磁盘控制器将缓冲区的数据传输到内存；
- 当磁盘控制器把数据传输到内存的操作完成后，磁盘控制器在总线上发出一个确认成功的信号到 DMA 控制器；
- DMA 控制器收到信号后，DMA 控制器发中断通知 CPU 指令完成，CPU 就可以直接用内存里面现成的数据了；

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408300006596.png" alt="DMA_E5_B7_A5_E4_BD_9C_E5_8E_9F_E7_90_86" style="zoom:50%;" />

### 设备驱动程序

为了屏蔽「设备控制器」的差异，引入了**设备驱动程序**。

设备控制器不属于操作系统范畴，它是属于硬件，而设备驱动程序属于操作系统的一部分，不同的设备控制器虽然功能不同，但是**设备驱动程序会提供统一的接口给操作系统**

设备驱动程序初始化的时候，要先注册一个该设备的**中断处理函数**。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408300010166.png" alt="_E9_A9_B1_E5_8A_A8_E7_A8_8B_E5_BA_8F" style="zoom:33%;" />

###  通用块层

对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过一个统一的**通用块层**，来管理不同的块设备。

通用块层是处于文件系统和磁盘驱动中间的一个块设备抽象层，它主要有两个功能：

- 第一个功能，向上为文件系统和应用程序，提供访问块设备的标准接口，向下把各种不同的磁盘设备抽象为统一的块设备，并在内核层面，提供一个框架来管理这些设备的驱动程序；
- 第二功能，通用层还会给文件系统和应用程序发来的 I/O 请求排队，接着会对队列重新排序、请求合并等方式，也就是 I/O 调度，主要目的是为了提高磁盘读写的效率。

Linux 内存支持 5 种 I/O 调度算法，分别是：

- 没有调度算法
- 先入先出调度算法
- 完全公平调度算法
- 优先级调度
- 最终期限调度算法

第一种，没有调度算法，是的，你没听错，它不对文件系统和应用程序的 I/O 做任何处理，这种算法常用在虚拟机 I/O 中，此时磁盘 I/O 调度算法交由物理机系统负责。

第二种，先入先出调度算法，这是最简单的 I/O 调度算法，先进入 I/O 调度队列的 I/O 请求先发生。

第三种，完全公平调度算法，大部分系统都把这个算法作为默认的 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。

第四种，优先级调度算法，顾名思义，优先级高的 I/O 请求先发生， 它适用于运行大量进程的系统，像是桌面环境、多媒体应用等。

第五种，最终期限调度算法，分别为读、写请求创建了不同的 I/O 队列，这样可以提高机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理，适用于在 I/O 压力比较大的场景，比如数据库等





### 键盘敲入字母时，期间发生了什么？

CPU 里面的内存接口，直接和系统总线通信，然后系统总线再接入一个 I/O 桥接器，这个 I/O 桥接器，另一边接入了内存总线，使得 CPU 和内存通信。再另一边，又接入了一个 I/O 总线，用来连接 I/O 设备，比如键盘、显示器等。

那当用户输入了键盘字符，**键盘控制器**就会产生扫描码数据，并将其缓冲在键盘控制器的寄存器中，紧接着键盘控制器通过总线给 CPU 发送**中断请求**。

CPU 收到中断请求后，操作系统会**保存被中断进程的 CPU 上下文**，然后调用键盘的**中断处理程序**。

键盘的中断处理程序是在**键盘驱动程序**初始化时注册的，那键盘**中断处理函数**的功能就是从键盘控制器的寄存器的缓冲区读取扫描码，再根据扫描码找到用户在键盘输入的字符，如果输入的字符是显示字符，那就会把扫描码翻译成对应显示字符的 ASCII 码，比如用户在键盘输入的是字母 A，是显示字符，于是就会把扫描码翻译成 A 字符的 ASCII 码。

得到了显示字符的 ASCII 码后，就会把 ASCII 码放到「读缓冲区队列」，接下来就是要把显示字符显示屏幕了，显示设备的驱动程序会定时从「读缓冲区队列」读取数据放到「写缓冲区队列」，最后把「写缓冲区队列」的数据一个一个写入到显示设备的控制器的寄存器中的数据缓冲区，最后将这些数据显示在屏幕里。

显示出结果后，**恢复被中断进程的上下文**。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408300018337.png" alt="CPU_20_E7_A1_AC_E4_BB_B6_E6_80_BB_E7_BA_BF_E5_9B_BE" style="zoom:50%;" />



## 网络系统

### 零拷贝

“零拷贝”（Zero-Copy）是一种优化计算机系统中数据传输效率的技术，用于减少 CPU 在数据传输时的拷贝操作，从而提升性能。它广泛应用于操作系统、网络通信和文件 I/O 中。

在零拷贝机制下，数据可以在内存和设备之间直接传递，而无需经过传统的数据拷贝步骤，因而避免了不必要的 CPU 开销。

 CPU 不再参与「将数据从磁盘控制器缓冲区搬运到内核空间」的工作，这部分工作全程由 **DMA** 完成

**要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数**，**用户的缓冲区是没有必要存在的**

#### **零拷贝的优点**

1. **减少 CPU 占用**：降低用户空间和内核空间之间的切换和数据拷贝。
2. **提高带宽利用率**：更高效地使用内存和总线。
3. **提升吞吐量**：对高流量传输尤其明显。



#### 使用场景

**传输小文件的时候，使用「零拷贝技术」**

大文件的传输不应该使用 PageCache，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache。



#### 实现零拷贝

零拷贝技术实现的方式：

1. mmap + write

   - mmap对大文件传输有一定优势，但是小文件可能出现碎片，并且在多个进程同时操作文件时可能产生引发coredump的signal。
   -  4 次上下文切换，1次CPU拷贝、2次DMA拷贝。
   -  `mmap` 可以将文件映射到用户空间，虽然 `mmap` 减少了一次数据拷贝，但 CPU 仍然需要进行 `write` 操作时的内核到用户空间拷贝，因此严格来说不完全是零拷贝。
   - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408301619684.png" alt="mmap_20_2B_20write_20_E9_9B_B6_E6_8B_B7_E8_B4_9D" style="zoom:33%;" />

2. sendfile

   -  Linux 内核版本 2.1引入，2次状态切换、1次CPU拷贝、2次DMA拷贝。

   -  ```c
     ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
     ```
     
     <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408301620898.png" alt="senfile-3_E6_AC_A1_E6_8B_B7_E8_B4_9D" style="zoom:33%;" />

3. sendfile+SG-DMA（*The Scatter-Gather Direct Memory Access*）

   - 2次状态切换、0次CPU拷贝、2次DMA拷贝，但是仍然无法对数据进行修改，并且需要硬件层面DMA的支持，并且sendfile只能将文件数据拷贝到socket描述符上，有一定的局限性。
   - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408301616582.png" alt="senfile-_E9_9B_B6_E6_8B_B7_E8_B4_9D" style="zoom: 33%;" />

4. splice方式

   splice系统调用是Linux 在 2.6 版本引入的，其不需要硬件支持，并且不再限定于socket上，实现两个普通文件之间的数据零拷贝。splice也有一些局限，它的两个文件描述符参数中有一个必须是管道设备。

   <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408301620367.png" alt="127450662-6316fa68-a493-42e2-9de1-b2857794142d" style="zoom: 50%;" />

   

#### 项目

##### Kafka

用了 Java NIO 库里的 `transferTo` 方法：

```java
@Overridepublic 
long transferFrom(FileChannel fileChannel, long position, long count) throws IOException { 
    return fileChannel.transferTo(position, count, socketChannel);
}
```

如果 Linux 系统支持 `sendfile()` 系统调用，那么 `transferTo()` 实际上最后就会使用到 `sendfile()` 系统调用函数

##### Nginx

Nginx 在发送静态文件时使用了 `sendfile` 系统调用。这使得它能够直接从文件系统读取文件并发送到客户端，而不需要在用户空间和内核空间之间来回复制数据，从而大大提高了传输效率。 

一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下：

```text
http {
...
    sendfile on
...
}
```

sendfile 配置的具体意思:

- 设置为 on 表示，使用零拷贝技术来传输文件：sendfile ，这样只需要 2 次上下文切换，和 2 次数据拷贝。
- 设置为 off 表示，使用传统的文件传输技术：read + write，这时就需要 4 次上下文切换，和 4 次数据拷贝。

##### **Apache Flink**

- **描述**：Flink 是一个分布式流处理框架，支持高吞吐量和低延迟的数据处理。
- **使用的零拷贝技术**：在数据分发和处理过程中，Flink 使用了零拷贝技术来优化数据传输和处理性能，特别是在数据交换和网络通信部分。

#####  **Redis**

- **描述**：Redis 是一个高性能的键值存储数据库，广泛应用于缓存、实时分析等场景。
- **使用的零拷贝技术**：在某些操作系统和配置下，Redis 可以利用 `sendfile` 进行高效的持久化数据传输和同步，减少数据在网络传输中的 CPU 负载。





### 大文件传输

- 传输大文件的时候，使用「异步 I/O + 直接 I/O」；
  - <img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408301631124.png" alt="_E5_BC_82_E6_AD_A5_20IO_20_E7_9A_84_E8_BF_87_E7_A8_8B" style="zoom: 50%;" />
- 传输小文件的时候，则使用「零拷贝技术」；

**在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术**。

直接 I/O 应用场景常见的两种：

- 应用程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启；
- 传输大文件的时候，由于大文件难以命中 PageCache 缓存，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I/O。
- 

####  nginx配置

我们可以用如下配置，来根据文件的大小来使用不同的方式：

```text
location /video/ { 
    sendfile on; 
    aio on; 
    directio 1024m; 
}
```

当文件大小大于 `directio` 值后，使用「异步 I/O + 直接 I/O」，否则使用「零拷贝技术」。



### I/O 多路复用：select/poll/epoll

 C10K 问题 ，C 是 Client 单词首字母缩写，C10K 就是单机同时处理 1 万个请求的问题。

**多路复用 API 返回的事件并不一定可读写的**，如果使用阻塞 I/O， 那么在调用 read/write 时则会发生程序阻塞，因此**最好搭配非阻塞 I/O**，以便应对极少数的特殊情况

#### 多进程

因为子进程会**复制父进程的文件描述符**，于是就可以直接使用「已连接 Socket 」和客户端通信了，

子进程关心「已连接 Socket」；父进程关心「监听 Socket」。

![_E5_A4_9A_E8_BF_9B_E7_A8_8B](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408301851528.png)

#### 多线程模型

使用**线程池**的方式来避免线程的频繁创建和销毁

需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。

![_E7_BA_BF_E7_A8_8B_E6_B1_A0](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408301855080.png)

#### I/O 多路复用

一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用。

我们熟悉的 select/poll/epoll 内核提供给用户态的多路复用系统调用，**进程可以通过一个系统调用函数从内核中获取多个事件**。

select/poll/epoll 是如何获取网络事件的呢？在获取事件时，先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中再处理这些连接对应的请求即可。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408301859916.png" alt="_E5_A4_9A_E8_B7_AF_E5_A4_8D_E7_94_A8" style="zoom: 50%;" />

####  select/poll

select 实现多路复用的方式是，将已连接的 Socket 都放到一个**文件描述符集合**，然后调用 select 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过**遍历**文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合**拷贝**回用户态里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。

所以，对于 select 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次「拷贝」文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。

**select 使用固定长度的 BitsMap**，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 `1024`，只能监听 0~1023 的文件描述符。

**poll** 用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。

但是 poll 和 select 并没有太大的本质区别，**都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。

####  epoll 

1. 先用e poll_create 创建一个 epol l对象 epfd，
2. 再通过 epoll_ctl 将需要监视的 socket 添加到epfd中，
3. 最后调用 epoll_wait 等待数据。

```c
int s = socket(AF_INET, SOCK_STREAM, 0);
bind(s, ...);
listen(s, ...)

int epfd = epoll_create(...);
epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中

while(1) {
    int n = epoll_wait(...);
    for(接收到数据的socket){
        //处理
    }
}
```

epoll 通过两个方面，很好解决了 select/poll 的问题。

- epoll 在内核里使用「红黑树」来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删改一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。
- epoll 使用事件驱动的机制，内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和无事件的 Socket ），大大提高了检测的效率。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408301915186.jpeg" alt="img" style="zoom:50%;" />

##### 边缘触发和水平触发

epoll 支持两种事件触发模式，分别是**边缘触发（\*edge-triggered，ET\*）\**和\**水平触发（\*level-triggered，LT\*）**。

这两个术语还挺抽象的，其实它们的区别还是很好理解的。

- 使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，**服务器端只会从 epoll_wait 中苏醒一次**，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；
- 使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，**服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束**，目的是告诉我们有数据需要读取；

**边缘触发模式一般和非阻塞 I/O 搭配使用**，程序会一直执行 I/O 操作，直到系统调用（如 `read` 和 `write`）返回错误，错误类型为 `EAGAIN` 或 `EWOULDBLOCK`。

一般来说，**边缘触发的效率比水平触发的效率要高**，因为边缘触发可以减少 epoll_wait 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。

select/poll 只有水平触发模式，**epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。**

注意：**多路复用 API 返回的事件并不一定可读写的**，如果使用阻塞 I/O， 那么在调用 read/write 时则会发生程序阻塞，因此**最好搭配非阻塞 I/O**，以便应对极少数的特殊情况



### 高性能网络模式：Reactor 和 Proactor

Reactor 是**非阻塞同步网络模式**， **Proactor 是异步网络模式**。

- **Reactor 是非阻塞同步网络模式，感知的是就绪可读写事件**。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的，读取完数据后应用进程才能处理数据。
- **Proactor 是异步网络模式， 感知的是已完成的读写事件**。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。

Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」，而 Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」



#### Reactor 

Reactor 模式也叫 `Dispatcher` 模式，即 **I/O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程**。

Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成，它俩负责的事情如下：

- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send；

常用：

- 单 Reactor 单进程 / 线程；
- 单 Reactor 多线程 / 进程；
- 多 Reactor 多进程 / 线程；

方案具体使用进程还是线程，要看使用的编程语言以及平台有关：

- Java 语言一般使用线程，比如 Netty;
- C 语言使用进程和线程都可以，例如 Nginx 使用的是进程，Memcache 使用的是线程。

##### 单 Reactor 单进程 / 线程

Java 语言实现的是「**单 Reactor \*单线程\***」的方案，因为 Java 程序是跑在 Java 虚拟机这个进程上面的，虚拟机中有很多线程，我们写的 Java 程序只是其中的一个线程而已。

![_E5_8D_95Reactor_E5_8D_95_E8_BF_9B_E7_A8_8B](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408302103507.png)

可以看到进程里有 **Reactor、Acceptor、Handler** 这三个对象：

- Reactor 对象的作用是监听和分发事件；
- Acceptor 对象的作用是获取连接；
- Handler 对象的作用是处理业务；

对象里的 select、accept、read、send 是系统调用函数，dispatch 和 「业务处理」是需要完成的操作，其中 dispatch 是分发事件操作。

接下来，介绍下「单 Reactor 单进程」这个方案：

- Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

单 Reactor 单进程的方案因为全部工作都在同一个进程内完成，所以实现起来比较简单，不需要考虑进程间通信，也不用担心多进程竞争。

但是，这种方案存在 2 个缺点：

- 第一个缺点，因为只有一个进程，**无法充分利用 多核 CPU 的性能**；
- 第二个缺点，Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，**如果业务处理耗时比较长，那么就造成响应的延迟**；

所以，单 Reactor 单进程的方案**不适用计算机密集型的场景，只适用于业务处理非常快速的场景**。

Redis 是由 C 语言实现的，在 Redis 6.0 版本之前采用的正是「单 Reactor 单进程」的方案，因为 Redis 业务处理主要是在内存中完成，操作的速度是很快的，性能瓶颈不在 CPU 上，所以 Redis 对于命令的处理是单进程的方案。

##### 单 Reactor 多线程 / 多进程

###### 单 Reactor 多线程

详细说一下这个方案：

- Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；

上面的三个步骤和单 Reactor 单线程方案是一样的，接下来的步骤就开始不一样了：

- Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给子线程里的 Processor 对象进行业务处理；
- 子线程里的 Processor 对象就进行业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send 方法将响应结果发送给 client；

单 Reator 多线程的方案优势在于**能够充分利用多核 CPU 的能**，那既然引入多线程，那么自然就带来了多线程竞争资源的问题。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408302104286.png" alt="_E5_8D_95Reactor_E5_A4_9A_E7_BA_BF_E7_A8_8B" style="zoom: 33%;" />

###### 单 Reactor 多进程

事实上，单 Reactor 多进程相比单 Reactor 多线程实现起来很麻烦，主要因为要考虑子进程 <-> 父进程的双向通信，并且父进程还得知道子进程要将数据发送给哪个客户端。

而多线程间可以共享数据，虽然要额外考虑并发问题，但是这远比进程间通信的复杂度低得多，因此实际应用中也看不到单 Reactor 多进程的模式。

另外，「单 Reactor」的模式还有个问题，**因为一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方**



##### 多 Reactor 多进程 / 线程

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408302106543.png" alt="_E4_B8_BB_E4_BB_8EReactor_E5_A4_9A_E7_BA_BF_E7_A8_8B" style="zoom:50%;" />

方案详细说明如下：

- 主线程中的 MainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 对象中的 accept 获取连接，将新的连接分配给某个子线程；
- 子线程中的 SubReactor 对象将 MainReactor 对象分配的连接加入 select 继续进行监听，并创建一个 Handler 用于处理连接的响应事件。
- 如果有新的事件发生时，SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应。
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

多 Reactor 多线程的方案虽然看起来复杂的，但是实际实现时比单 Reactor 多线程的方案要简单的多，原因如下：

- 主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理。
- 主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。

大名鼎鼎的两个开源软件 Netty 和 Memcache 都采用了「多 Reactor 多线程」的方案。

采用了「多 Reactor 多进程」方案的开源软件是 Nginx，不过方案与标准的多 Reactor 多进程有些差异。

具体差异表现在主进程中仅仅用来初始化 socket，并没有创建 mainReactor 来 accept 连接，而是由子进程的 Reactor 来 accept 连接，通过锁来控制一次只有一个子进程进行 accept（防止出现惊群现象），子进程 accept 新连接后就放到自己的 Reactor 进行处理，不会再分配给其他子进程



####  Proactor

介绍一下 Proactor 模式的工作流程：

- Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核；
- Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作；
- Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor；
- Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理；
- Handler 完成业务处理；

可惜的是，在 Linux 下的异步 I/O 是不完善的， `aio` 系列函数是由 POSIX 定义的异步操作接口，不是真正的操作系统级别支持的，而是在用户空间模拟出来的异步，并且仅仅支持基于本地文件的 aio 异步操作，网络编程中的 socket 是不支持的，这也使得基于 Linux 的高性能网络程序都是使用 Reactor 方案。

而 Windows 里实现了一套完整的支持 socket 的异步编程接口，这套接口就是 `IOCP`，是由操作系统级别实现的异步 I/O，真正意义上异步 I/O，因此在 Windows 里实现高性能网络程序可以使用效率更高的 Proactor 方案

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408302110643.png" alt="Proactor" style="zoom:50%;" />





### 一致性哈希算法+虚拟节点

不同的**负载均衡算法**适用的业务场景也不同的。

轮询这类的策略只能适用与每个节点的数据都是相同的场景，访问任意节点都能请求到数据。但是不适用分布式系统，因为分布式系统意味着数据水平切分到了不同的节点上，访问数据的时候，一定要寻址存储该数据的节点。

哈希算法虽然能建立数据和节点的映射关系，但是每次在节点数量发生变化的时候，最坏情况下所有数据都需要迁移，这样太麻烦了，所以不适用节点数量变化的场景。

为了减少迁移的数据量，就出现了一致性哈希算法。

一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响。

但是一致性哈希算法不能够均匀的分布节点，会出现大量请求都集中在一个节点的情况，在这种情况下进行容灾与扩容时，容易出现雪崩的连锁反应。

为了解决一致性哈希算法不能够均匀的分布节点的问题，就需要引入虚拟节点，对一个真实节点做多个副本。不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。

引入虚拟节点后，可以会提高节点的均衡度，还会提高系统的稳定性。所以，带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景。

## 用户界面













## Linux指令

### cpu缓存

在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L1 Cache 「数据」缓存的容量大小：

```bash
$ cat /sys/devices/system/cpu/cpu0/cache/index0/size
32K
```

而查看 L1 Cache 「指令」缓存的容量大小，则是：

```bash
$ cat /sys/devices/system/cpu/cpu0/cache/index1/size
32K
```

在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L2 Cache 的容量大小：

```bash
$ cat /sys/devices/system/cpu/cpu0/cache/index2/size
256K
```

在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L3 Cache 的容量大小：

```bash
$ cat /sys/devices/system/cpu/cpu0/cache/index3/size 
3072K
```

### 系统里有哪些软中断？

在 Linux 系统里，我们可以通过查看 `/proc/softirqs` 的 内容来知晓「软中断」的运行情况，以及 `/proc/interrupts` 的 内容来知晓「硬中断」的运行情况。

接下来，就来简单的解析下 `/proc/softirqs` 文件的内容，在我服务器上查看到的文件内容如下：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161536069.jpeg" alt="img" style="zoom:50%;" />

每一个 CPU 都有自己对应的不同类型软中断的**累计运行次数**，有 3 点需要注意下。

第一点，要注意第一列的内容，它是代表着软中断的类型，在我的系统里，软中断包括了 10 个类型，分别对应不同的工作类型，比如 `NET_RX` 表示网络接收中断，`NET_TX` 表示网络发送中断、`TIMER` 表示定时中断、`RCU` 表示 RCU 锁中断、`SCHED` 表示内核调度中断。

第二点，要注意同一种类型的软中断在不同 CPU 的分布情况，正常情况下，同一种中断在不同 CPU 上的累计次数相差不多，比如我的系统里，`NET_RX` 在 CPU0 、CPU1、CPU2、CPU3 上的中断次数基本是同一个数量级，相差不多。

第三点，这些数值是系统运行以来的累计中断次数，数值的大小没什么参考意义，但是系统的**中断次数的变化速率**才是我们要关注的，我们可以使用 `watch -d cat /proc/softirqs` 命令查看中断次数的变化速率。

软中断内核线程

一般来说，名字在中括号里的都可以认为是内核线程。

如果在 top 命令发现，CPU 在软中断上的使用率比较高，而且 CPU 使用率最高的进程也是软中断 ksoftirqd 的时候，这种一般可以认为系统的开销被软中断占据了。

这时我们就可以分析是哪种软中断类型导致的，一般来说都是因为网络接收软中断导致的，如果是的话，可以用 sar 命令查看是哪个网卡的有大量的网络包接收，再用 tcpdump 抓网络包，做进一步分析该网络包的源头是不是非法地址，如果是就需要考虑防火墙增加规则，如果不是，则考虑硬件升级等。

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161538278.png" alt="ksoftirqd" style="zoom:50%;" />



# 计算机组成原理

袁春风的教材上有提到比较器的概念（408命题组雷神锤），cache在找到对应的组后，需要对组内的每一行逐一进行<tag, valid>的键值对匹配直到成功为止，比较器的作用是通过保存该行的tag部分加快每一行匹配的过程，所以每一个组内有多少行就有多少个比较器，置于为什么不用再乘上组数，我不懂比较器的构造，猜测应该是成本原因。

直接映射每个组只有一行，所以只有一个比较器；全映射只有一个组，那么有多少行就有多少个比较器；n路组相联，每一组有n行，所以是n个比较器。

## 数值表示

![IEEE_E6_A0_87_E5_87_86](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161541581.png)

- *符号位*：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；
- *指数位*：指定了小数点在数据中的位置，指数可以是负数，也可以是正数，**指数位的长度越长则数值的表达范围就越大**；
- *尾数位*：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且**尾数的长度决定了这个数的精度**，因此如果要表示精度更高的小数，则就要提高尾数位的长度；

用 `32` 位来表示的浮点数，则称为**单精度浮点数**，也就是我们编程语言中的 `float` 变量，而用 `64` 位来表示的浮点数，称为**双精度浮点数**，也就是 `double` 变量，它们的结构如下：

![float](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202408161604161.png)





# 设计模式

**单一职责原则**：每个类只负责一项职责。

**开闭原则**：对扩展开放，对修改关闭。

**里氏替换原则**：子类能替换父类，并且行为一致。

**依赖倒置原则**：依赖抽象，不依赖具体实现。

**接口隔离原则**：使用多个小接口，而不是一个大的总接口。

**迪米特法则**：尽量减少对象之间的了解和依赖。



## 面向对象

**SOLID原则**是面向对象设计中的五个基本原则，用于提高代码的可维护性和可扩展性。它们分别是：

1. **单一职责原则（SRP）**：
   - 每个类应只有一个原因引起变化，即每个类应该只有一个职责。这样可以降低类的复杂性，增加代码的可读性。

2. **开放/封闭原则（OCP）**：
   - 软件实体（类、模块、函数等）应对扩展开放，对修改封闭。也就是说，应该通过添加新代码来扩展功能，而不是修改现有代码。

3. **里氏替换原则（LSP）**：
   - 子类对象应能够替换父类对象而不影响程序的正确性。这意味着子类必须符合父类的行为预期。

4. **接口隔离原则（ISP）**：
   - 不应强迫类依赖于它不使用的接口。建议将大接口拆分为小接口，以便每个类只需要实现它所需要的接口。

5. **依赖倒置原则（DIP）**：
   - 高层模块不应依赖于低层模块，二者应依赖于抽象。抽象不应依赖于细节，细节应依赖于抽象。这样可以减少模块之间的耦合。



## 设计模式

是软件设计中的最佳实践，通常分为三大类：创建型、结构型和行为型。以下是各类设计模式的简要介绍：

### 创建型模式
1. **单例模式**：确保一个类只有一个实例，并提供全局访问点。
2. **工厂方法模式**：定义一个创建对象的接口，但由子类决定实例化哪个类。
3. **抽象工厂模式**：提供一个接口，用于创建相关或依赖对象的家族，而无需指定具体类。
4. **建造者模式**：使用多个简单的对象一步步构建一个复杂的对象。
5. **原型模式**：通过复制已有的实例来创建新的对象，而不是通过新建实例。

### 结构型模式
1. **适配器模式**：将一个接口转换成客户端所期望的另一种接口。
2. **桥接模式**：将抽象和实现分离，使二者可以独立变化。
3. **组合模式**：将对象组合成树形结构以表示部分-整体的层次结构。
4. **装饰者模式**：动态地给对象添加额外的职责。
5. **外观模式**：为一组接口提供一个统一的高层接口，方便使用。
6. **享元模式**：通过共享大量细粒度对象来减少内存消耗。
7. **代理模式**：为其他对象提供一个代理以控制对该对象的访问。

### 行为型模式
1. **责任链模式**：将请求的发送者和接收者解耦，将请求沿着处理链传递。
2. **命令模式**：将请求封装成对象，以便参数化客户端。
3. **观察者模式**：定义一种一对多的依赖关系，以便当对象状态改变时，所有依赖者都会收到通知。
4. **状态模式**：允许对象在内部状态改变时改变其行为。
5. **策略模式**：定义一系列算法，将它们封装起来，并使它们可以互换。
6. **模板方法模式**：定义一个算法的框架，而将一些步骤延迟到子类中实现。
7. **迭代器模式**：提供一种方法来顺序访问集合对象的元素，而不暴露其内部表示。

这些模式提供了解决特定问题的标准化方法，帮助提升代码的可读性和复用性。你想深入了解某个具体模式吗？



## 单例模式

### 应用背景

单例模式是一类最经典最简单的设计模式. 在单例模式下，我们声明一个类并保证这个类只存在全局唯一的实例供外部反复使用.

单例模式的适用场景包括：

- • 一些只允许存在一个实例的类，比如全局统一的监控统计模块
- • 一些实例化时很耗费资源的类，比如协程池、连接池、和第三方交互的客户端等
- • 一些入参繁杂的系统模块组件，比如 controller、service、dao 等

分为饿汉式和懒汉式两种类型：

- • 饿汉式：从一开始就完成单例的初始化工作，以备不时之需（肚子饿了，先干为敬.）
- • 懒汉式：贯彻佛系思想，不到逼不得已（需要被使用了），不执行单例的初始化工作

### 饿汉式实现流程

饿汉式和懒汉式中的“饿”和“懒”体现在单例初始化时机的不同. “饿” 指的是，对于单例对象而言，不论其后续有没有被使用到以及何时才会被使用到，都会在程序启动之初完成其初始化工作.

在实现上，可以将饿汉式单例模式的执行步骤拆解如下：

- • 单例类和构造方法声明为不可导出类型，避免被外部直接获取到（避免让外界拥有直接初始化的能力，导致单例模式被破坏）
- • 在代码启动之初，就初始化好一个全局单一的实例，作为后续所谓的“单例”
- • 暴露一个可导出的单例获取方法 GetXXX()，用于返回这个单例对象

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409242108925.webp" alt="640" style="zoom:50%;" />

下面给出实现饿汉式单例模式的代码示例：

- • singleton 是需要被单例模式保护的类型
- • singleton 首字母小写，是不可导出的类型，避免被外界直接获取
- • 在包初始化函数 init 中完成了 singleton 单例的初始化工作
- • 对外暴露可导出方法 GetInstance，返回提前初始化好的全局单例对象 s

#### 代码规范性讨论

上述代码在实现上没有逻辑问题，但是存在一个比较容易引起争议的规范性问题，就是在对外可导出的 GetInstance 方法中，返回了不可导出的类型 singleton.

代码执行流程上 ok，但这种实现方式存在代码坏味道，相应的问题在 stackoverflow 上引起过讨论，对应链接如下，大家感兴趣可以去了解原贴中的讨论内容：

https://stackoverflow.com/questions/21470398/return-an-unexported-type-from-a-function

 

不建议这么做的原因主要在于：

- • singleton 是包内的不可导出类型，在包外即便获取到了，也无法直接作为方法的入参或者出参进行传递，显得很呆
- • singleton 的对外暴露，使得 singleton 所在 package 的代码设计看起来是自相矛盾的，混淆了 singleton 这个不可导出类型的边界和定位

综上，规范的处理方式是，在不可导出单例类 singleton 的基础上包括一层接口 interface，将其作为对对导出方法 GetInstance 的返回参数类型:

```go
type Instance interface {
    Work()
}

func GetInstance() Instance {
    return s
}
```

### 懒汉式实现流程

第2章聊完了饿汉式单例. 饥饿使人进取，饿汉是相对比较勤奋的，提前做了充足的准备工作，保证了单例对象的正常供应.

下面我们聊聊与饿汉针锋相对另一种实现模式——懒汉式的设计思路.

懒汉式讲究的是”佛系”，某件事情如果是可做可不做，那我一定选择不做. 直到万不得已非做不可的时候，我才会采取行动（deadline 是第一生产力）.

懒汉式的执行步骤如下：

- • 单例类声明为不可导出类型，避免被外界直接获取到
- • 声明一个全局单例变量, 但不进行初始化（注意只声明，不初始化）
- • 暴露一个对外公开的方法,用于获取这个单例
- • 在这个获取方法被调用时，会判断单例是否初始化过，倘若没有，则在此时才完成初始化工作

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409242110997.webp" alt="640 (1)" style="zoom: 33%;" />

我们将流程升级为加锁 double check 模式：

- • 在加锁前，先检查一轮单例的初始化状态，倘若已初始化过，则直接返回，以做到最大限度的无锁操作
- • 倘若通过第一轮检查，则进行加锁，保证并发安全性
- • 加锁成功后，需要执行第二轮检查，确保在此时单例仍未初始化过的前提下，才执行初始化工作

此处得以解决懒汉3.0中并发问题的核心在于，加锁之后多了一次 double check 动作，由于这轮检查工作是在加锁之后执行的，因此能够保证 singleton 的初始化状态是稳定不变的，并发问题彻底得以解决.



### 两种模式对比

饿汉式与懒汉式没有绝对的优劣之分，需要权衡看待：

- • 饿汉式在程序运行之初就完成单例的初始化，说白了，不够智能，不够极限，不够”懒“. 说不定这个单例对象迟迟不被使用到，甚至永远都不被使用，那么这次初始化动作可能只是一次无谓的性能损耗
- • 懒汉式在单例被首次使用时才执行初始化，看起来显得”聪明“一些. 但是，我们需要意识到，倘若初始化工作中存在异常问题（如 panic，fatal），则这枚定时炸弹会在程序运行过程才暴露出来，这对于我们的运行项目而言会带来更严重的伤害. 相比之下，倘若使用的是饿汉模式，则这种实例化的问题会在代码编译运行之初就提前暴露，更有利于问题的定位和解决



### sync.Once 实现原理

#### 4.1 数据结构

sync.Once 是 Golang 提供的用于支持实现单例模式的标准库工具，其对应的数据结构如下：

在 sync.Once 的定义类中 包含了两个核心字段：

- • done：一个整型 uint32，用于标识用户传入的任务函数是否已经执行过了
- • m：一把互斥锁 sync.Mutex，用于保护标识值 done ，避免因并发问题导致数据不一致

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409242111339.webp" alt="640 (2)" style="zoom:50%;" />

#### Once.Do

sync.Once 本质上也是通过加锁 double check 机制，实现了任务的全局单次执行，实现的方法链路和具体源码展示如下：

<img src="https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202409242112414.webp" alt="640 (3)" style="zoom:50%;" />

```
func (o *Once) Do(f func()) {
    // 锁外的第一次 check，读取 Once.done 的值
    if atomic.LoadUint32(&o.done) == 0 {
        o.doSlow(f)
    }
}

func (o *Once) doSlow(f func()) {
    // 加锁
    o.m.Lock()
    defer o.m.Unlock()
    // double check
    if o.done == 0 {
        // 任务执行完成后，将 Once.done 标识为 1
        defer atomic.StoreUint32(&o.done, 1)
        // 保证全局唯一一次执行用户注入的任务
        f()
    }
}
```

单例工具 sync.Once 的使用方式非常简单. 用户调用 sync.Once.Do 方法，并在方法入参传入一个需要保证全局只执行一次的闭包任务函数 f func() 即可.

sync.Once.Do 方法的实现步骤如下：

- • first check：第一次检查 Once.done 的值是否为 0，这步是无锁化的
- • easy return：倘若 Once.done 的值为 0，说明任务已经执行过，直接返回
- • lock：加锁
- • double check：再次检查 Once.done 的值是否为 0
- • execute func：倘若通过 double check，真正执行用户传入的执行函数 f
- • update：执行完 f 后，将 Once.done 的值设为 1
- • return：解锁并返回



## 工厂模式

### 简单工厂模式

- 简单工厂模式不是一个正式的设计模式，但它是工厂模式的基础。它使用一个单独的工厂类来创建不同的对象，根据传入的参数决定创建哪种类型的对象。

工厂模式包含以下几个主要角色：

- 抽象产品（Abstract Product）：定义了产品的共同接口或抽象类。它可以是具体产品类的父类或接口，规定了产品对象的共同方法。
- 具体产品（Concrete Product）：实现了抽象产品接口，定义了具体产品的特定行为和属性。
- 抽象工厂（Abstract Factory）：声明了创建产品的抽象方法，可以是接口或抽象类。它可以有多个方法用于创建不同类型的产品。
- 具体工厂（Concrete Factory）：实现了抽象工厂接口，负责实际创建具体产品的对象。



### 工厂方法模式

- 工厂方法模式定义了一个创建对象的接口，但由子类决定实例化哪个类。工厂方法将对象的创建延迟到子类。

### 抽象工厂模式

- 抽象工厂模式提供一个创建一系列相关或互相依赖对象的接口，而无需指定它们具体的类。



- 抽象工厂（Abstract Factory）：声明了一组用于创建产品对象的方法，每个方法对应一种产品类型。抽象工厂可以是接口或抽象类。
- 具体工厂（Concrete Factory）：实现了抽象工厂接口，负责创建具体产品对象的实例。
- 抽象产品（Abstract Product）：定义了一组产品对象的共同接口或抽象类，描述了产品对象的公共方法。
- 具体产品（Concrete Product）：实现了抽象产品接口，定义了具体产品的特定行为和属性。



## 观察者模式

- • 观察者模式适用于多对一的订阅/发布场景，其实现思路是在观察者与被观察对象之间添加收口了发布订阅功能的中间层，核心宗旨是实现“观察者”与“被观察对象”之间的解耦
- • 通过 UML 类图结合具体代码示例，对观察者模式进行实践. 根据变更事件的通知模式，观察者模式可以分为同步和异步两种模型
- • 本文给出两个践行了观察者模式的工程案例，一个是 Message Queue 的发布订阅模式，一个是 ETCD 服务端对 watch 功能的实现思路

## 装饰器模式

装饰器模式和继承模式做一个对比总结：

- • 继承强调的是等级制度和子类种类，这部分架构需要一开始就明确好
- • 装饰器模式强调的是“装饰”的过程，而不强调输入与输出，能够动态地为对象增加某种特定的附属能力，相比于继承模式显得更加灵活，且符合开闭原则

## 适配器模式

适配器模式的作用是能够实现两个不兼容或弱兼容接口之间的适配桥接作用，该设计模式中会涉及到如下几个核心角色：

- • 目标 target：是一类含有指定功能的接口
- • 使用方 client：需要使用 target 的用户
- • 被适配的类 adaptee：和目标类 target 功能类似，但不完全吻合
- • 适配器类：adapter：能够将 adaptee 适配转换成 target 的功能类

下面举个直观点的例子来说，我们作为用户（client）现在手中持有一个两孔的插头，需要匹配的目标是一个两孔的插座（target），但是现状是我们只找到了三孔的插座（adaptee），于是我们通过在三孔插座上插上一个实现三孔转两孔的适配器（adapter），最终实现了两孔插头与三孔插座之间的适配使用.

 

# 专有名词

## 哈希

hash 译作散列，是一种将任意长度的输入压缩到某一固定长度的输出摘要的过程，由于这种转换属于压缩映射，输入空间远大于输出空间，因此不同输入可能会映射成相同的输出结果. 此外，hash在压缩过程中会存在部分信息的遗失，因此这种映射关系具有不可逆的特质.

（1）hash 的可重入性：相同的 key，必然产生相同的 hash 值；

（2）hash 的离散性：只要两个 key 不相同，不论其相似度的高低，产生的 hash 值会在整个输出域内均匀地离散化；

（3）hash 的单向性：企图通过 hash 值反向映射回 key 是无迹可寻的.

（4）hash 冲突：由于输入域（key）无穷大，输出域（hash 值）有限，因此必然存在不同 key 映射到相同 hash 值的情况，称之为 hash 冲突.



### 哈希的基本概念

1. **哈希函数**:
   - 哈希函数（Hash Function）是一个函数，它接受输入（通常是任意长度的字符串、数字或其他数据类型），并输出一个固定长度的值。哈希函数的输出通常是一个整数或二进制值。
   - 理想的哈希函数会将不同的输入映射到不同的哈希值上，尽可能减少冲突（即不同的输入映射到相同的哈希值）。
   - 哈希函数的常见用法包括数据检索、数据验证、加密等。

2. **哈希值**:
   - 哈希值是通过哈希函数从输入数据生成的输出。这个值通常用于快速查找、数据比较或验证。
   - 哈希值的长度通常是固定的，例如 32 位、64 位等。

3. **哈希表**:
   - 哈希表（Hash Table）是一种基于哈希函数的数据结构，它使用键-值对（key-value pairs）来存储数据。通过哈希函数，键被映射到表中的一个位置（或桶），从而可以快速找到对应的值。
   - 哈希表的查找、插入和删除操作通常具有 O(1) 的时间复杂度，这使得它在很多应用中非常高效。

### 哈希的应用场景

1. **哈希表**:
   - 哈希表是一种非常高效的查找数据结构，用于实现字典、映射等数据类型。它允许快速的插入、删除和查找操作。
   - 例如，在编程语言中，`map`（Go）、`dict`（Python）、`HashMap`（Java）等都是哈希表的实现。

2. **数据校验**:
   - 哈希函数用于数据完整性校验，如文件校验和（Checksum）、消息摘要（Message Digest）。常见的算法包括 MD5、SHA-1、SHA-256 等。
   - 这些哈希值可以帮助检测数据在传输或存储过程中是否被篡改。

3. **密码学**:
   - 在密码学中，哈希函数用于生成不可逆的哈希值，常用于密码存储、数字签名等场景。
   - 加密哈希函数，如 SHA-256、SHA-3，能够产生难以逆向工程的哈希值，确保数据的安全性。

4. **负载均衡**:
   - 哈希函数可以用于分布式系统中的负载均衡，通过将请求哈希为一个值，再根据该值分配到不同的服务器上，达到均衡负载的效果。

5. **数据分片**:
   - 在分布式数据库中，哈希函数可以用来将数据分布到不同的分片（shards）或节点上，确保数据的均匀分布。
   - **一致性哈希**

### 选择哈希函数的注意事项

1. **均匀性**:
   - 一个好的哈希函数应当能将输入均匀分布到哈希空间中，避免“堆积效应”，即大量输入映射到相同的哈希值。

2. **低冲突率**:
   - 理想的哈希函数应当尽可能减少冲突。尽管冲突不可避免，但好的设计可以最大限度地减少冲突的发生。

3. **计算效率**:
   - 哈希函数应当足够快，特别是在处理大量数据或实时应用中，计算效率非常重要。

4. **不可逆性（对于密码学）**:
   - 在安全相关的应用中，哈希函数应当是不可逆的，意味着从哈希值不能轻易恢复原始数据。

### 代码示例

下面是一个简单的哈希函数示例，它将字符串哈希为一个整数：

```go
package main

import (
    "fmt"
)

func simpleHash(s string) int {
    hash := 0
    for i := 0; i < len(s); i++ {
        hash = int(s[i]) + (hash << 6) + (hash << 16) - hash
    }
    return hash
}

func main() {
    data := "example"
    fmt.Printf("Hash value for %s: %d\n", data, simpleHash(data))
}
```

在这个例子中，`simpleHash` 函数将输入字符串 `example` 转换为一个整数哈希值。哈希值可用于快速查找、数据校验等操作。



### 哈希冲突

**哈希冲突**（Hash Collision）是指在使用哈希函数时，不同的输入数据被映射到相同的哈希值或哈希桶（bucket）的现象。这种情况在哈希表或哈希函数相关的应用中经常出现，因为哈希函数的输出范围通常是有限的，而输入数据的可能性范围往往是无限或非常大的。

### 哈希冲突的原因
哈希冲突的主要原因在于哈希函数的输出空间有限，不能保证每个不同的输入都映射到不同的哈希值。例如，如果你有一个哈希函数将任意字符串映射到一个 32 位整数，那么最多只能有 \(2^{32}\) 个不同的哈希值。而在实际应用中，可能存在远多于这个数量的输入，因此哈希冲突是不可避免的。

### 处理哈希冲突的常见方法
处理哈希冲突的方法主要有以下几种：

1. **链地址法（Chaining）**:
   - **概念**: 每个桶内存储一个链表（或其他集合结构），所有映射到同一个哈希值的数据项都放入该链表中。
   - **优点**: 简单易实现，动态调整性好，能很好地应对哈希冲突频繁的情况。
   - **缺点**: 当大量数据映射到同一个桶时，链表会变长，导致查找效率下降。

   **示例**:
   ```go
   type Node struct {
       key   int
       value string
       next  *Node
   }
   
   type HashMap struct {
       buckets []*Node
       size    int
   }
   
   func NewHashMap(size int) *HashMap {
       return &HashMap{
           buckets: make([]*Node, size),
           size:    size,
       }
   }
   
   func (hm *HashMap) Put(key int, value string) {
       index := key % hm.size
       node := hm.buckets[index]
       if node == nil {
           hm.buckets[index] = &Node{key: key, value: value}
       } else {
           for node != nil {
               if node.key == key {
                   node.value = value
                   return
               }
               if node.next == nil {
                   node.next = &Node{key: key, value: value}
                   return
               }
               node = node.next
           }
       }
   }
   
   func (hm *HashMap) Get(key int) string {
       index := key % hm.size
       node := hm.buckets[index]
       for node != nil {
           if node.key == key {
               return node.value
           }
           node = node.next
       }
       return ""
   }
   ```

2. **开放寻址法（Open Addressing）**:
   - **概念**: 当发生哈希冲突时，通过探测（线性探测、二次探测或双重散列）找到下一个空闲的桶，并将数据存储在该位置。
   - **优点**: 不需要额外的存储空间（如链表），所有数据都存储在哈希表中。
   - **缺点**: 随着哈希表变满，冲突处理的效率会下降，查找和插入操作可能变慢。

   **探测方法**:
   - **线性探测**: 当发生冲突时，顺序查找下一个可用桶。
   - **二次探测**: 在每次冲突时，以二次方步长查找下一个可用桶，避免“主聚集”问题。
   - **双重散列**: 使用第二个哈希函数决定探测的步长。

3. **再哈希法（Rehashing）**:
   - **概念**: 当发生冲突时，使用另一个哈希函数生成新的哈希值，并尝试将数据放入新的位置。
   - **优点**: 再哈希可以有效减少冲突。
   - **缺点**: 再哈希的计算成本较高，可能会影响性能。

4. **扩展哈希表**:
   - **概念**: 动态调整哈希表的大小（扩展容量），重新计算所有已有数据的哈希值，分配到新的位置。
   - **优点**: 能有效减少哈希冲突。
   - **缺点**: 重新分配哈希表时需要进行大量计算，导致性能开销。

哈希冲突的影响

- **查找效率下降**: 哈希冲突会导致查找、插入和删除操作的效率下降，特别是在冲突频繁的情况下。
- **内存利用率**: 为处理哈希冲突，需要额外的内存（如链表），影响内存利用率。
- **复杂性增加**: 哈希冲突的处理增加了系统的复杂性，特别是在实现更复杂的冲突处理策略时。







## 编码

当然可以！这里是一些常见的编码种类和它们的简要介绍：

### 1. **字符编码**
字符编码用于在计算机中表示文本字符。常见的字符编码包括：
- **ASCII**：美国标准信息交换码，用于表示128个字符，包括字母、数字和控制字符。
- **UTF-8**：一种变长字符编码，可以表示所有的Unicode字符，兼容ASCII。
- **UTF-16**：另一种Unicode字符编码，使用16位或32位表示字符。
- **UTF-32**：固定长度的Unicode字符编码，每个字符使用32位。
- **GB2312/GBK/GB18030**：中国国家标准的字符集，分别扩展了不同数量的汉字字符。

### 2. **图像编码**
图像编码用于在计算机中存储和传输图像。常见的图像编码格式包括：
- **JPEG**：有损压缩图像格式，适用于照片和复杂图像。
- **PNG**：无损压缩图像格式，支持透明背景。
- **GIF**：支持动画的无损压缩图像格式，适用于简单动画。
- **BMP**：未压缩的图像格式，文件体积较大。
- **TIFF**：用于高质量图像存储，无损压缩。

### 3. **音频编码**
音频编码用于存储和传输音频数据。常见的音频编码格式包括：
- **MP3**：有损压缩音频格式，适用于音乐和音频流媒体。
- **AAC**：有损压缩音频格式，比MP3压缩率更高，质量更好。
- **FLAC**：无损压缩音频格式，保留原始音质。
- **WAV**：未压缩音频格式，文件体积较大。
- **OGG**：开源有损压缩音频格式，适用于各种音频应用。

### 4. **视频编码**
视频编码用于存储和传输视频数据。常见的视频编码格式包括：
- **H.264/AVC**：广泛使用的视频编码标准，适用于高清和全高清视频。
- **H.265/HEVC**：下一代视频编码标准，比H.264具有更高的压缩率。
- **VP8/VP9**：Google开发的开源视频编码格式，适用于网络视频。
- **AV1**：开源视频编码格式，旨在替代VP9和H.265。

### 5. **数据编码**
数据编码用于将数据表示为适合传输和存储的格式。常见的数据编码包括：
- **Base64**：用于将二进制数据编码为文本，以便在文本环境中传输。
- **URL编码**：用于将URL中不可用的字符转换为安全的字符。
- **JSON**：一种轻量级的数据交换格式，易于人类阅读和编写。
- **XML**：一种可扩展标记语言，用于表示复杂的数据结构。

### 6. **错误检测和纠错编码**
这些编码用于检测和纠正数据传输中的错误：
- **奇偶校验**：一种简单的错误检测方法，通过添加一个奇偶校验位来检测单个比特错误。
- **CRC（循环冗余校验）**：一种更复杂的错误检测方法，用于检测传输中的多位错误。
- **Hamming编码**：一种错误纠正码，可以纠正单个比特错误并检测双比特错误。
- **Reed-Solomon编码**：一种强大的错误纠正码，广泛用于光盘、磁盘和数据传输中。

每种编码都有其特定的应用场景和优势。如果你对某种特定的编码有更深入的兴趣，欢迎告诉我，我可以提供更详细的信息。

## 哈夫曼编码

![image-20250319103027412](https://raw.githubusercontent.com/lanyoumeng/Drawing-bed/main/docs/202503191030894.png)

## 一些

- 权威DNS：保存了相应域名的权威信息。权威DNS即通俗上“这个域名我说了算”的服务器
- LocalDNS：缓存+递归查询，运营商（集团网）部署的本地DNS服务器，直接接受网内客户端请求
- 根DNS服务器：全球有13台，LocalDNS未命中缓存查询的起点服务器，其公网地址具体可参考https://www.iana.org/domains/root/servers
- DNS Update：DNS主服务器master接受外部的变更指令
- DNS Notify：DNS主服务器master接受变更命令后，会自增自身的serial号，同时将变更的serial号告知从服务器slave
- DNS IXFR：DNS从服务器slave以增量的形式向master要求获取本次变更的内容
- DNS AXFR：DNS从服务器slave以全量的形式向master要求获取当前的全量数据
- 对称加密：使用相同的秘钥来加密传输内容，一端加密后，对端收到数据会用相同的秘钥来解密
- 非对称加密：如果用公钥对数据进行加密，只有用对应的私钥才能解密；如果用私钥对数据进行加密，那么只有用对应的公钥才能解密。
- 静态加速：针对视频、图片等不变的内容，将其缓存在靠近用户的边缘节点，缓存预热后用户直接从边缘获取，从而加速访问速度；
- 动态加速DCDN：针对API类返回值不同的请求，通过特殊的网络优化方式（路由优化、传输优化）等技术加速其达到源站的速度。
- VIP：虚拟IP，一般作为四层反向代理的入口，client看起来一直在与VIP交互
- RS：Real Server，VIP后实际承受client请求的服务，可能是物理机/虚拟机/容器POD
- DPDK：Data Plane Development Kit，主要用户4层负载均衡，用于转发的网络加速领域比较多；以极大提高网卡报文的处理性能和吞吐量，提高数据平面应用程序的工作效率
- SSL/TLS：(Secure Sockets Layer 安全套接字协议),及其继任者传输层安全（Transport Layer Security，TLS）是为网络通信提供安全及数据完整性的一种安全协议
- DPDK：Data Plane Development Kit，一种从数据面去加速网络报文处理的工具，可以极大提高数据处理性能和吞吐量，提高数据平面应用程序的工作效率



## 同步和并行，并发

"同步"、"并行"和"并发"是计算机科学领域中的重要概念，它们涉及到多任务处理和多线程编程的不同方面。以下是它们的定义和区别：

1. 同步 (Synchronization)： 同步是一种控制多线程或多进程之间的执行顺序的机制，以确保它们在某些点上彼此协调和按照特定的顺序执行。同步的目的是避免竞态条件（Race Condition）和确保数据的一致性。常见的同步工具包括锁、互斥体、信号量和条件变量。
2. 并行 (Parallelism)： 并行是指同时执行多个任务，通常在多个处理器或核心上。这意味着不同的任务在同一时刻执行，以提高性能。并行计算通常用于处理计算密集型任务，例如图像处理、科学模拟和数据分析。
3. 并发 (Concurrency)： 并发是指同时处理多个任务，但不一定是在同一时刻。在并发模型中，多个任务可以在不同的时间点交替执行，每个任务可能会以小步骤执行，以便在多任务之间进行切换。并发通常用于处理 I/O 密集型任务，如网络通信、文件操作和用户界面响应。

区别：

- 同步是一种控制多线程之间执行顺序的机制，目的是确保数据一致性。并行和并发是多任务处理的两种不同方式。
- 并行是同时执行多个任务，通常需要多个处理器或核心。并发是同时处理多个任务，但不一定需要多个处理器，任务之间可以交替执行。
- 并行通常用于处理计算密集型任务，而并发通常用于处理 I/O 密集型任务。
- 同步通常用于处理多线程或多进程之间的竞态条件，以确保线程安全。并行和并发可以与同步结合使用，以确保多任务之间的正确协调和数据一致性。

总之，同步、并行和并发是多任务处理的关键概念，它们在不同的上下文中具有不同的应用。理解它们之间的区别和如何有效地使用它们对于编写高效、稳定的并发程序非常重要。

## Curl

一个命令行工具和库，用于在各种操作系统上进行网络数据传输。它支持多种协议，包括HTTP、HTTPS、FTP、FTP上传、SCP、SFTP、LDAP等，可以用于从终端或脚本中发送和接收数据。

Curl的全称是"Client URL"，意味着它可以充当一个客户端，通过URL来与服务器进行通信，发送请求并获取响应。它是一个开源工具，被广泛用于命令行环境下进行网络调试、API测试、下载文件、上传文件等任务。

在命令行中使用Curl时，可以通过简单的命令来完成各种网络操作，例如发送GET或POST请求、设置请求头、传递数据等。下面是一个使用Curl发送GET请求的示例：

```Plaintext
curl https://api.example.com/data
```

这个命令将会向"https://api.example.com/data"发送一个GET请求，并输出服务器返回的数据。

```Shell
curl -XPOST -H"Content-Type: application/json" -d'{"username":"root","password":"miniblog1234","nickname":"root","email":"nosbelm@qq.com","phone":"1818888xxxx"}' http://127.0.0.1:8080/v1/users*
```

## 拷贝

在编程语言中，拷贝通常分为浅拷贝和深拷贝两种方式。

浅拷贝（Shallow Copy）创建一个新对象，新对象与原始对象共享相同的内存地址。这意味着新对象和原始对象之间存在一种浅层连接，其中的引用类型字段指向相同的对象。如果修改新对象的引用类型字段，则原始对象也会受到影响。浅拷贝只复制对象的表面结构，不会递归复制对象的子对象。

深拷贝（Deep Copy）创建一个全新的对象，该对象与原始对象完全独立，彼此之间没有任何连接。深拷贝递归地复制原始对象及其所有子对象，包括引用类型字段指向的对象。这意味着无论对原始对象还是对深拷贝对象进行修改，彼此之间都不会产生任何影响。

拷贝的选择取决于具体的需求和数据结构。浅拷贝可以提高性能并减少内存占用，但在涉及修改对象的情况下可能会导致意外的结果。深拷贝则更安全，但可能会消耗更多的资源和时间。

## AJAX

AJAX（Asynchronous JavaScript and XML）是一种用于创建交互性网页应用程序的技术，它允许在不刷新整个网页的情况下，通过异步请求从服务器获取数据。尽管 AJAX 包含 "XML" 这个名称，但实际上它不仅限于 XML 数据，而可以与多种数据格式一起使用，如 JSON、HTML、文本等。

以下是 AJAX 的主要组成部分和工作原理：

1. **XMLHttpRequest 对象**：
   1. AJAX 的核心是 `XMLHttpRequest` 对象，它允许 JavaScript 发送 HTTP 请求到服务器并接收响应。
   2. 最新的浏览器也支持 `Fetch API`，它提供了更简洁和现代的方式来进行网络请求。
2. **事件驱动**：
   1. AJAX 请求是异步的，这意味着它们不会阻塞网页的其他操作。当请求完成时，将触发相应的事件处理程序，允许对响应进行处理。
3. **数据格式**：
   1. 服务器通常会以 JSON 或 XML 格式返回数据，但也可以是其他格式。
   2. JavaScript 可以解析这些数据并将其用于更新网页内容。
4. **DOM 操作**：
   1. 一旦获取到响应数据，JavaScript 可以使用它来更新网页的 DOM（文档对象模型），从而实现动态内容的显示。
5. **异步通信**：
   1. AJAX 请求是异步的，因此它们可以在后台进行，而不会阻止用户与网页交互。
6. **跨域请求**：
   1. AJAX 具有同源策略（Same-Origin Policy）限制，这意味着它不能直接从不同源（不同域、协议或端口）获取数据。为了进行跨域请求，通常需要在服务器端设置跨域资源共享（CORS）规则。

AJAX 的典型用途包括：

- 动态加载页面内容，以减少页面刷新。
- 异步验证用户输入，如用户名的可用性。
- 实时更新数据，如聊天应用程序中的新消息。
- 异步加载远程数据，如从服务器获取数据并显示在网页上。

在现代 Web 开发中，通常使用更高级的工具和框架，如 Axios、jQuery AJAX、Fetch API 或一些 JavaScript 框架（如 Angular、React、Vue.js）来简化和增强 AJAX 请求的处理。这些工具和框架提供了更友好的 API，并处理了许多底层细节，使 AJAX 开发更容易。

AJAX请求的过程如下：

1. 创建XMLHttpRequest对象或使用浏览器提供的Fetch API：在JavaScript中，使用XMLHttpRequest对象（XHR）或Fetch API来创建一个HTTP请求。
2. 配置请求：设置HTTP请求的方法（GET、POST、PUT、DELETE等）、URL和需要发送的数据（如果有的话）。
3. 发送请求：使用`xhr.send()`方法（或Fetch API中的`fetch()`方法）发送请求到服务器。
4. 处理响应：一旦服务器处理请求并返回响应，JavaScript会在回调函数中处理响应数据。
5. 更新网页内容：根据服务器返回的数据，JavaScript可以动态地更新网页的内容，例如更新表格、列表、图表等。

AJAX请求允许网页与服务器进行无刷新的交互，从而提供更好的用户体验和性能。它广泛应用于Web应用程序、单页面应用（SPA）、数据加载、表单提交等场景。

以下是使用XMLHttpRequest对象发送AJAX请求的简单示例：

```JavaScript
// 创建XMLHttpRequest对象
var xhr = new XMLHttpRequest();

// 配置请求
xhr.open('GET', 'https://api.example.com/data', true);

// 设置回调函数处理响应
xhr.onreadystatechange = function() {
  if (xhr.readyState === XMLHttpRequest.DONE && xhr.status === 200) {
    // 处理服务器返回的数据
    var data = JSON.parse(xhr.responseText);
    console.log(data);
  }
};

// 发送请求
xhr.send();
```

在上面的示例中，我们创建了一个XMLHttpRequest对象，设置了GET请求的URL，然后定义了回调函数来处理服务器的响应。一旦响应数据返回，我们将通过JSON.parse()方法解析JSON数据，并在控制台上输出。

## DOM

（Document Object Model）是一种表示HTML或XML文档结构的编程接口。它将文档中的每个元素、属性、文本和其他内容都视为对象，使得开发者可以使用编程语言（如JavaScript）来操作和访问文档的内容。

DOM 提供了一种对文档的结构化表示，使得开发者可以通过编程的方式动态地访问、修改和操作文档的内容，而无需直接操作原始的HTML或XML文本。DOM 在网页开发中非常常用，特别是在与JavaScript配合使用时，用于实现动态和交互式的网页效果。

在 DOM 中，每个HTML或XML元素都被视为一个节点（Node），节点之间形成一种层次结构，每个节点都可以有子节点和父节点。节点分为以下几种类型：

1. 元素节点（Element Node）：表示HTML或XML中的标签元素，如`<div>`、`<p>`等。
2. 文本节点（Text Node）：表示元素节点中的文本内容。
3. 属性节点（Attribute Node）：表示元素节点的属性，如`id`、`class`等。
4. 注释节点（Comment Node）：表示HTML或XML中的注释，如`<!-- 注释内容 -->`。

通过DOM，开发者可以使用JavaScript或其他支持DOM的编程语言来访问和操作这些节点。例如，可以使用DOM方法和属性来获取元素的文本内容、添加新元素、修改属性、处理事件等。

以下是一个简单的示例，演示如何使用DOM来获取元素并修改其内容：

```HTML
<!DOCTYPE html>
<html>
<head>
  <title>DOM示例</title>
</head>
<body>
  <div id="myDiv">Hello, DOM!</div>

  <script>
    // 使用DOM获取元素并修改其内容
    var myDiv = document.getElementById('myDiv');
    myDiv.innerHTML = 'Hello, Updated DOM!';
  </script>
</body>
</html>
```

在上面的示例中，通过`document.getElementById('myDiv')`方法获取了id为`myDiv`的元素，并使用`innerHTML`属性修改了其内容。这是一个简单的DOM操作示例，实际中可以进行更复杂的DOM操作来实现交互式的网页效果。

## CORS

（Cross-Origin Resource Sharing）是一种安全策略，用于在Web浏览器中限制跨域请求的访问权限。跨域请求是指在一个域名下的Web页面请求另一个域名下的资源，例如通过JavaScript发起的Ajax请求。

同源策略（Same-Origin Policy）是浏览器的一个重要安全功能，它限制了网页中的JavaScript代码只能访问同源（同协议、同域名、同端口）下的资源。这样可以防止恶意网站获取用户敏感信息或在用户无意间执行恶意代码。

而CORS是一种例外，它允许服务器在响应中包含一个特殊的HTTP头部，告诉浏览器哪些跨域请求是被允许的。如果服务器没有设置CORS头部，浏览器会拒绝跨域请求，阻止客户端JavaScript获取或发送跨域数据。

CORS头部包括以下几个关键字段：

- `Access-Control-Allow-Origin`: 指定允许访问该资源的外部域名。可以设置为具体的域名，如`https://example.com`，或为通配符`*`，表示允许所有域名访问。
- `Access-Control-Allow-Methods`: 指定允许的HTTP方法（如GET、POST等）。
- `Access-Control-Allow-Headers`: 指定允许的自定义请求头。
- `Access-Control-Allow-Credentials`: 布尔值，指示是否允许发送身份凭证（例如Cookie）。
- `Access-Control-Expose-Headers`: 指定浏览器可以访问的响应头。

当浏览器发现AJAX请求跨域时，会在发送真正的请求之前，先发送一个预检请求（Preflight Request）来检查服务器是否允许跨域请求。预检请求使用HTTP OPTIONS方法，包含一些CORS相关的头部，服务器接收并处理这个请求后，告知浏览器是否允许跨域访问。

CORS是一种重要的Web安全机制，可以确保跨域请求的安全性，并且在现代Web应用中被广泛使用。开发者需要在服务器端设置CORS头部，以允许其他域名访问自己的资源，同时遵守CORS的安全策略，确保不会因为CORS问题导致安全漏洞。

## Base64编码

是一种常用的编码方式，用于将二进制数据转换成可打印的ASCII字符。它的主要作用是在不可靠的网络环境中安全地传输二进制数据，例如在电子邮件、URL和HTTP请求中传递二进制数据。

Base64编码使用64个可打印字符（包括字母、数字和几个特殊符号）来表示二进制数据。编码后的数据长度通常会比原始二进制数据稍微增加，因为每3个字节的二进制数据会编码为4个字符。

Base64编码的过程如下：

1. 将二进制数据按照每3个字节（24位）一组进行划分。
2. 对每组3个字节进行处理，将其拆分为4个6位的小组。
3. 将每个6位的小组转换成Base64字符对应的索引，得到4个Base64字符。
4. 如果原始数据长度不是3的倍数，会进行填充操作。

以下是一个简单的例子，将"Hello"这个字符串进行Base64编码的过程：

1. 将"Hello"转换为二进制数据：01001000 01100101 01101100 01101100 01101111
2. 将二进制数据按照3个字节一组进行划分：010010 000110 010101 101100 110000 110110 110110 011011 011011 11
3. 对每组3个字节进行处理，拆分为4个6位的小组：010010 000110 010101 101100 110000 110110 110110 011011 011011 11
4. 将每个6位的小组转换成Base64字符：S G V s Y m x k Y 2 w 3 b w ==
5. 最终Base64编码结果为："SGVsbG8="

Base64编码是可逆的，也就是说可以通过Base64解码将编码后的数据还原成原始的二进制数据。在编程中，许多编程语言和库都提供了Base64编码和解码的函数或方法，方便开发者进行数据转换。



## argv和argc

`argv` 是一个在C和C++编程语言中常用的标准参数，通常用于传递命令行参数给程序。`argv` 的全名是 "argument vector"（参数向量），它通常与 `argc`（argument count，参数计数）一起使用。这两者一起构成了程序的命令行参数传递机制。

具体来说：

1. `argc` 是一个整数，表示命令行参数的数量，包括程序名称本身。通常，`argc` 的值至少为 1。
2. `argv` 是一个指向字符指针数组（数组的元素是字符串指针）的指针，其中每个指针指向一个命令行参数的字符串。`argv[0]` 通常指向程序的名称，而 `argv[1]`、`argv[2]`、... 依次指向传递给程序的参数字符串。

举个例子，如果你运行一个程序 `myprogram`，并在命令行中输入以下内容：

```Plaintext
myprogram arg1 arg2 arg3
```

那么 `argc` 的值将是 4，`argv` 数组将包含以下内容：

- `argv[0]` 指向字符串 "myprogram"
- `argv[1]` 指向字符串 "arg1"
- `argv[2]` 指向字符串 "arg2"
- `argv[3]` 指向字符串 "arg3"

在程序中，你可以通过访问 `argc` 和 `argv` 来获取和处理这些命令行参数，以便程序能够根据用户提供的参数来执行不同的操作。这是一种非常常见的方式，用于从命令行或脚本中向程序传递输入或配置信息。
